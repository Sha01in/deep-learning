{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f836a729ac8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(range(10))\n",
    "    return lb.transform(x)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, *image_shape], name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    height = conv_ksize[0]\n",
    "    width = conv_ksize[1]\n",
    "    color_channels = x_tensor.get_shape().as_list()[3]\n",
    "    # Weights & bias\n",
    "    weights = tf.Variable(tf.truncated_normal([height, width, color_channels, conv_num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    # Apply Convolution\n",
    "    l_conv_strides = list(conv_strides)\n",
    "    l_conv_strides.insert(0,1)\n",
    "    l_conv_strides.append(1)\n",
    "    conv_strides=tuple(l_conv_strides)\n",
    "    l_pool_ksize=list(pool_ksize)\n",
    "    l_pool_ksize.insert(0,1)\n",
    "    l_pool_ksize.append(1)\n",
    "    pool_ksize=tuple(l_pool_ksize)\n",
    "    l_pool_strides=list(pool_strides)\n",
    "    l_pool_strides.insert(0,1)\n",
    "    l_pool_strides.append(1)\n",
    "    pool_strides=tuple(l_pool_strides)\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, conv_strides, padding='SAME')\n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    # Apply Max Pooling\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=pool_ksize, strides=pool_strides, padding='SAME')\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs=num_outputs, activation_fn=tf.sigmoid)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv_size = (3,3)\n",
    "    conv_strides = (2,2)\n",
    "    pool_size = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "\n",
    "    #conv_layer = conv2d_maxpool(x, 10, [2, 2], [2, 2], [2, 2], [2, 2])\n",
    "    conv_layer = conv2d_maxpool(x, 12, conv_size, conv_strides, pool_size, pool_strides)\n",
    "    conv_layer = conv2d_maxpool(x, 24, conv_size, conv_strides, pool_size, pool_strides)\n",
    "    conv_layer = conv2d_maxpool(x, 36, conv_size, conv_strides, pool_size, pool_strides)\n",
    "\n",
    "    #conv_layer = conv2d_maxpool(conv_layer, 32, conv_size, conv_strides, pool_size, pool_strides)\n",
    "\n",
    "    #conv3 = conv2d_maxpool(conv2, 64, conv_size, conv_strides, pool_size, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    conv_layer = flatten(conv_layer)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    conv_layer = fully_conn(conv_layer, 24)\n",
    "    #conv_layer = fully_conn(conv_layer, 12)\n",
    "    conv_layer = tf.nn.dropout(conv_layer, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return output(conv_layer, 10)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "\n",
    "    valid_acc = session.run(accuracy, feed_dict={x:valid_features, y:valid_labels, keep_prob:1.0})\n",
    "\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "keep_probability = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2113 Validation Accuracy: 0.200200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1637 Validation Accuracy: 0.248400\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.1240 Validation Accuracy: 0.293000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.0963 Validation Accuracy: 0.327400\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.0715 Validation Accuracy: 0.354400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.0486 Validation Accuracy: 0.369200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.0202 Validation Accuracy: 0.379000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.9951 Validation Accuracy: 0.389200\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.9769 Validation Accuracy: 0.398400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.9625 Validation Accuracy: 0.407400\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.9499 Validation Accuracy: 0.413600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.9380 Validation Accuracy: 0.422400\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.9264 Validation Accuracy: 0.429000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.9148 Validation Accuracy: 0.431400\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.9050 Validation Accuracy: 0.434200\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.8967 Validation Accuracy: 0.442600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.8896 Validation Accuracy: 0.446200\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.8821 Validation Accuracy: 0.451600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.8748 Validation Accuracy: 0.452800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.8681 Validation Accuracy: 0.454200\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.8622 Validation Accuracy: 0.453400\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.8572 Validation Accuracy: 0.455200\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.8527 Validation Accuracy: 0.456400\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.8485 Validation Accuracy: 0.461000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.8441 Validation Accuracy: 0.464200\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.8387 Validation Accuracy: 0.468400\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.8328 Validation Accuracy: 0.472400\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.8277 Validation Accuracy: 0.474800\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.8232 Validation Accuracy: 0.475600\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.8195 Validation Accuracy: 0.479200\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.8161 Validation Accuracy: 0.481600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.8136 Validation Accuracy: 0.484400\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.8123 Validation Accuracy: 0.484400\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.8119 Validation Accuracy: 0.482800\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.8116 Validation Accuracy: 0.480200\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.8098 Validation Accuracy: 0.479800\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.8071 Validation Accuracy: 0.481600\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.8043 Validation Accuracy: 0.485200\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.8013 Validation Accuracy: 0.491000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.8002 Validation Accuracy: 0.492400\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.8004 Validation Accuracy: 0.493200\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.8001 Validation Accuracy: 0.487400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.8066 Validation Accuracy: 0.477400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.8070 Validation Accuracy: 0.479200\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.8025 Validation Accuracy: 0.483400\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.7950 Validation Accuracy: 0.489800\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.7855 Validation Accuracy: 0.493000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.7892 Validation Accuracy: 0.488000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.8008 Validation Accuracy: 0.473800\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.7907 Validation Accuracy: 0.485400\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.7811 Validation Accuracy: 0.491400\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.7777 Validation Accuracy: 0.496200\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.7794 Validation Accuracy: 0.497800\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.7869 Validation Accuracy: 0.496200\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.7928 Validation Accuracy: 0.488000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.7880 Validation Accuracy: 0.495800\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.7808 Validation Accuracy: 0.502800\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.7762 Validation Accuracy: 0.502800\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.7740 Validation Accuracy: 0.501600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.7733 Validation Accuracy: 0.500200\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.7736 Validation Accuracy: 0.497400\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.7746 Validation Accuracy: 0.494600\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.7760 Validation Accuracy: 0.492400\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.7767 Validation Accuracy: 0.491800\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.7730 Validation Accuracy: 0.495600\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.7626 Validation Accuracy: 0.504000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.7549 Validation Accuracy: 0.509200\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.7615 Validation Accuracy: 0.501600\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.7681 Validation Accuracy: 0.494800\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.7624 Validation Accuracy: 0.498400\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.7559 Validation Accuracy: 0.504000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.7519 Validation Accuracy: 0.505000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.7495 Validation Accuracy: 0.506600\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.7480 Validation Accuracy: 0.507200\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.7465 Validation Accuracy: 0.506400\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.7452 Validation Accuracy: 0.507000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.7440 Validation Accuracy: 0.508200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.7428 Validation Accuracy: 0.508000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.7416 Validation Accuracy: 0.509200\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.7404 Validation Accuracy: 0.510000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.7393 Validation Accuracy: 0.510200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.7380 Validation Accuracy: 0.511400\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.7368 Validation Accuracy: 0.511800\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.7357 Validation Accuracy: 0.513000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.7346 Validation Accuracy: 0.514200\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.7335 Validation Accuracy: 0.515400\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.7325 Validation Accuracy: 0.515800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.7315 Validation Accuracy: 0.516400\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.7304 Validation Accuracy: 0.516200\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.7294 Validation Accuracy: 0.517400\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.7284 Validation Accuracy: 0.517400\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.7274 Validation Accuracy: 0.518000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.7264 Validation Accuracy: 0.518800\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.7254 Validation Accuracy: 0.518800\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.7244 Validation Accuracy: 0.519000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.7233 Validation Accuracy: 0.520200\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.7223 Validation Accuracy: 0.520200\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.7212 Validation Accuracy: 0.520400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.7202 Validation Accuracy: 0.521000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.7191 Validation Accuracy: 0.521600\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.7179 Validation Accuracy: 0.522400\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     1.7167 Validation Accuracy: 0.523000\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     1.7156 Validation Accuracy: 0.522400\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     1.7145 Validation Accuracy: 0.522800\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     1.7134 Validation Accuracy: 0.522800\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     1.7125 Validation Accuracy: 0.523400\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     1.7115 Validation Accuracy: 0.521400\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     1.7106 Validation Accuracy: 0.522400\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     1.7100 Validation Accuracy: 0.521800\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     1.7097 Validation Accuracy: 0.521000\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     1.7099 Validation Accuracy: 0.518400\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     1.7109 Validation Accuracy: 0.516000\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     1.7128 Validation Accuracy: 0.514800\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     1.7159 Validation Accuracy: 0.512600\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     1.7195 Validation Accuracy: 0.508400\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     1.7189 Validation Accuracy: 0.507000\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     1.7133 Validation Accuracy: 0.512000\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     1.7084 Validation Accuracy: 0.515800\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     1.7069 Validation Accuracy: 0.516200\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     1.7062 Validation Accuracy: 0.517400\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     1.7058 Validation Accuracy: 0.517600\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     1.7054 Validation Accuracy: 0.515200\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     1.7047 Validation Accuracy: 0.514600\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     1.7035 Validation Accuracy: 0.516000\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     1.7020 Validation Accuracy: 0.515600\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     1.7003 Validation Accuracy: 0.518200\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     1.6986 Validation Accuracy: 0.518400\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     1.6976 Validation Accuracy: 0.520000\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     1.6972 Validation Accuracy: 0.520800\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     1.6983 Validation Accuracy: 0.520400\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     1.7011 Validation Accuracy: 0.519000\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     1.7041 Validation Accuracy: 0.512400\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     1.7034 Validation Accuracy: 0.510200\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     1.6990 Validation Accuracy: 0.513400\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     1.6945 Validation Accuracy: 0.516000\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     1.6916 Validation Accuracy: 0.517200\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     1.6907 Validation Accuracy: 0.513800\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     1.6904 Validation Accuracy: 0.512400\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     1.6902 Validation Accuracy: 0.511800\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     1.6901 Validation Accuracy: 0.511800\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     1.6899 Validation Accuracy: 0.510800\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     1.6898 Validation Accuracy: 0.508600\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     1.6896 Validation Accuracy: 0.508200\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     1.6897 Validation Accuracy: 0.509600\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     1.6899 Validation Accuracy: 0.506800\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     1.6902 Validation Accuracy: 0.506600\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     1.6907 Validation Accuracy: 0.507000\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     1.6914 Validation Accuracy: 0.507800\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     1.6923 Validation Accuracy: 0.507000\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     1.6925 Validation Accuracy: 0.507600\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     1.6900 Validation Accuracy: 0.514000\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     1.6854 Validation Accuracy: 0.523800\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     1.6845 Validation Accuracy: 0.526400\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     1.6881 Validation Accuracy: 0.531000\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     1.6901 Validation Accuracy: 0.528000\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     1.6890 Validation Accuracy: 0.530200\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     1.6870 Validation Accuracy: 0.529200\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     1.6851 Validation Accuracy: 0.525800\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     1.6834 Validation Accuracy: 0.523200\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     1.6820 Validation Accuracy: 0.523400\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     1.6809 Validation Accuracy: 0.522600\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     1.6798 Validation Accuracy: 0.521600\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     1.6782 Validation Accuracy: 0.521600\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     1.6763 Validation Accuracy: 0.522200\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     1.6747 Validation Accuracy: 0.523400\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     1.6730 Validation Accuracy: 0.522800\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     1.6715 Validation Accuracy: 0.523200\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     1.6702 Validation Accuracy: 0.524400\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     1.6691 Validation Accuracy: 0.524400\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     1.6680 Validation Accuracy: 0.524200\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     1.6671 Validation Accuracy: 0.524400\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     1.6662 Validation Accuracy: 0.525400\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     1.6655 Validation Accuracy: 0.525600\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     1.6647 Validation Accuracy: 0.526600\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     1.6640 Validation Accuracy: 0.527200\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     1.6635 Validation Accuracy: 0.527200\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     1.6631 Validation Accuracy: 0.527800\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     1.6627 Validation Accuracy: 0.528000\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     1.6626 Validation Accuracy: 0.527600\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     1.6625 Validation Accuracy: 0.528600\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     1.6625 Validation Accuracy: 0.530400\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     1.6626 Validation Accuracy: 0.531800\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     1.6628 Validation Accuracy: 0.531600\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     1.6629 Validation Accuracy: 0.532000\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     1.6629 Validation Accuracy: 0.531200\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     1.6630 Validation Accuracy: 0.529000\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     1.6631 Validation Accuracy: 0.529400\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     1.6631 Validation Accuracy: 0.526800\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     1.6633 Validation Accuracy: 0.525200\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     1.6635 Validation Accuracy: 0.522800\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     1.6637 Validation Accuracy: 0.522000\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     1.6639 Validation Accuracy: 0.521200\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     1.6638 Validation Accuracy: 0.518200\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     1.6626 Validation Accuracy: 0.516600\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     1.6607 Validation Accuracy: 0.520400\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     1.6588 Validation Accuracy: 0.522000\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     1.6580 Validation Accuracy: 0.521000\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     1.6587 Validation Accuracy: 0.523000\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     1.6604 Validation Accuracy: 0.524600\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     1.6619 Validation Accuracy: 0.523400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2593 Validation Accuracy: 0.094600\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2274 Validation Accuracy: 0.101000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.1960 Validation Accuracy: 0.113600\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.1722 Validation Accuracy: 0.126000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.1540 Validation Accuracy: 0.126200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1363 Validation Accuracy: 0.132200\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.0956 Validation Accuracy: 0.117800\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.0801 Validation Accuracy: 0.108400\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.0663 Validation Accuracy: 0.101200\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.0606 Validation Accuracy: 0.101000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.0585 Validation Accuracy: 0.103000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.0310 Validation Accuracy: 0.101200\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.0236 Validation Accuracy: 0.099800\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.0202 Validation Accuracy: 0.100600\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.0183 Validation Accuracy: 0.102200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.0267 Validation Accuracy: 0.103200\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     2.0047 Validation Accuracy: 0.101800\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.9987 Validation Accuracy: 0.099400\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.9949 Validation Accuracy: 0.100400\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.9930 Validation Accuracy: 0.101200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.0078 Validation Accuracy: 0.101200\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.9850 Validation Accuracy: 0.100000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.9784 Validation Accuracy: 0.098000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.9764 Validation Accuracy: 0.100200\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.9732 Validation Accuracy: 0.101000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.9914 Validation Accuracy: 0.100400\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.9678 Validation Accuracy: 0.098400\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.9625 Validation Accuracy: 0.097600\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.9623 Validation Accuracy: 0.100200\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.9577 Validation Accuracy: 0.099600\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.9782 Validation Accuracy: 0.099200\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.9539 Validation Accuracy: 0.097600\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.9507 Validation Accuracy: 0.096800\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.9508 Validation Accuracy: 0.100000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.9460 Validation Accuracy: 0.099800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.9678 Validation Accuracy: 0.099800\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.9431 Validation Accuracy: 0.097600\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.9411 Validation Accuracy: 0.097800\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.9409 Validation Accuracy: 0.100200\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.9368 Validation Accuracy: 0.100000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.9591 Validation Accuracy: 0.099800\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.9345 Validation Accuracy: 0.097400\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.9330 Validation Accuracy: 0.097600\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.9327 Validation Accuracy: 0.100000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.9293 Validation Accuracy: 0.100400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.9519 Validation Accuracy: 0.100200\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.9274 Validation Accuracy: 0.098000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.9260 Validation Accuracy: 0.098200\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.9255 Validation Accuracy: 0.100400\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.9227 Validation Accuracy: 0.101200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.9459 Validation Accuracy: 0.101200\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.9215 Validation Accuracy: 0.099000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.9197 Validation Accuracy: 0.098000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.9195 Validation Accuracy: 0.101000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.9172 Validation Accuracy: 0.101800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.9408 Validation Accuracy: 0.102400\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.9164 Validation Accuracy: 0.099600\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.9143 Validation Accuracy: 0.098800\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.9140 Validation Accuracy: 0.101400\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.9126 Validation Accuracy: 0.102800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.9365 Validation Accuracy: 0.103800\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.9119 Validation Accuracy: 0.100600\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.9094 Validation Accuracy: 0.099600\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.9094 Validation Accuracy: 0.101400\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.9084 Validation Accuracy: 0.104400\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.9327 Validation Accuracy: 0.104800\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.9080 Validation Accuracy: 0.102200\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.9051 Validation Accuracy: 0.100000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.9052 Validation Accuracy: 0.101800\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.9045 Validation Accuracy: 0.106400\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.9289 Validation Accuracy: 0.107200\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.9050 Validation Accuracy: 0.104600\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.9010 Validation Accuracy: 0.100600\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.9016 Validation Accuracy: 0.103000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.9010 Validation Accuracy: 0.108600\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.9256 Validation Accuracy: 0.110400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.9023 Validation Accuracy: 0.108800\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.8973 Validation Accuracy: 0.101600\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.8980 Validation Accuracy: 0.104600\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.8975 Validation Accuracy: 0.111000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.9224 Validation Accuracy: 0.112000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.8999 Validation Accuracy: 0.111600\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.8940 Validation Accuracy: 0.105800\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.8946 Validation Accuracy: 0.106400\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.8947 Validation Accuracy: 0.114400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.9194 Validation Accuracy: 0.116000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.8980 Validation Accuracy: 0.116200\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.8913 Validation Accuracy: 0.109800\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.8914 Validation Accuracy: 0.110400\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.8924 Validation Accuracy: 0.122000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.9169 Validation Accuracy: 0.123000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.8957 Validation Accuracy: 0.126800\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.8897 Validation Accuracy: 0.117400\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.8880 Validation Accuracy: 0.118000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.8898 Validation Accuracy: 0.137200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.9152 Validation Accuracy: 0.140800\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.8919 Validation Accuracy: 0.149200\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.8878 Validation Accuracy: 0.143200\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.8861 Validation Accuracy: 0.157800\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.8861 Validation Accuracy: 0.215000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.9110 Validation Accuracy: 0.285800\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.8670 Validation Accuracy: 0.424600\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     1.8516 Validation Accuracy: 0.478400\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.8416 Validation Accuracy: 0.482800\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.8447 Validation Accuracy: 0.497200\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.8647 Validation Accuracy: 0.481400\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.8408 Validation Accuracy: 0.495200\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     1.8355 Validation Accuracy: 0.496000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.8331 Validation Accuracy: 0.486800\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.8333 Validation Accuracy: 0.500200\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.8572 Validation Accuracy: 0.491000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.8351 Validation Accuracy: 0.501200\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     1.8265 Validation Accuracy: 0.502800\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.8290 Validation Accuracy: 0.494000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.8275 Validation Accuracy: 0.503200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.8520 Validation Accuracy: 0.496600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.8313 Validation Accuracy: 0.503000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     1.8207 Validation Accuracy: 0.507400\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.8261 Validation Accuracy: 0.497600\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.8239 Validation Accuracy: 0.504600\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.8480 Validation Accuracy: 0.500400\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.8280 Validation Accuracy: 0.505600\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     1.8162 Validation Accuracy: 0.509400\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.8235 Validation Accuracy: 0.500200\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.8205 Validation Accuracy: 0.506200\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.8446 Validation Accuracy: 0.504400\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.8251 Validation Accuracy: 0.508200\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     1.8125 Validation Accuracy: 0.509200\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.8212 Validation Accuracy: 0.499800\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.8175 Validation Accuracy: 0.510200\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.8414 Validation Accuracy: 0.507200\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.8225 Validation Accuracy: 0.510000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     1.8093 Validation Accuracy: 0.512600\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.8191 Validation Accuracy: 0.500800\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.8148 Validation Accuracy: 0.511800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.8385 Validation Accuracy: 0.510200\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.8200 Validation Accuracy: 0.509800\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     1.8063 Validation Accuracy: 0.514000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     1.8172 Validation Accuracy: 0.502400\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.8123 Validation Accuracy: 0.512400\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.8359 Validation Accuracy: 0.513200\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.8177 Validation Accuracy: 0.509800\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     1.8036 Validation Accuracy: 0.516000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     1.8152 Validation Accuracy: 0.504600\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.8100 Validation Accuracy: 0.515200\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.8333 Validation Accuracy: 0.514800\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.8157 Validation Accuracy: 0.512000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     1.8010 Validation Accuracy: 0.518000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     1.8133 Validation Accuracy: 0.507200\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.8078 Validation Accuracy: 0.514000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.8310 Validation Accuracy: 0.514200\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     1.8137 Validation Accuracy: 0.513200\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     1.7984 Validation Accuracy: 0.519000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     1.8115 Validation Accuracy: 0.508000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     1.8055 Validation Accuracy: 0.515200\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.8286 Validation Accuracy: 0.514800\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     1.8119 Validation Accuracy: 0.514600\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     1.7961 Validation Accuracy: 0.520600\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     1.8099 Validation Accuracy: 0.506800\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     1.8035 Validation Accuracy: 0.518000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.8263 Validation Accuracy: 0.517000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     1.8100 Validation Accuracy: 0.515600\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     1.7941 Validation Accuracy: 0.520600\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     1.8082 Validation Accuracy: 0.507600\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     1.8016 Validation Accuracy: 0.518000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.8240 Validation Accuracy: 0.518600\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     1.8082 Validation Accuracy: 0.515200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     1.7919 Validation Accuracy: 0.519800\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     1.8065 Validation Accuracy: 0.508200\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     1.7998 Validation Accuracy: 0.518600\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.8219 Validation Accuracy: 0.519600\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     1.8065 Validation Accuracy: 0.516200\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     1.7899 Validation Accuracy: 0.521200\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     1.8049 Validation Accuracy: 0.508600\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     1.7982 Validation Accuracy: 0.518000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.8200 Validation Accuracy: 0.521000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     1.8048 Validation Accuracy: 0.515000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     1.7879 Validation Accuracy: 0.523600\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     1.8033 Validation Accuracy: 0.508200\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     1.7964 Validation Accuracy: 0.518400\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.8181 Validation Accuracy: 0.519400\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     1.8032 Validation Accuracy: 0.516200\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     1.7864 Validation Accuracy: 0.525400\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     1.8018 Validation Accuracy: 0.509000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     1.7951 Validation Accuracy: 0.520400\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.8165 Validation Accuracy: 0.522600\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     1.8017 Validation Accuracy: 0.518600\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     1.7847 Validation Accuracy: 0.526600\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     1.8003 Validation Accuracy: 0.508800\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     1.7941 Validation Accuracy: 0.520200\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.8150 Validation Accuracy: 0.525200\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     1.8001 Validation Accuracy: 0.520600\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     1.7833 Validation Accuracy: 0.526000\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     1.7986 Validation Accuracy: 0.509600\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     1.7928 Validation Accuracy: 0.522200\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.8136 Validation Accuracy: 0.527400\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     1.7988 Validation Accuracy: 0.522400\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     1.7817 Validation Accuracy: 0.526000\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     1.7967 Validation Accuracy: 0.511600\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     1.7919 Validation Accuracy: 0.524600\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.8121 Validation Accuracy: 0.529200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     1.7975 Validation Accuracy: 0.528800\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     1.7800 Validation Accuracy: 0.525200\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     1.7949 Validation Accuracy: 0.514800\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     1.7909 Validation Accuracy: 0.525600\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.8105 Validation Accuracy: 0.528200\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     1.7969 Validation Accuracy: 0.525600\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     1.7775 Validation Accuracy: 0.525000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     1.7936 Validation Accuracy: 0.518200\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     1.7907 Validation Accuracy: 0.522600\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.8087 Validation Accuracy: 0.527400\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     1.7973 Validation Accuracy: 0.523400\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     1.7756 Validation Accuracy: 0.526600\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     1.7944 Validation Accuracy: 0.518600\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     1.7895 Validation Accuracy: 0.524000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.8071 Validation Accuracy: 0.530600\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     1.7980 Validation Accuracy: 0.522200\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     1.7762 Validation Accuracy: 0.524600\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     1.7979 Validation Accuracy: 0.517600\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     1.7860 Validation Accuracy: 0.528800\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.8070 Validation Accuracy: 0.527400\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     1.7967 Validation Accuracy: 0.521400\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     1.7789 Validation Accuracy: 0.519200\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     1.7977 Validation Accuracy: 0.517200\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     1.7837 Validation Accuracy: 0.529200\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.8071 Validation Accuracy: 0.525000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     1.7976 Validation Accuracy: 0.515200\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     1.7828 Validation Accuracy: 0.515000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     1.7973 Validation Accuracy: 0.518800\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     1.7844 Validation Accuracy: 0.532800\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.8055 Validation Accuracy: 0.525600\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     1.7943 Validation Accuracy: 0.520800\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     1.7791 Validation Accuracy: 0.522800\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     1.7972 Validation Accuracy: 0.521200\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     1.7819 Validation Accuracy: 0.531000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.8033 Validation Accuracy: 0.529200\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     1.7908 Validation Accuracy: 0.522600\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     1.7749 Validation Accuracy: 0.523200\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     1.7994 Validation Accuracy: 0.513800\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     1.7825 Validation Accuracy: 0.531400\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.8024 Validation Accuracy: 0.527600\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     1.7886 Validation Accuracy: 0.528200\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     1.7694 Validation Accuracy: 0.527600\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     1.7965 Validation Accuracy: 0.518400\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     1.7796 Validation Accuracy: 0.534600\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.7984 Validation Accuracy: 0.528600\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     1.7863 Validation Accuracy: 0.529000\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     1.7649 Validation Accuracy: 0.533000\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     1.7909 Validation Accuracy: 0.524800\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     1.7790 Validation Accuracy: 0.537400\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.7983 Validation Accuracy: 0.532000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     1.7857 Validation Accuracy: 0.533600\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     1.7639 Validation Accuracy: 0.534600\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     1.7882 Validation Accuracy: 0.527000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     1.7785 Validation Accuracy: 0.538600\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.7974 Validation Accuracy: 0.532800\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     1.7847 Validation Accuracy: 0.532800\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     1.7633 Validation Accuracy: 0.534800\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     1.7866 Validation Accuracy: 0.528000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     1.7776 Validation Accuracy: 0.538400\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.7965 Validation Accuracy: 0.531800\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     1.7846 Validation Accuracy: 0.529600\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     1.7643 Validation Accuracy: 0.533000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     1.7865 Validation Accuracy: 0.525400\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     1.7785 Validation Accuracy: 0.537200\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.7971 Validation Accuracy: 0.532000\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     1.7860 Validation Accuracy: 0.526400\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     1.7675 Validation Accuracy: 0.527400\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     1.7890 Validation Accuracy: 0.521600\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     1.7849 Validation Accuracy: 0.523400\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.8020 Validation Accuracy: 0.522400\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     1.7906 Validation Accuracy: 0.517800\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     1.7700 Validation Accuracy: 0.520200\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     1.7937 Validation Accuracy: 0.516000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     1.7857 Validation Accuracy: 0.524000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.7985 Validation Accuracy: 0.525600\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     1.7826 Validation Accuracy: 0.535600\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     1.7604 Validation Accuracy: 0.537400\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     1.7814 Validation Accuracy: 0.533400\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     1.7730 Validation Accuracy: 0.541000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.7920 Validation Accuracy: 0.533400\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     1.7816 Validation Accuracy: 0.535800\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     1.7585 Validation Accuracy: 0.538200\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     1.7804 Validation Accuracy: 0.531200\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     1.7713 Validation Accuracy: 0.541800\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.7901 Validation Accuracy: 0.535000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     1.7805 Validation Accuracy: 0.536000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     1.7574 Validation Accuracy: 0.538600\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     1.7793 Validation Accuracy: 0.532600\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     1.7697 Validation Accuracy: 0.541600\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.7884 Validation Accuracy: 0.535400\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     1.7798 Validation Accuracy: 0.536800\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     1.7566 Validation Accuracy: 0.539400\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     1.7781 Validation Accuracy: 0.533600\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     1.7684 Validation Accuracy: 0.540800\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.7869 Validation Accuracy: 0.536600\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     1.7792 Validation Accuracy: 0.535800\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     1.7559 Validation Accuracy: 0.539000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     1.7770 Validation Accuracy: 0.534000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     1.7672 Validation Accuracy: 0.541400\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.7856 Validation Accuracy: 0.536800\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     1.7786 Validation Accuracy: 0.536200\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     1.7552 Validation Accuracy: 0.539600\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     1.7759 Validation Accuracy: 0.535800\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     1.7659 Validation Accuracy: 0.541600\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.7843 Validation Accuracy: 0.537200\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     1.7782 Validation Accuracy: 0.535400\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     1.7545 Validation Accuracy: 0.539800\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     1.7746 Validation Accuracy: 0.536000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     1.7649 Validation Accuracy: 0.541200\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.7832 Validation Accuracy: 0.538000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     1.7776 Validation Accuracy: 0.536000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     1.7538 Validation Accuracy: 0.540600\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     1.7735 Validation Accuracy: 0.536400\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     1.7638 Validation Accuracy: 0.540600\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.7822 Validation Accuracy: 0.538600\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     1.7770 Validation Accuracy: 0.535800\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     1.7532 Validation Accuracy: 0.539000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     1.7722 Validation Accuracy: 0.537400\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     1.7629 Validation Accuracy: 0.541000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.7810 Validation Accuracy: 0.538800\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     1.7766 Validation Accuracy: 0.534400\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     1.7526 Validation Accuracy: 0.539400\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     1.7712 Validation Accuracy: 0.538400\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     1.7619 Validation Accuracy: 0.541800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.7800 Validation Accuracy: 0.539800\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     1.7759 Validation Accuracy: 0.533800\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     1.7521 Validation Accuracy: 0.538400\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     1.7702 Validation Accuracy: 0.538000\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     1.7611 Validation Accuracy: 0.540800\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.7790 Validation Accuracy: 0.539800\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     1.7751 Validation Accuracy: 0.533400\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     1.7514 Validation Accuracy: 0.538000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     1.7692 Validation Accuracy: 0.539000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     1.7603 Validation Accuracy: 0.541000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.7783 Validation Accuracy: 0.539800\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     1.7743 Validation Accuracy: 0.534000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     1.7506 Validation Accuracy: 0.537600\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     1.7681 Validation Accuracy: 0.537600\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     1.7593 Validation Accuracy: 0.541200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.7775 Validation Accuracy: 0.540400\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     1.7737 Validation Accuracy: 0.534400\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     1.7499 Validation Accuracy: 0.537600\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     1.7673 Validation Accuracy: 0.537800\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     1.7583 Validation Accuracy: 0.542600\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.7766 Validation Accuracy: 0.540000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     1.7731 Validation Accuracy: 0.535000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     1.7493 Validation Accuracy: 0.538200\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     1.7662 Validation Accuracy: 0.538600\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     1.7575 Validation Accuracy: 0.543200\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.7757 Validation Accuracy: 0.540800\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     1.7727 Validation Accuracy: 0.535600\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     1.7489 Validation Accuracy: 0.537400\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     1.7653 Validation Accuracy: 0.538800\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     1.7568 Validation Accuracy: 0.542400\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.7748 Validation Accuracy: 0.542000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     1.7721 Validation Accuracy: 0.534400\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     1.7479 Validation Accuracy: 0.537800\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     1.7644 Validation Accuracy: 0.538200\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     1.7561 Validation Accuracy: 0.541200\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.7742 Validation Accuracy: 0.542200\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     1.7715 Validation Accuracy: 0.534400\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     1.7474 Validation Accuracy: 0.537000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     1.7631 Validation Accuracy: 0.537600\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     1.7553 Validation Accuracy: 0.541200\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.7733 Validation Accuracy: 0.541200\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     1.7711 Validation Accuracy: 0.534400\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     1.7466 Validation Accuracy: 0.537400\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     1.7623 Validation Accuracy: 0.536600\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     1.7548 Validation Accuracy: 0.540800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.7728 Validation Accuracy: 0.541800\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     1.7710 Validation Accuracy: 0.532600\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     1.7463 Validation Accuracy: 0.536400\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     1.7612 Validation Accuracy: 0.539200\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     1.7542 Validation Accuracy: 0.541600\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.7719 Validation Accuracy: 0.540600\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     1.7712 Validation Accuracy: 0.532600\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     1.7458 Validation Accuracy: 0.537800\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     1.7603 Validation Accuracy: 0.537600\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     1.7546 Validation Accuracy: 0.539600\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.7711 Validation Accuracy: 0.540400\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     1.7719 Validation Accuracy: 0.529400\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     1.7450 Validation Accuracy: 0.539200\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     1.7596 Validation Accuracy: 0.539000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     1.7557 Validation Accuracy: 0.541200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.7701 Validation Accuracy: 0.542000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     1.7746 Validation Accuracy: 0.528400\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     1.7442 Validation Accuracy: 0.538200\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     1.7611 Validation Accuracy: 0.532600\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     1.7566 Validation Accuracy: 0.540200\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.7678 Validation Accuracy: 0.540800\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     1.7805 Validation Accuracy: 0.516200\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     1.7462 Validation Accuracy: 0.545400\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     1.7661 Validation Accuracy: 0.533000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     1.7493 Validation Accuracy: 0.543600\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.7801 Validation Accuracy: 0.532600\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     1.7740 Validation Accuracy: 0.525600\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     1.7524 Validation Accuracy: 0.541000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     1.7627 Validation Accuracy: 0.537800\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     1.7492 Validation Accuracy: 0.541000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.7815 Validation Accuracy: 0.533000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     1.7691 Validation Accuracy: 0.533000\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     1.7499 Validation Accuracy: 0.541400\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     1.7581 Validation Accuracy: 0.539000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     1.7514 Validation Accuracy: 0.537800\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.7752 Validation Accuracy: 0.533600\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     1.7696 Validation Accuracy: 0.531600\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     1.7476 Validation Accuracy: 0.541600\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     1.7566 Validation Accuracy: 0.540200\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     1.7498 Validation Accuracy: 0.538800\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.7727 Validation Accuracy: 0.534600\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     1.7681 Validation Accuracy: 0.532200\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     1.7461 Validation Accuracy: 0.543400\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     1.7563 Validation Accuracy: 0.538800\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     1.7485 Validation Accuracy: 0.539600\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.7712 Validation Accuracy: 0.536400\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     1.7671 Validation Accuracy: 0.532400\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     1.7448 Validation Accuracy: 0.542800\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     1.7559 Validation Accuracy: 0.537800\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     1.7478 Validation Accuracy: 0.540200\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.7699 Validation Accuracy: 0.538600\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     1.7660 Validation Accuracy: 0.532800\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     1.7441 Validation Accuracy: 0.540800\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     1.7558 Validation Accuracy: 0.537200\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     1.7470 Validation Accuracy: 0.538800\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.7683 Validation Accuracy: 0.539600\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     1.7650 Validation Accuracy: 0.534800\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     1.7434 Validation Accuracy: 0.540600\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     1.7558 Validation Accuracy: 0.536000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     1.7467 Validation Accuracy: 0.537200\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.7670 Validation Accuracy: 0.539200\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     1.7642 Validation Accuracy: 0.535600\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     1.7427 Validation Accuracy: 0.540200\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     1.7557 Validation Accuracy: 0.536400\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     1.7462 Validation Accuracy: 0.536800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.7657 Validation Accuracy: 0.540400\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     1.7633 Validation Accuracy: 0.536000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     1.7421 Validation Accuracy: 0.540000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     1.7559 Validation Accuracy: 0.535400\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     1.7459 Validation Accuracy: 0.536200\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.7644 Validation Accuracy: 0.540400\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     1.7626 Validation Accuracy: 0.536000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     1.7412 Validation Accuracy: 0.539800\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     1.7560 Validation Accuracy: 0.535400\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     1.7457 Validation Accuracy: 0.534800\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.7632 Validation Accuracy: 0.542400\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     1.7618 Validation Accuracy: 0.536400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     1.7405 Validation Accuracy: 0.538200\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     1.7561 Validation Accuracy: 0.533600\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     1.7454 Validation Accuracy: 0.534000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.7621 Validation Accuracy: 0.542000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     1.7611 Validation Accuracy: 0.536400\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     1.7396 Validation Accuracy: 0.537600\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     1.7562 Validation Accuracy: 0.533400\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     1.7450 Validation Accuracy: 0.533200\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.7608 Validation Accuracy: 0.541800\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     1.7601 Validation Accuracy: 0.537000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     1.7390 Validation Accuracy: 0.540400\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     1.7559 Validation Accuracy: 0.531600\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     1.7442 Validation Accuracy: 0.532200\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.7595 Validation Accuracy: 0.541400\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     1.7595 Validation Accuracy: 0.537800\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     1.7429 Validation Accuracy: 0.540400\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     1.7583 Validation Accuracy: 0.530400\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     1.7465 Validation Accuracy: 0.530000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.7611 Validation Accuracy: 0.539600\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     1.7629 Validation Accuracy: 0.539000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     1.7456 Validation Accuracy: 0.537800\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     1.7597 Validation Accuracy: 0.531800\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     1.7466 Validation Accuracy: 0.531800\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.7610 Validation Accuracy: 0.539200\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     1.7613 Validation Accuracy: 0.540000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     1.7424 Validation Accuracy: 0.537800\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     1.7577 Validation Accuracy: 0.534200\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     1.7443 Validation Accuracy: 0.531400\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.7600 Validation Accuracy: 0.540400\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     1.7595 Validation Accuracy: 0.539800\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     1.7399 Validation Accuracy: 0.539000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     1.7560 Validation Accuracy: 0.534200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     1.7425 Validation Accuracy: 0.531200\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.7593 Validation Accuracy: 0.541400\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     1.7579 Validation Accuracy: 0.540600\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     1.7377 Validation Accuracy: 0.540000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     1.7545 Validation Accuracy: 0.535600\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     1.7414 Validation Accuracy: 0.531400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.7581 Validation Accuracy: 0.541000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     1.7568 Validation Accuracy: 0.541800\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     1.7363 Validation Accuracy: 0.540400\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     1.7533 Validation Accuracy: 0.534800\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     1.7400 Validation Accuracy: 0.533000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.7581 Validation Accuracy: 0.538400\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     1.7560 Validation Accuracy: 0.541200\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     1.7352 Validation Accuracy: 0.540200\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     1.7521 Validation Accuracy: 0.534400\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     1.7387 Validation Accuracy: 0.533600\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.7575 Validation Accuracy: 0.538000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     1.7554 Validation Accuracy: 0.541800\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     1.7342 Validation Accuracy: 0.539400\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     1.7513 Validation Accuracy: 0.534400\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     1.7375 Validation Accuracy: 0.534200\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.7570 Validation Accuracy: 0.538600\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     1.7547 Validation Accuracy: 0.541600\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     1.7335 Validation Accuracy: 0.539200\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     1.7507 Validation Accuracy: 0.533000\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     1.7364 Validation Accuracy: 0.534200\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     1.7563 Validation Accuracy: 0.539400\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     1.7540 Validation Accuracy: 0.542000\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     1.7324 Validation Accuracy: 0.538400\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     1.7505 Validation Accuracy: 0.533000\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     1.7360 Validation Accuracy: 0.535200\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     1.7566 Validation Accuracy: 0.538800\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     1.7531 Validation Accuracy: 0.543200\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     1.7318 Validation Accuracy: 0.538800\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     1.7499 Validation Accuracy: 0.530400\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     1.7354 Validation Accuracy: 0.534000\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     1.7562 Validation Accuracy: 0.539200\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     1.7517 Validation Accuracy: 0.543800\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     1.7314 Validation Accuracy: 0.537400\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     1.7500 Validation Accuracy: 0.529600\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     1.7363 Validation Accuracy: 0.534600\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     1.7573 Validation Accuracy: 0.538000\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     1.7502 Validation Accuracy: 0.544000\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     1.7306 Validation Accuracy: 0.534200\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     1.7491 Validation Accuracy: 0.530400\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     1.7372 Validation Accuracy: 0.535000\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     1.7583 Validation Accuracy: 0.535200\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     1.7483 Validation Accuracy: 0.545400\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     1.7300 Validation Accuracy: 0.536600\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     1.7464 Validation Accuracy: 0.532800\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     1.7378 Validation Accuracy: 0.534800\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     1.7601 Validation Accuracy: 0.533400\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     1.7490 Validation Accuracy: 0.549600\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     1.7305 Validation Accuracy: 0.540800\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     1.7418 Validation Accuracy: 0.536400\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     1.7382 Validation Accuracy: 0.536000\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     1.7619 Validation Accuracy: 0.530400\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     1.7535 Validation Accuracy: 0.540000\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     1.7317 Validation Accuracy: 0.539000\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     1.7424 Validation Accuracy: 0.541600\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     1.7366 Validation Accuracy: 0.541000\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     1.7579 Validation Accuracy: 0.535800\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     1.7548 Validation Accuracy: 0.538200\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     1.7351 Validation Accuracy: 0.534000\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     1.7438 Validation Accuracy: 0.549600\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     1.7356 Validation Accuracy: 0.543600\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     1.7555 Validation Accuracy: 0.537600\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     1.7545 Validation Accuracy: 0.538400\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     1.7374 Validation Accuracy: 0.531600\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     1.7454 Validation Accuracy: 0.548000\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     1.7342 Validation Accuracy: 0.542400\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     1.7538 Validation Accuracy: 0.539200\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     1.7520 Validation Accuracy: 0.539200\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     1.7361 Validation Accuracy: 0.532000\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     1.7464 Validation Accuracy: 0.546200\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     1.7344 Validation Accuracy: 0.541800\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     1.7524 Validation Accuracy: 0.538400\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     1.7493 Validation Accuracy: 0.540800\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     1.7335 Validation Accuracy: 0.535600\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     1.7448 Validation Accuracy: 0.545600\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     1.7340 Validation Accuracy: 0.542400\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     1.7511 Validation Accuracy: 0.541200\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     1.7473 Validation Accuracy: 0.542400\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     1.7315 Validation Accuracy: 0.536400\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     1.7425 Validation Accuracy: 0.547600\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     1.7329 Validation Accuracy: 0.541800\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     1.7502 Validation Accuracy: 0.542600\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     1.7460 Validation Accuracy: 0.543600\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     1.7297 Validation Accuracy: 0.537800\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     1.7403 Validation Accuracy: 0.549400\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     1.7315 Validation Accuracy: 0.541600\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     1.7494 Validation Accuracy: 0.543600\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     1.7450 Validation Accuracy: 0.544200\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     1.7288 Validation Accuracy: 0.538000\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     1.7385 Validation Accuracy: 0.547600\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     1.7307 Validation Accuracy: 0.542400\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     1.7490 Validation Accuracy: 0.542800\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     1.7441 Validation Accuracy: 0.543400\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     1.7280 Validation Accuracy: 0.538600\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     1.7371 Validation Accuracy: 0.546000\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     1.7300 Validation Accuracy: 0.542600\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     1.7487 Validation Accuracy: 0.541800\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     1.7434 Validation Accuracy: 0.541800\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     1.7274 Validation Accuracy: 0.538200\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     1.7361 Validation Accuracy: 0.545400\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     1.7294 Validation Accuracy: 0.542400\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     1.7483 Validation Accuracy: 0.540600\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     1.7429 Validation Accuracy: 0.541800\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     1.7268 Validation Accuracy: 0.539600\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     1.7350 Validation Accuracy: 0.544600\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     1.7291 Validation Accuracy: 0.542000\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     1.7483 Validation Accuracy: 0.541800\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     1.7422 Validation Accuracy: 0.541200\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     1.7261 Validation Accuracy: 0.539400\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     1.7341 Validation Accuracy: 0.543800\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     1.7284 Validation Accuracy: 0.541600\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     1.7479 Validation Accuracy: 0.540000\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     1.7417 Validation Accuracy: 0.539200\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     1.7259 Validation Accuracy: 0.538400\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     1.7340 Validation Accuracy: 0.544200\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     1.7281 Validation Accuracy: 0.541200\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     1.7474 Validation Accuracy: 0.540600\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     1.7410 Validation Accuracy: 0.538800\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     1.7252 Validation Accuracy: 0.538200\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     1.7335 Validation Accuracy: 0.545200\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     1.7267 Validation Accuracy: 0.543400\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     1.7469 Validation Accuracy: 0.541800\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     1.7404 Validation Accuracy: 0.539200\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     1.7244 Validation Accuracy: 0.539200\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     1.7329 Validation Accuracy: 0.544400\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     1.7261 Validation Accuracy: 0.543200\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     1.7465 Validation Accuracy: 0.543400\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     1.7404 Validation Accuracy: 0.538400\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     1.7240 Validation Accuracy: 0.540400\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     1.7321 Validation Accuracy: 0.544400\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     1.7254 Validation Accuracy: 0.543200\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     1.7455 Validation Accuracy: 0.543200\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     1.7401 Validation Accuracy: 0.537800\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     1.7238 Validation Accuracy: 0.539200\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     1.7315 Validation Accuracy: 0.543600\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     1.7249 Validation Accuracy: 0.543600\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     1.7449 Validation Accuracy: 0.543000\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     1.7395 Validation Accuracy: 0.538000\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     1.7230 Validation Accuracy: 0.540400\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     1.7311 Validation Accuracy: 0.544400\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     1.7243 Validation Accuracy: 0.542800\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     1.7443 Validation Accuracy: 0.544000\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     1.7389 Validation Accuracy: 0.539800\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     1.7228 Validation Accuracy: 0.541600\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     1.7306 Validation Accuracy: 0.544400\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     1.7238 Validation Accuracy: 0.543200\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     1.7437 Validation Accuracy: 0.543600\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     1.7381 Validation Accuracy: 0.539000\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     1.7227 Validation Accuracy: 0.541400\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     1.7299 Validation Accuracy: 0.544400\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     1.7233 Validation Accuracy: 0.542200\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     1.7432 Validation Accuracy: 0.543000\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     1.7375 Validation Accuracy: 0.538600\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     1.7222 Validation Accuracy: 0.541600\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     1.7292 Validation Accuracy: 0.545000\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     1.7228 Validation Accuracy: 0.542200\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     1.7427 Validation Accuracy: 0.542600\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     1.7368 Validation Accuracy: 0.538200\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     1.7219 Validation Accuracy: 0.542400\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     1.7287 Validation Accuracy: 0.545200\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     1.7224 Validation Accuracy: 0.541800\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     1.7424 Validation Accuracy: 0.543400\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     1.7361 Validation Accuracy: 0.538200\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     1.7216 Validation Accuracy: 0.543000\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     1.7282 Validation Accuracy: 0.544200\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     1.7218 Validation Accuracy: 0.542200\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     1.7421 Validation Accuracy: 0.543200\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     1.7356 Validation Accuracy: 0.539800\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     1.7213 Validation Accuracy: 0.542800\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     1.7276 Validation Accuracy: 0.544200\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     1.7212 Validation Accuracy: 0.542600\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     1.7417 Validation Accuracy: 0.542800\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     1.7349 Validation Accuracy: 0.540200\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     1.7209 Validation Accuracy: 0.543400\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     1.7273 Validation Accuracy: 0.542600\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     1.7208 Validation Accuracy: 0.543200\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     1.7414 Validation Accuracy: 0.541800\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     1.7343 Validation Accuracy: 0.540000\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     1.7206 Validation Accuracy: 0.542800\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     1.7269 Validation Accuracy: 0.543000\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     1.7203 Validation Accuracy: 0.543600\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     1.7411 Validation Accuracy: 0.544200\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     1.7335 Validation Accuracy: 0.540000\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     1.7203 Validation Accuracy: 0.541200\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     1.7266 Validation Accuracy: 0.542600\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     1.7198 Validation Accuracy: 0.544000\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     1.7406 Validation Accuracy: 0.544200\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     1.7330 Validation Accuracy: 0.538600\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     1.7198 Validation Accuracy: 0.540400\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     1.7265 Validation Accuracy: 0.542200\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     1.7196 Validation Accuracy: 0.544200\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     1.7400 Validation Accuracy: 0.544200\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     1.7324 Validation Accuracy: 0.537600\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     1.7196 Validation Accuracy: 0.542600\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     1.7260 Validation Accuracy: 0.542000\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     1.7191 Validation Accuracy: 0.544400\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     1.7398 Validation Accuracy: 0.543600\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     1.7321 Validation Accuracy: 0.538800\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     1.7194 Validation Accuracy: 0.542800\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     1.7258 Validation Accuracy: 0.542000\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     1.7189 Validation Accuracy: 0.543600\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     1.7392 Validation Accuracy: 0.543400\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     1.7315 Validation Accuracy: 0.538600\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     1.7193 Validation Accuracy: 0.543800\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     1.7256 Validation Accuracy: 0.539800\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     1.7183 Validation Accuracy: 0.543000\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     1.7388 Validation Accuracy: 0.543600\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     1.7315 Validation Accuracy: 0.536200\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     1.7196 Validation Accuracy: 0.544800\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     1.7253 Validation Accuracy: 0.539600\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     1.7182 Validation Accuracy: 0.543800\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     1.7387 Validation Accuracy: 0.543600\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     1.7316 Validation Accuracy: 0.536000\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     1.7199 Validation Accuracy: 0.546600\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     1.7255 Validation Accuracy: 0.537400\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     1.7182 Validation Accuracy: 0.543600\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     1.7386 Validation Accuracy: 0.544000\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     1.7318 Validation Accuracy: 0.534600\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     1.7204 Validation Accuracy: 0.545200\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     1.7263 Validation Accuracy: 0.537400\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     1.7182 Validation Accuracy: 0.538400\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     1.7386 Validation Accuracy: 0.543000\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     1.7332 Validation Accuracy: 0.532600\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     1.7226 Validation Accuracy: 0.543200\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     1.7282 Validation Accuracy: 0.534600\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     1.7187 Validation Accuracy: 0.539800\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     1.7393 Validation Accuracy: 0.543800\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     1.7351 Validation Accuracy: 0.527600\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     1.7267 Validation Accuracy: 0.539200\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     1.7343 Validation Accuracy: 0.530600\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     1.7211 Validation Accuracy: 0.539600\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     1.7469 Validation Accuracy: 0.540600\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     1.7351 Validation Accuracy: 0.528800\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     1.7204 Validation Accuracy: 0.544400\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     1.7288 Validation Accuracy: 0.534600\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     1.7201 Validation Accuracy: 0.545800\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     1.7459 Validation Accuracy: 0.543600\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     1.7375 Validation Accuracy: 0.535400\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     1.7227 Validation Accuracy: 0.546000\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     1.7254 Validation Accuracy: 0.538800\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     1.7202 Validation Accuracy: 0.544200\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     1.7423 Validation Accuracy: 0.545200\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     1.7341 Validation Accuracy: 0.538600\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     1.7211 Validation Accuracy: 0.545200\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     1.7229 Validation Accuracy: 0.538000\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     1.7185 Validation Accuracy: 0.544800\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     1.7407 Validation Accuracy: 0.543200\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     1.7327 Validation Accuracy: 0.541200\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     1.7186 Validation Accuracy: 0.545800\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     1.7208 Validation Accuracy: 0.537200\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     1.7166 Validation Accuracy: 0.545200\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     1.7395 Validation Accuracy: 0.544600\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     1.7321 Validation Accuracy: 0.544200\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     1.7172 Validation Accuracy: 0.546200\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     1.7192 Validation Accuracy: 0.539200\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     1.7149 Validation Accuracy: 0.546000\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     1.7386 Validation Accuracy: 0.544400\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     1.7317 Validation Accuracy: 0.545200\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     1.7165 Validation Accuracy: 0.547600\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     1.7177 Validation Accuracy: 0.538600\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     1.7139 Validation Accuracy: 0.546400\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     1.7377 Validation Accuracy: 0.543200\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     1.7315 Validation Accuracy: 0.544200\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     1.7159 Validation Accuracy: 0.546600\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     1.7165 Validation Accuracy: 0.539200\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     1.7131 Validation Accuracy: 0.546000\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     1.7373 Validation Accuracy: 0.542600\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     1.7313 Validation Accuracy: 0.543800\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     1.7154 Validation Accuracy: 0.547200\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     1.7151 Validation Accuracy: 0.539800\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     1.7123 Validation Accuracy: 0.548200\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     1.7369 Validation Accuracy: 0.542600\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     1.7315 Validation Accuracy: 0.544200\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     1.7153 Validation Accuracy: 0.546800\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     1.7140 Validation Accuracy: 0.542000\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     1.7116 Validation Accuracy: 0.547400\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     1.7364 Validation Accuracy: 0.541800\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     1.7314 Validation Accuracy: 0.544200\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     1.7153 Validation Accuracy: 0.545600\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     1.7130 Validation Accuracy: 0.543000\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     1.7109 Validation Accuracy: 0.547800\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     1.7356 Validation Accuracy: 0.540400\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     1.7309 Validation Accuracy: 0.545600\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     1.7148 Validation Accuracy: 0.546400\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     1.7123 Validation Accuracy: 0.542400\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     1.7104 Validation Accuracy: 0.547000\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     1.7347 Validation Accuracy: 0.540000\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     1.7305 Validation Accuracy: 0.547000\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     1.7144 Validation Accuracy: 0.545600\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     1.7117 Validation Accuracy: 0.543800\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     1.7096 Validation Accuracy: 0.547200\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     1.7337 Validation Accuracy: 0.541000\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     1.7300 Validation Accuracy: 0.547200\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     1.7137 Validation Accuracy: 0.546800\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     1.7113 Validation Accuracy: 0.542800\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     1.7090 Validation Accuracy: 0.546400\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     1.7328 Validation Accuracy: 0.540400\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     1.7299 Validation Accuracy: 0.548200\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     1.7124 Validation Accuracy: 0.546400\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     1.7116 Validation Accuracy: 0.544000\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     1.7083 Validation Accuracy: 0.546800\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     1.7311 Validation Accuracy: 0.540400\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     1.7289 Validation Accuracy: 0.550000\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     1.7096 Validation Accuracy: 0.546200\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     1.7142 Validation Accuracy: 0.542800\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     1.7110 Validation Accuracy: 0.544800\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     1.7346 Validation Accuracy: 0.543600\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     1.7323 Validation Accuracy: 0.547400\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     1.7129 Validation Accuracy: 0.544600\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     1.7289 Validation Accuracy: 0.530400\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     1.7262 Validation Accuracy: 0.535800\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     1.7377 Validation Accuracy: 0.539400\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     1.7308 Validation Accuracy: 0.546200\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     1.7106 Validation Accuracy: 0.543400\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     1.7256 Validation Accuracy: 0.533000\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     1.7200 Validation Accuracy: 0.537800\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     1.7329 Validation Accuracy: 0.541800\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     1.7284 Validation Accuracy: 0.547000\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     1.7075 Validation Accuracy: 0.542400\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     1.7219 Validation Accuracy: 0.536600\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     1.7170 Validation Accuracy: 0.540000\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     1.7305 Validation Accuracy: 0.542200\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     1.7275 Validation Accuracy: 0.547200\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     1.7067 Validation Accuracy: 0.543000\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     1.7202 Validation Accuracy: 0.537200\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     1.7153 Validation Accuracy: 0.538600\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     1.7287 Validation Accuracy: 0.543600\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     1.7271 Validation Accuracy: 0.549600\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     1.7060 Validation Accuracy: 0.544400\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     1.7189 Validation Accuracy: 0.537400\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     1.7140 Validation Accuracy: 0.541600\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     1.7273 Validation Accuracy: 0.543800\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     1.7269 Validation Accuracy: 0.549000\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     1.7053 Validation Accuracy: 0.545600\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     1.7180 Validation Accuracy: 0.539200\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     1.7129 Validation Accuracy: 0.541200\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     1.7263 Validation Accuracy: 0.544200\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     1.7270 Validation Accuracy: 0.548800\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     1.7045 Validation Accuracy: 0.544800\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     1.7167 Validation Accuracy: 0.539000\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     1.7117 Validation Accuracy: 0.540600\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     1.7254 Validation Accuracy: 0.544200\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     1.7273 Validation Accuracy: 0.547600\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     1.7040 Validation Accuracy: 0.546000\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     1.7163 Validation Accuracy: 0.540200\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     1.7104 Validation Accuracy: 0.541000\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     1.7252 Validation Accuracy: 0.543800\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     1.7273 Validation Accuracy: 0.547800\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     1.7032 Validation Accuracy: 0.546400\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     1.7156 Validation Accuracy: 0.542000\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     1.7093 Validation Accuracy: 0.542000\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     1.7246 Validation Accuracy: 0.542800\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     1.7272 Validation Accuracy: 0.545800\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     1.7024 Validation Accuracy: 0.547600\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     1.7151 Validation Accuracy: 0.543400\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     1.7083 Validation Accuracy: 0.542800\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     1.7249 Validation Accuracy: 0.544200\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     1.7270 Validation Accuracy: 0.545600\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     1.7017 Validation Accuracy: 0.546600\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     1.7139 Validation Accuracy: 0.544000\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     1.7076 Validation Accuracy: 0.544400\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     1.7245 Validation Accuracy: 0.542800\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     1.7266 Validation Accuracy: 0.546800\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     1.7011 Validation Accuracy: 0.547400\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     1.7123 Validation Accuracy: 0.541800\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     1.7076 Validation Accuracy: 0.545600\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     1.7240 Validation Accuracy: 0.542600\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     1.7259 Validation Accuracy: 0.548000\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     1.7007 Validation Accuracy: 0.546000\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     1.7107 Validation Accuracy: 0.541400\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     1.7077 Validation Accuracy: 0.545200\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     1.7237 Validation Accuracy: 0.543800\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     1.7252 Validation Accuracy: 0.547200\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     1.7002 Validation Accuracy: 0.546400\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     1.7090 Validation Accuracy: 0.541000\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     1.7079 Validation Accuracy: 0.544600\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     1.7234 Validation Accuracy: 0.544400\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     1.7243 Validation Accuracy: 0.546800\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     1.7000 Validation Accuracy: 0.545600\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     1.7081 Validation Accuracy: 0.538000\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     1.7071 Validation Accuracy: 0.544800\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     1.7237 Validation Accuracy: 0.543600\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     1.7240 Validation Accuracy: 0.548600\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     1.7001 Validation Accuracy: 0.547200\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     1.7074 Validation Accuracy: 0.540200\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     1.7061 Validation Accuracy: 0.544800\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     1.7243 Validation Accuracy: 0.541200\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     1.7227 Validation Accuracy: 0.549200\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     1.7008 Validation Accuracy: 0.547600\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     1.7069 Validation Accuracy: 0.541000\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     1.7038 Validation Accuracy: 0.544800\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     1.7268 Validation Accuracy: 0.542600\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     1.7212 Validation Accuracy: 0.550400\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     1.7022 Validation Accuracy: 0.545800\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     1.7058 Validation Accuracy: 0.542200\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     1.7020 Validation Accuracy: 0.545400\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     1.7287 Validation Accuracy: 0.540000\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     1.7198 Validation Accuracy: 0.550400\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     1.7043 Validation Accuracy: 0.544400\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     1.7044 Validation Accuracy: 0.544000\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     1.7011 Validation Accuracy: 0.543400\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     1.7298 Validation Accuracy: 0.538800\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     1.7188 Validation Accuracy: 0.549600\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     1.7054 Validation Accuracy: 0.542600\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     1.7037 Validation Accuracy: 0.542600\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     1.7009 Validation Accuracy: 0.544000\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     1.7281 Validation Accuracy: 0.539200\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     1.7186 Validation Accuracy: 0.546400\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     1.7049 Validation Accuracy: 0.543000\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     1.7047 Validation Accuracy: 0.541000\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     1.7009 Validation Accuracy: 0.543600\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     1.7242 Validation Accuracy: 0.541200\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     1.7190 Validation Accuracy: 0.546800\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     1.7029 Validation Accuracy: 0.544800\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     1.7050 Validation Accuracy: 0.541000\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     1.7005 Validation Accuracy: 0.543400\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     1.7214 Validation Accuracy: 0.542800\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     1.7190 Validation Accuracy: 0.545600\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     1.7010 Validation Accuracy: 0.546200\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     1.7049 Validation Accuracy: 0.541600\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     1.7002 Validation Accuracy: 0.542600\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     1.7195 Validation Accuracy: 0.543000\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     1.7191 Validation Accuracy: 0.544400\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     1.6998 Validation Accuracy: 0.548600\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     1.7045 Validation Accuracy: 0.541200\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     1.6999 Validation Accuracy: 0.542400\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     1.7181 Validation Accuracy: 0.543600\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     1.7188 Validation Accuracy: 0.542000\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     1.6990 Validation Accuracy: 0.549600\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     1.7041 Validation Accuracy: 0.541800\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     1.6996 Validation Accuracy: 0.542600\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     1.7170 Validation Accuracy: 0.544000\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     1.7184 Validation Accuracy: 0.541800\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     1.6985 Validation Accuracy: 0.548200\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     1.7038 Validation Accuracy: 0.541400\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     1.6994 Validation Accuracy: 0.543200\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     1.7161 Validation Accuracy: 0.544600\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     1.7179 Validation Accuracy: 0.543400\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     1.6981 Validation Accuracy: 0.548000\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     1.7033 Validation Accuracy: 0.542000\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     1.6994 Validation Accuracy: 0.542400\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     1.7155 Validation Accuracy: 0.545800\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     1.7176 Validation Accuracy: 0.543800\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     1.6977 Validation Accuracy: 0.548200\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     1.7032 Validation Accuracy: 0.541400\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     1.6993 Validation Accuracy: 0.542600\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     1.7148 Validation Accuracy: 0.544800\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     1.7172 Validation Accuracy: 0.543000\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     1.6972 Validation Accuracy: 0.548400\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     1.7028 Validation Accuracy: 0.541600\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     1.6990 Validation Accuracy: 0.542400\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     1.7143 Validation Accuracy: 0.545400\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     1.7171 Validation Accuracy: 0.542600\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     1.6970 Validation Accuracy: 0.548200\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     1.7028 Validation Accuracy: 0.542200\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     1.6988 Validation Accuracy: 0.542600\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     1.7139 Validation Accuracy: 0.548400\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     1.7168 Validation Accuracy: 0.542200\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     1.6966 Validation Accuracy: 0.548000\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     1.7028 Validation Accuracy: 0.541600\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     1.6984 Validation Accuracy: 0.542600\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     1.7135 Validation Accuracy: 0.548000\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     1.7168 Validation Accuracy: 0.542600\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     1.6965 Validation Accuracy: 0.547400\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     1.7030 Validation Accuracy: 0.542200\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     1.6981 Validation Accuracy: 0.543600\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     1.7133 Validation Accuracy: 0.550000\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     1.7170 Validation Accuracy: 0.539800\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     1.6960 Validation Accuracy: 0.546000\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     1.7034 Validation Accuracy: 0.543200\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     1.6975 Validation Accuracy: 0.543600\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     1.7132 Validation Accuracy: 0.549400\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     1.7176 Validation Accuracy: 0.540000\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     1.6959 Validation Accuracy: 0.546000\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     1.7043 Validation Accuracy: 0.543200\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     1.6973 Validation Accuracy: 0.544200\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     1.7133 Validation Accuracy: 0.548000\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     1.7185 Validation Accuracy: 0.538800\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     1.6959 Validation Accuracy: 0.546800\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     1.7055 Validation Accuracy: 0.541000\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     1.6972 Validation Accuracy: 0.544400\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     1.7147 Validation Accuracy: 0.546200\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     1.7194 Validation Accuracy: 0.536600\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     1.6971 Validation Accuracy: 0.545800\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     1.7072 Validation Accuracy: 0.540400\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     1.6983 Validation Accuracy: 0.542400\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     1.7184 Validation Accuracy: 0.545400\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     1.7175 Validation Accuracy: 0.538800\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     1.7018 Validation Accuracy: 0.542400\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     1.7048 Validation Accuracy: 0.542000\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     1.7019 Validation Accuracy: 0.540800\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     1.7204 Validation Accuracy: 0.541400\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     1.7143 Validation Accuracy: 0.539000\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     1.7066 Validation Accuracy: 0.540400\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     1.7069 Validation Accuracy: 0.536400\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     1.7082 Validation Accuracy: 0.536000\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     1.7152 Validation Accuracy: 0.544000\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     1.7133 Validation Accuracy: 0.543600\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     1.7032 Validation Accuracy: 0.538200\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     1.7109 Validation Accuracy: 0.535600\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     1.7071 Validation Accuracy: 0.538800\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     1.7131 Validation Accuracy: 0.546800\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     1.7133 Validation Accuracy: 0.546600\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     1.6996 Validation Accuracy: 0.540200\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     1.7107 Validation Accuracy: 0.535800\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     1.7042 Validation Accuracy: 0.540800\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     1.7129 Validation Accuracy: 0.547400\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     1.7122 Validation Accuracy: 0.546200\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     1.6981 Validation Accuracy: 0.542000\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     1.7099 Validation Accuracy: 0.535600\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     1.7028 Validation Accuracy: 0.539400\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     1.7127 Validation Accuracy: 0.545400\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     1.7110 Validation Accuracy: 0.544400\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     1.6974 Validation Accuracy: 0.544000\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     1.7095 Validation Accuracy: 0.534000\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     1.7021 Validation Accuracy: 0.538400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5407824456691742\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3Xe8XFXV//HPui29E1oChF4E5BEBAYEgNkAFCyBYKOqj\nYsEu9uDz2AsKVhTlEUHAgvwUUaSEJkgHIaGGUJIQSO/JLev3x9oz59yTuXPnJnNLbr7v12tec+fs\nfc7ZM3fKmj1r723ujoiIiIiIQEN/N0BEREREZKBQcCwiIiIikig4FhERERFJFByLiIiIiCQKjkVE\nREREEgXHIiIiIiKJgmMRERERkUTBsYiIiIhIouBYRERERCRRcCwiIiIikig4FhERERFJFByLiIiI\niCQKjkVEREREEgXHIiIiIiKJguN+ZmY7mNlbzOyDZvY5MzvbzD5iZieY2cvNbGR/t7ErZtZgZseZ\n2WVm9oSZLTMzz13+3N9tFBlozGxK4XUyrR51Byozm1q4D6f1d5tERKpp6u8GbI7MbDzwQeB9wA7d\nVO8wsxnALcDVwPXuvqaXm9itdB/+ABzZ322RvmdmFwGndlOtDVgCLADuJZ7Dv3P3pb3bOhERkQ2n\nnuM+ZmZvAGYA/0v3gTHE/2hvIpj+K/C23mtdj/yGHgTG6j3aLDUBWwB7AKcAPwXmmNk0M9MX801I\n4bV7UX+3R0SkN+kDqg+Z2YnA71j/S8ky4D/A88BaYBywPbBnhbr9zsxeARyb2/Q0cA5wN7A8t31V\nX7ZLNgkjgK8Ah5vZ0e6+tr8bJCIikqfguI+Y2c5Eb2s+2H0I+ALwN3dvq7DPSOAI4ATgzcDoPmhq\nLd5SuH2cuz/QLy2RgeLTRJpNXhOwFfBK4EziC1/JkURP8hl90joREZEaKTjuO18DhuRuXwe8yd1X\nd7WDu68g8oyvNrOPAO8lepf72/65v2crMBZggbvPrrD9CeA2Mzsf+C3xJa/kNDM7z93v74sGborS\nY2r93Y6N4e7T2cTvg4hsXgbcT/aDkZkNA96U29QKnFotMC5y9+Xufq67X1f3Bvbclrm/5/ZbK2ST\n4e6rgHcAj+U2G/CB/mmRiIhIZQqO+8bLgGG52/9y9005qMxPL9fab62QTUr6MnhuYfNR/dEWERGR\nriitom9sXbg9py9PbmajgcOAScAEYtDcfODf7v7Mhhyyjs2rCzPbiUj3mAy0ALOBG939hW72m0zk\nxG5H3K95ab/nNqItk4CXADsBY9PmRcAzwO2b+VRm1xdu72xmje7e3pODmNnewF7ANsQgv9nufmkN\n+7UABwNTiF9AOoAXgAfrkR5kZrsCBwLbAmuA54A73b1PX/MV2rUbsB8wkXhOriKe6w8BM9y9ox+b\n1y0z2w54BZHDPop4Pc0FbnH3JXU+105Eh8Z2QCPxXnmbu8/aiGPuTjz+WxOdC23ACuBZ4HHgEXf3\njWy6iNSLu+vSyxfg7YDnLtf00XlfDlwDrCucP395kJhmy6ocZ2qV/bu6TE/7zt7QfQttuChfJ7f9\nCOBGIsgpHmcd8BNgZIXj7QX8rYv9OoA/ApNqfJwbUjt+CjzZzX1rB/4JHFnjsf+vsP8FPfj/f6Ow\n71+q/Z97+Ny6qHDs02rcb1iFx2TLCvXyz5vpue2nEwFd8RhLujnv7sClxBfDrv43zwGfAFo24PE4\nFPh3F8dtI8YO7J/qTimUT6ty3JrrVth3LPA/xJeyas/JF4FfAQd08z+u6VLD+0dNz5W074nA/VXO\n15peT6/owTGn5/afndt+EPHlrdJ7ggN3AAf34DzNwCeJvPvuHrclxHvOa+rx+tRFF1027tLvDdgc\nLsCrCm+Ey4GxvXg+A75d5U2+0mU6MK6L4xU/3Go6Xtp39obuW2hDpw/qtO2jNd7Hu8gFyMRsG6tq\n2G82sF0Nj/cZG3AfHfge0NjNsUcAjxT2O6mGNr228Ng8B0yo43PsokKbTqtxvw0KjonBrFdUeSwr\nBsfEa+GrRBBV6//loVr+77lzfL7G5+E6Iu96SmH7tCrHrrluYb83A4t7+Hy8v5v/cU2XGt4/un2u\nEDPzXNfDc/8AaKjh2NNz+8xO2z5C9U6E/P/wxBrOMZFY+Kanj9+f6/Ua1UUXXTb8orSKvnEP0WPY\nmG6PBH5jZqd4zEhRb78A3lPYto7o+ZhL9Ci9nFigoeQI4GYzO9zdF/dCm+oqzRn9w3TTid6lJ4lg\naD9g51z1lwPnA6eb2ZHA5WQpRY+kyzpiXul9cvvtQG2LnRRz91cDDxM/Wy8jAsLtgX2JlI+STxBB\n29ldHdjdV6b7+m9gaNp8gZnd7e5PVtrHzLYGLiZLf2kHTnH3hd3cj74wqXDbgVra9QNiSsPSPveR\nBdA7ATsWdzAzI3re31UoWk0ELqW8/12I50zp8XoJ8C8zO8Ddq84OY2YfI2aiyWsn/l/PEikA/0Wk\nfzQTAWfxtVlXqU3fZ/30p+eJX4oWAMOJFKR96DyLTr8zs1HATcT/JG8xcGe63oZIs8i3/SziPe2d\nPTzfO4HzcpseInp71xLvI/uTPZbNwEVmdp+7P97F8Qz4E/F/z5tPzGe/gPgyNSYdfxeU4igysPR3\ndL65XIjV7Yq9BHOJBRH2oX4/d59aOEcHEViMLdRrIj6klxbq/67CMYcSPVily3O5+ncUykqXrdO+\nk9PtYmrJp7rYr7xvoQ0XFfYv9Yr9Fdi5Qv0TiSAo/zgcnB5zB/4F7Fdhv6lEsJY/1zHdPOalKfa+\nkc5RsTeY+FLyWWBloV0H1fB//UChTXdT4ed/IlAv9rh9qReez8X/x2k17vffhf2e6KLe7FydfCrE\nxcDkCvWnVNh2duFci9LjOLRC3R2Bqwr1/0H1dKN9WL+38dLi8zf9T04kcptL7cjvM63KOabUWjfV\nfx0RnOf3uQk4pNJ9IYLLNxI/6d9TKNuC7DWZP94f6Pq1W+n/MLUnzxXg14X6y4D3A82FemOIX1+K\nvfbv7+b403N1V5C9T1wJ7FKh/p7AA4VzXF7l+McW6j5ODDyt+Fwifh06DrgM+H29X6u66KJLzy/9\n3oDN5UL0gqwpvGnmLwuJvMQvAa8BRmzAOUYSuWv54368m30OonOw5nST90YX+aDd7NOjD8gK+19U\n4TG7hCo/oxJLblcKqK8DhlTZ7w21fhCm+ltXO16F+gcXngtVj5/br5hW8MMKdb5QqHN9tcdoI57P\nxf9Ht/9P4kvWzMJ+FXOoqZyO840etO8ldE6leJYKgVthHyNyb/PnPLZK/RsLdX9UQ5uKgXHdgmOi\nN3h+sU21/v+BraqU5Y95UQ+fKzW/9omBw/m6q4BDuzn+hwv7rKCLFLFUf3qF/8GPqP5FaCs6p6ms\n6eocxNiDUr1WYMcePFbrfXHTRRdd+v6iqdz6iMdCB+8i3lQrGQ8cQ+RHXgssNrNbzOz9abaJWpxK\n9KaU/N3di1NnFdv1b+DLhc1n1Xi+/jSX6CGqNsr+QqJnvKQ0Sv9dXmXZYnf/K/BobtPUag1x9+er\nHa9C/duBH+c2HW9mtfy0/V4gP2L+o2Z2XOmGmb2SWMa75EXgnd08Rn3CzIYSvb57FIp+XuMh7ge+\n2INTfobsp2oHTvDKi5SUubsTK/nlZyqp+Fows5fQ+XnxGJEmU+34D6d29Zb30XkO8huBj9T6/3f3\n+b3Sqp75aOH2Oe5+W7Ud3P1HxC9IJSPoWerKQ0Qnglc5x3wi6C0ZQqR1VJJfCfJ+d3+q1oa4e1ef\nDyLShxQc9yF3/z3x8+atNVRvJqYY+xkwy8zOTLls1byjcPsrNTbtPCKQKjnGzMbXuG9/ucC7ydd2\n93VA8YP1MnefV8Pxb8j9vWXK462nq3J/t7B+fuV63H0ZcBLxU37Jr81sezObAPyOLK/dgXfXeF/r\nYQszm1K47GJmh5jZZ4AZwNsK+1zi7vfUePwfeI3TvZnZWODk3Kar3f2OWvZNwckFuU1HmtnwClWL\nr7Vvp+dbd35F703l+L7C7aoB30BjZiOA43ObFhMpYbUofnHqSd7xue5ey3ztfyvcfmkN+0zsQTtE\nZIBQcNzH3P0+dz8MOJzo2aw6D28ygehpvCzN07qe1POYX9Z5lrvfWWObWoHf5w9H170iA8W1NdYr\nDlr7Z437PVG43eMPOQujzGzbYuDI+oOlij2qFbn73UTecsk4Iii+iMjvLvmOu/+9p23eCN8Bnipc\nHie+nHyL9QfM3cb6wVw1f+lB3UOJL5clf+jBvgC35P5uIlKPig7O/V2a+q9bqRf3991W7CEzm0ik\nbZTc5Zvesu4H0Hlg2pW1/iKT7uuM3KZ90sC+WtT6OnmkcLur94T8r047mNmHajy+iAwQGiHbT9z9\nFtKHsJntRfQov5z4gNiPyl9cTiRGOld6s92bzjMh/LuHTbqD+Em5ZH/W7ykZSIofVF1ZVrj9aMVa\n3e/XbWqLmTUCryZmVTiACHgrfpmpYFyN9XD3H6RZN0pLkh9SqHIHkXs8EK0mZhn5co29dQDPuPui\nHpzj0MLthekLSa0aC7cr7fuy3N+Pe88WorirB3VrVQzgb6lYa2Dbv3B7Q97D9kp/NxDvo909Dsu8\n9tVKi4v3dPWecBnw8dztH5nZ8cRAw2t8E5gNSGRzp+B4AHD3GUSvxy+h/LPw8cQb7L6F6mea2YXu\nfm9he7EXo+I0Q1UUg8aB/nNgravMtdVpv+aKtRIzO5jIn92nWr0qas0rLzmdmM5s+8L2JcDJ7l5s\nf39oJx7vhURbbwEu7WGgC51TfmoxuXC7J73OlXRKMUr50/n/V8Up9aoo/ipRD8W0n5m9cI7e1h/v\nYTWvVunurYXMtorvCe5+p5n9hM6dDa9Olw4z+w/xy8nN1LCKp4j0PaVVDEDuvsTdLyJ6Pr5aoUpx\n0ApkyxSXFHs+u1P8kKi5J7M/bMQgs7oPTjOz1xODnzY0MIYevhZTgPn1CkWf7G7gWS853d2tcGly\n9wnuvpu7n+TuP9qAwBhi9oGeqHe+/MjC7Xq/1uphQuF2XZdU7iP98R7WW4NVP0z8erOqsL2ByFU+\nk+hhnmdmN5rZ22oYUyIifUTB8QDm4SvEohV5r+6P9sj60sDF39J5MYLZxLK9RxPLFo8lpmgqB45U\nWLSih+edQEz7V/ROM9vcX9dVe/k3wKYYtGwyA/EGo/Te/XVigZrPArez/q9REJ/BU4k89JvMbJs+\na6SIdElpFZuG84lZCkommdkwd1+d21bsKerpz/RjCreVF1ebM+nca3cZcGoNMxfUOlhoPbmV34qr\nzUGs5vdFKv/isLko9k7v5e71TDOo92utHor3udgLuykYdO9haQq4bwPfNrORwIHEXM5HErnx+c/g\nw4C/m9mBPZkaUkTqb3PvYdpUVBp1XvzJsJiXuUsPz7FbN8eTyo7N/b0UeG+NU3ptzNRwHy+c9046\nz3ryZTM7bCOOv6kr5nBuUbHWBkrTveV/8t+5q7pd6OlrsxbFZa737IVz9LZB/R7m7ivc/QZ3P8fd\npxJLYH+RGKRasi9wRn+0T0QyCo43DZXy4or5eA/Ref7bA3t4juLUbbXOP1urwfozb/4D/FZ3X1nj\nfhs0VZ6ZHQB8M7dpMTE7xrvJHuNG4NKUerE5Ks5pXGkqto2VHxC7axpEW6sD6t0Y1r/Pm+KXo+J7\nTk//b/nXVAexcMyA5e4L3P1rrD+l4Rv7oz0iklFwvGnYvXB7RXEBjPQzXP7DZRczK06NVJGZNREB\nVvlw9Hwape4UfyasdYqzgS7/U25NA4hSWsQpPT1RWinxMjrn1J7h7s+4+z+IuYZLJhNTR22ObqDz\nl7ETe+Ect+f+bgDeWstOKR/8hG4r9pC7v0h8QS450Mw2ZoBoUf7121uv3bvonJf75q7mdS8ys33p\nPM/zQ+6+vJ6N60WX0/nxndJP7RCRRMFxHzCzrcxsq404RPFntuld1Lu0cLu4LHRXPkznZWevcfeF\nNe5bq+JI8nqvONdf8nmSxZ91u/Iualz0o+AXxACfkvPd/c+521+g85eaN5rZprAUeF2lPM/843KA\nmdU7IL2kcPszNQZyZ1A5V7weLijc/n4dZ0DIv3575bWbfnXJrxw5nspzuldSzLH/bV0a1QfStIv5\nX5xqScsSkV6k4Lhv7EksAf1NM9uy29o5ZvZW4IOFzcXZK0r+j84fYm8yszO7qFs6/gHEzAp55/Wk\njTWaRedeoSN74Rz94T+5v/c3syOqVTazA4kBlj1iZv9N5x7Q+4BP5+ukD9m30/k58G0zyy9Ysbn4\nKp3TkX7V3f+myMy2MbNjKpW5+8PATblNuwHf7+Z4exGDs3rLhcD83O1XA+fWGiB38wU+P4fwAWlw\nWW8ovvf8T3qP6pKZfRA4LrdpJfFY9Asz+2BasbDW+kfTefrBWhcqEpFeouC47wwnpvR5zsyuNLO3\nVnsDNbM9zewC4Ao6r9h1L+v3EAOQfkb8RGHz+Wb2HTPrNJLbzJrM7HRiOeX8B90V6Sf6ukppH/le\nzalm9kszO8rMdi0sr7wp9SoXlyb+o5m9qVjJzIaZ2ceB64lR+AtqPYGZ7Q38ILdpBXBSpRHtaY7j\n9+Y2tRDLjvdWMDMgufv9xGCnkpHA9WZ2npl1OYDOzMaa2YlmdjkxJd+7q5zmI0B+lb8Pmdklxeev\nmTWknuvpxEDaXpmD2N1XEe3Nfyk4i7jfB1fax8yGmNkbzOyPVF8R8+bc3yOBq83szel9qrg0+sbc\nh5uBi3ObRgD/NLP3pPSvfNtHm9m3gR8VDvPpDZxPu14+CzyTngvHd7WMdXoPfjex/HveJtPrLTJY\naSq3vtdMrH53PICZPQE8QwRLHcSH517AdhX2fQ44odoCGO7+KzM7HDg1bWoAPgV8xMxuB+YR0zwd\nwPqj+Gewfi91PZ1P56V935MuRTcRc39uCn5FzB6xa7o9AbjKzJ4mvsisIX6GPoj4ggQxOv2DxNym\nVZnZcOKXgmG5zR9w9y5XD3P3P5jZz4APpE27Aj8D3lnjfRoU3P0bKVj777SpkQhoP2JmTxFLkC8m\nXpNjicdpSg+O/x8z+yyde4xPAU4yszuAZ4lAcn9iZgKIX08+Ti/lg7v7tWb2KeB7ZPMzHwn8y8zm\nAQ8SKxYOI/LS9yWbo7vSrDglvwQ+CQxNtw9Pl0o2NpXjw8RCGaXVQcek83/LzO4kvlxsDRyca0/J\nZe7+0408fz0MJZ4LpwBuZo8BT5FNL7cN8F+sP/3cn919Y1d0FJGNpOC4bywigt9KU0rtQm1TFl0H\nvK/G1c9OT+f8GNkH1RCqB5y3Asf1Zo+Lu19uZgcRwcGg4O5rU0/xDWQBEMAO6VK0ghiQ9UiNpzif\n+LJU8mt3L+a7VvJx4otIaVDWO8zsenffrAbpufv7zexBYrBi/gvGjtS2EEvVuXLd/dz0BeZ/yF5r\njXT+EljSRnwZvLlCWd2kNs0hAsp8r+U2dH6O9uSYs83sNCKoH9ZN9Y3i7stSCsyf6Jx+NYFYWKcr\nP6by6qH9zYhB1cWB1UWXk3VqiEg/UlpFH3D3B4mejlcRvUx3A+017LqG+IB4g7u/ptZlgdPqTJ8g\npja6lsorM5U8TPwUe3hf/BSZ2nUQ8UF2F9GLtUkPQHH3R4CXET+HdvVYrwB+A+zr7n+v5bhmdjKd\nB2M+QvR81tKmNcTCMfnla883sw0ZCLhJc/cfE4Hwd4E5NezyGPFT/SHu3u0vKWk6rsOJ+aYr6SBe\nh4e6+29qavRGcvcriMGb36VzHnIl84nBfFUDM3e/nBg/cQ6RIjKPznP01o27LwGOInpeH6xStZ1I\nVTrU3T+8EcvK19NxxGN0B53TbirpINp/rLu/XYt/iAwM5j5Yp58d2FJv027psiVZD88yotf3YWBG\nGmS1secaQ3x4TyIGfqwgPhD/XWvALbVJcwsfTvQaDyMe5znALSknVPpZ+oLwUuKXnLHENFpLgCeJ\n11x3wWS1Y+9KfCndhvhyOwe4092f3dh2b0SbjLi/LwEmEqkeK1LbHgZm+gD/IDCz7YnHdSvivXIR\nMJd4XfX7SnhdMbOhwN7Er4NbE499KzFo9gng3n7OjxaRChQci4iIiIgkSqsQEREREUkUHIuIiIiI\nJAqORUREREQSBcciIiIiIomCYxERERGRRMGxiIiIiEii4FhEREREJFFwLCIiIiKSKDgWEREREUkU\nHIuIiIiIJAqORUREREQSBcciIiIiIomCYxERERGRRMGxiIiIiEii4FhEREREJFFwLCIiIiKSKDgW\nEREREUkUHIuIiIiIJAqORUREREQSBcciIiIiIomCYxERERGRRMGxiIiIiEii4FhEREREJFFwPAiZ\n2XQzczM7bQP2PS3tO72exxURERHZFDT1dwN6k5l9DBgLXOTus/u5OSIiIiIywA3q4Bj4GLADMB2Y\n3a8t2XQsBR4FnunvhoiIiIj0tcEeHEsPufuVwJX93Q4RERGR/qCcYxERERGRpM+CYzPbwszONLOr\nzOwRM1tuZivNbIaZfd/Mtq2wz9Q0AGx2leOuN4DMzKaZmRMpFQA3pjpeZbDZzmb2czObZWZrzGyx\nmd1sZu81s8Yuzl0eoGZmo83s22b2pJmtTsf5qpkNzdU/ysz+YWYL0n2/2cwO6+Zx63G7CvuPM7Nz\nc/s/Z2YXmNk2tT6etTKzBjN7l5n908xeNLN1ZjbXzC43s4N6ejwRERGRvtaXaRVnA59Mf7cBy4Ax\nwJ7p8k4ze7W7P1iHc60A5gMTiS8Ai4F1ufJF+cpm9gbg90ApkF0KjAAOS5eTzOx4d1/ZxfnGAXcC\nuwMrgUZgR+BLwH7Am8zsTOBHgKf2DU/Hvs7MXuXutxUPWod2TQDuAnYGVhOP+yTgfcDxZnaEu8/s\nYt8eMbNRwJ+AV6dNDiwHtgFOBN5mZme5+4/qcT4RERGR3tCXaRXPAJ8H9gWGufsEYAjwcuAfRCB7\nqZnZxp7I3b/r7lsDz6ZNb3H3rXOXt5TqmtnOwGVEAHoTsIe7jwVGAe8H1hIB3w+rnPIr6fowdx8J\njCQC0DbgjWb2JeAHwDeBCe4+BpgC3A60AOcWD1indn0p1X8jMDK1bSrwFPF4/97Mmqvs3xO/Se25\nF3gdMDzdz/HAF4F24IdmdmidziciIiJSd30WHLv7ee7+DXf/j7u3pW3t7n4PcBwwA3gJcHhftSn5\nPNEb+yRwjLs/mtq21t0vAD6a6p1hZrt0cYwRwBvc/da07zp3/yURMAJ8Ffitu3/e3ZekOk8DJxM9\nrAeY2fa90K7RwFvd/a/u3pH2vwk4muhJfwlwUjePT7fM7NXA8cQsF69y92vdfU0632J3/xrwZeL5\n9rmNPZ+IiIhIbxkQA/LcfS3wz3Szz3oWUy/1W9PNc919VYVqvwTmAAa8rYtD/d7dn6iw/brc398o\nFqYAubTf3r3QrltKAXvhvI8Cf0g3u9q3J05N179w96Vd1LkkXR9ZS660iIiISH/o0+DYzPYwsx+Z\n2YNmtszMOkqD5ICzUrX1Bub1op2IvGeAGytVSD2u09PNl3VxnP90sf2FdL2GLAgump+ux/VCu6Z3\nsR0iVaPavj1xSLr+opk9X+lC5D5D5FpPqMM5RUREROquzwbkmdnbiTSDUo5rBzHAbG26PZJIIxjR\nV20i8m5L5lSp91yF+nnzutjenq7nu7t3Uyef+1uvdlXbt1TW1b49UZr5YmyN9YfX4ZwiIiIiddcn\nPcdmNhH4BREAXk4Mwhvq7uNKg+TIBqVt9IC8DTS0+yr9YqC2K6/0PHqzu1sNl9n92VgRERGRrvRV\nWsXRRM/wDOAUd7/H3VsLdbaqsF9buq4WII6pUtadF3N/FwfE5U2uUL831atd1VJUSmX1uE+l1JBq\nbRUREREZ8PoqOC4FcQ+WZk3ISwPQXlVhvyXpeksza+ni2AdUOW/pXF31Rs/KnePIShXMrIGY/gxi\nmrK+UK92HVHlHKWyetyn29P10XU4loiIiEi/6avguDSDwd5dzGP8PmKhiqLHiJxkI+bq7SRNYfbW\n4vacZem6Yi5sygP+U7p5lplVyoV9L7FwhhMLcvS6OrbrCDM7pLjRzHYlm6WiHvfponT9OjN7fbWK\nZjauWrmIiIhIf+qr4Pg6IojbGzjPzMYCpCWXPw38GFhY3Mnd1wFXpZvnmtkr0xLFDWb2WmL6t9VV\nzvtwuj45v4xzwdeJVe22Ba42s91T24aY2fuA81K9C939yRrvbz3Uo13LgD+Z2TGlLyVpuepriAVY\nHgau2NiGuvvfiWDegCvN7NMpz5x0zvFmdryZ/T/g+xt7PhEREZHe0ifBcZpX9wfp5oeBxWa2mFjW\n+dvA9cDPutj9c0TgvB1wC7Ek8UpiVb0lwLQqp74wXZ8ALDWzZ81stpldlmvbk8RiHGuINIVHUtuW\nAxcQQeT1wMdqv8cbr07t+h9iqeqrgZVmthy4meilfxE4sULu94Z6N/BnIj/828B8M1tsZsuI/9+V\nVOj9FxERERlI+nKFvE8A/w3cR6RKNKa/PwYcSzb4rrjfLOAg4HdEQNdITGH2NWLBkGWV9kv73gC8\nmZjTdzWRhrADsHWh3l+AfYgZNWYTU42tAm5NbX6du6/s8Z3eSHVo10LgQOKLyXxiqeq56Xj7ufuM\nOrZ1pbu/GXgD0Ys8N7W3mZjj+QrgdOAj9TqniIiISL1Z19PvioiIiIhsXgbE8tEiIiIiIgOBgmMR\nERERkUTBsYiIiIhIouBYRERERCRRcCwiIiIikig4FhERERFJFByLiIiIiCQKjkVEREREEgXHIiIi\nIiJJU383QERkMDKzp4DRxNLvIiLSc1OAZe6+Y1+edNAGx68/+k0OsOC5p8vbjnvTawCY8Z+7AXj/\nh75ULtt2u0kANDVFZ/roMaPKZWPGTwTgmWeeAeAvf/lLueyqq/5fHHPGwwAsW7a0XNbS0gLA0CHN\nADQ0NJfLRowZCsCQ5sbytjVrWuN8I8bEbe8olzViAKxbtw4A87Zy2cgJWwBw6sknAXDYQYeWy66/\n6aao3xD3a9z4CeWyhU/HY/OpadMMEam30cOGDRu/5557ju/vhoiIbIpmzpzJ6tWr+/y8gzY4fuC+\nBwHYd5+fo2cCAAAgAElEQVR9ytte+vIDAbjrlukAWEcWYK5ZswaAlpYIVleuyI41f8FiAEaOGg3A\naaedXi47+eSTAXj00UcBuP/++8plN998MwAzH5kBwAtzF5TLbGmcYMjoYeVtLy6LNowcEYH51qOG\nZvWHjgWgrTWeJMvmzi6XrV62HIDnn58PwE233VEuW74y6o8bE20/6ID9y2U3PD8PkYHKzBy4yd2n\n1lh/KnAjcI67T8ttnw4c4e59/SVw9p577jn+nnvu6ePTiogMDvvvvz/33nvv7L4+r3KORQYJM/MU\nCIqIiMgGGrQ9xyKy2bkT2BNY0F3FvvLQnKVMOfvq/m5Gv5r9zWP7uwkiIj0yaIPjIUMi3/fFxUvK\n21avixzexuYRAPzm0ovLZe9456kAbDEx0gObU74wwJgxkQNcynuZP39VuWzRokUAjE11Ts+lXLzj\nHe8A4OmU23vv3XeXy27565Wx/8JF5W3NYyLnuKMj2tm8NjvPrHkLAfD2SAXZbnSWcrG2MXKZH7gz\njr/djruUy5YtehGAdo90kd132b5ctsPuuyIyWLj7KuCR/m6HiIhs2pRWIdJHzOw0M/ujmc0ys9Vm\ntszMbjOzd1aoO9vMZndxnGkphWJq7rieio9IZaXLtMK+J5rZzWa2NLXhP2b2OTMb0lUbzGykmZ1r\nZs+mfe43s+NTnSYz+4KZPW5ma8zsSTP7cBftbjCzD5jZXWa2wsxWpr8/aGZdvheZ2bZmdrGZvZDO\nf4+ZnVKh3tRK97kaM3udmf3NzBaY2drU/u+Y2dhajyEiIoPLoO05nrTTbgBsM2Wv8rar/n4DAAvS\naLtrfn9FuWyPl7wUgDe+8ejYYNnYHSfiji22iF5la8xmmNh28rZxzAXxS25bR3u5bMKECZ2u5z6f\nzZwxfspkAPbc76XlbWPTDBmHHHYUABf99EflsrcfcggAixdFb/K9t95YLnthcfQqH5iONXbipHLZ\nrdfOjHYNjzb869+3l8sOfsURSJ/6KfAwcDMwD5gAHANcbGa7u/uXqu1cxf3AOcBXgKeBi3Jl00t/\nmNnXgc8RaQeXAiuAo4GvA68zs9e6+7rCsZuBfwLjgauAFuBk4I9m9lrgTOAg4BpgLXACcL6Zveju\nlxeOdTFwCvAs8EvAgTcDPwFeCbyjwn0bB/wLWAL8GhgLnAhcYmaT3P073T46XTCzrwDTgEXAX4EX\ngH2BTwHHmNnB7r5sQ48vIiKbpkEbHIsMQHu7+5P5DWbWQgSWZ5vZz9x9Tk8P6u73A/enYG92fqaG\n3HkOJgLjZ4ED3f35tP1zwJXAG4ig8OuFXbcF7gWmuvvatM/FRID/e+DJdL+WpLLvE6kNZwPl4NjM\nTiYC4/uAw919Rdr+ReAm4BQzu9rdLy2cf990nre7x9yGZvZN4B7ga2b2R3ef1bNHDMzsSCIwvh04\nptT+VHYaEYifA3y8hmN1NR3FHj1tl4iI9L9BGxwvWBTzDb/1XVPL24aOjt7gy+ZG/LHdTlkn2e/T\nfMXPPv88AG0rFpfLXn3MMQCU+oS32Hrrctm990ae77KlUd+zjmOamyMveNiw+MV6zrPPlcvWtEXv\n87IVWcfUiDTn8bjUQ33k0W8pl+20e8x//fSsxwH46zXZfjOeeAqAXfaJnuMjjj2wXPb0Ew8BML8t\n2vDP2+4vl61dG+c7+g1vRHpfMTBO29aZ2Y+BVwFHAb/ppdOfka7/txQYp/O3mdkniR7s97J+cAzw\nsVJgnPa5JS1wsSPw2Xxg6e6zzOw24JVm1uhefkWUzn92KTBO9Vea2WeB69L5i8FxezpHR26fp8zs\nPKKn/F1EENtTH03X78u3Px3/IjM7i+jJ7jY4FhGRwWXQBsciA42ZbQ98lgiCtweGFapMWm+n+nlZ\nur6hWODuj5nZc8COZjbG3ZfmipdUCuqBuURwXKnXdA7x3rJ1+rt0/g5yaR45NxFB8H9VKHvG3Z+q\nsH06ERxX2qcWBwOtwAlmdkKF8hZgoplNcPeF1Q7k7vtX2p56lF9WqUxERAYuBccifcDMdiKmGhsH\n3AJcCywlgsIpwKnAeoPi6mhMuu5q5Zd5RMA+NrWrZGnl6rQBFALpTmVEvnL+/Isq5DSXeq8XAFtW\nONb8Ls5f6v0e00V5dyYQ739f6abeSKBqcCwiIoPLoA2OGzpitbnbb72uvG3YmJie7dmnnwCg2bLP\n7jUro/6f/3wVALvvOLlc5tdeC8DDM2Jw2xve+KZy2f/9+kIAGtOy0x25CUBKC3JN2iGmVluRW1ra\nW2Ng3ahRWefh1ltGusboG26N9o7Mlnq+6bZI3/j3TX8FYOHC7FgjR0Z8cO/9MYvV3vv+p1y2274v\nB2DeHXfG9fMvlsseeVyzXvWhTxAB2enuflG+IOXjnlqo30H0XlayITMplJ4wWxN5wkXbFOrV21Jg\nvJk1u3trvsDMmoAtgEqD37bq4nil3KYNbe9SoMHdtbSziIh0MmiDY5EBpjT59B8rlFWaNmQxsG+l\nYBJ4eRfn6AAauyi7j/iJfyqF4NjMdgEmA08V82/r6D4ineRw4PpC2eFEu++tsN/2ZjbF3WcXtk/N\nHXdD3AEca2YvcfeHN/AY3dp70hju0SIYIiKblEEbHE/eKjrXnngk+7xdlRbVWLUseom9fWW5bMyo\nGCM0aXL0GE896qhy2cJFMU3b0iXRSdXYlD1slqZ8a20tjTvyctk220Rn3Hve814Abr3llnLZA3dF\n7/CadWvK20pTsj38YPQSb73dDuWylQvj1/BZL8SiIYuWZL/0Dh8Rvc/Dho0GYObMGeWykcPHAbBg\n3rMArFtTHlfFY4+p57gPzU7XU4G/lDaa2euIgWhFdxLB7OnABbn6pwGHdnGOhcB2XZT9CngP8EUz\n+3/u/mI6XiPwXWLO8wtruicb5ldEcPwNM5uaFuzAzIYD30x1Kp2/EfiWmZ2cm61iR2JAXRvw2w1s\nz7nAscAvzOxt7j43X2hmI4B93P2ODTy+iIhsogZtcCwywPyECHR/b2Z/IAa07Q28HrgCOKlQ//xU\n/6dmdhQxBdt+xECyvxJTrxVdD7zdzP5C9MK2Aje7+83u/i8z+zbwGeCh1IaVxDzHewO3Ahs8Z3B3\n3P1SMzuOmKP4YTP7M/FN8nhiYN/l7n5JhV0fJOZRvsfMriWb53gs8JkuBgvW0p7rzexs4BvA42b2\nN+ApIsd4B6I3/1bi/yMiIpsRBccifcDdH0xz6/4v0WPZBDwAvIVY4OKkQv0ZZvZqYmq1NxK9pLcQ\nwfFbqBwcn0UEnEcRU7M1ENOc3ZyO+Vkzuw/4MPBuYsDck8AXge9VGixXZycTM1OcAbw/bZsJfI9Y\nIKWSxUQA/23iy8JoYAbw3QpzIveIu38rTTv3UWIRkuOIXOQ5RG/9Rh1fREQ2TYM2OB45PO5ae241\nu0kTI8Xg6dYYTD90WDYgb8fJE+OPllil7oUly8tly1dEOsXKVZGGsa61rVw2dkwMhnt8Vsw2NWL4\niHLZ4sUx9/GMhx6M2wuzwXDLl8fYoy23zMZWta2L2OSJZ2LA4E3/vrNctue20a5tt47ZvtrWlqd9\n5dlnZgMwfETUWb4sS9VYvjDa9cILMeh/VW5eZVtTTGWV3uTu/yLmM67Eihvc/VYiH7foQWIBi2L9\nF4iFNqq14TLgsu7amupOqVI2tUrZacBpFbZ3ED3oP6nx/PnHZL0ltivUn07lx3FqlX1uJXqIRURE\nAHJTK4iIiIiIbOYGbc/x0uUx+M5ahpa3NVoMRhs3Pnp3W9uywXOrV0b9gw+YCsB2221TLrvxpn8C\n0JAG4q1clvW+jh0dg+BKA93aO7Jjehqsd911MRXc+HHZrFHbT9kJgMnbZ+d5fn4Mslu4IFbSa129\nulz2n+fie8yZH4yR7y/Mz6Z//cUFP0/njh7ttvZsutwRTbGtoSH9qzuy3uIxw4cjIiIiIhn1HIuI\niIiIJIO25/htJ74HgKambB2FZaujd3jWE48DsHTJ4nLZiKbIJ37yyShbNP/pclnrqpimbe999wNg\n9732yo6Zcoen7Bbbxk7IFu4YMy5ynMeNj21j022Ada3R07x6TZYf3NYW52ldHdvWrllVLps7J6Zy\nW7A48pZbRmS9wx/80AcAePqpmK5t4sRsCrgZD9we9VvicfD2LFd54phRiIiIiEhGPcciIiIiIomC\nYxERERGRZNCmVZzxnvVntGpO07q1rotBaq0d7eWyjrZIc1iyIlIZ/vfr/1sus5ZYge7D7/0gAFN2\n2blcdtjhRwKwrj0Guq3MDaJbuy6O2ZFmpFq3LptGtr092tCea0Nbewzma0yzUblng/t22j1SLVak\nqdja27P9XpwXKRcdKVVj4fNPZXe6I/YrreS3enWWqjF/0VJEREREJKOeYxERERGRZND2HH/tW+cC\nsP/+Ly1v2//l+wLQmAbpNTdl3w2a0zRtDY3RW7vjttuWy958Qixetv2UGOj2yBNPlMta0n6NLXHt\nZAPeSD2/HamXt7kpW5CkdL587zCpd7cjTQfX2pr1NHtjtHX4yJiGrq0tO88eu+8JwGFTjwDg+qv/\nUS677vq/AbBgSfQSjxyVTd/WMnwkIiIiIpJRz7GIiIiISDJoe47piNzapx9/tLxp4bw5ADQPST3H\nQ7Plo5uGxGIh7WsjZ7ijLeu1ffyJOMbTz80GYMzYbLq20eNjYY+WIfFQThyfLfThpanZ2iK/2HLf\nRUrbmpqyNpA6kVesiTY0N2c9zaNHRS+vpfzlxqZsKrd1Kbe5dP2Od7+jXPboYzE13ZV/ihWDd98t\ny5febeddEREREZGMeo5FRERERBIFxyIiIiIiyaBNq1j4/HwAlsx/sbytsSW+C7SkFIqWIVlKQ2Nz\nbGtOA9+ahg0tl818ZGYqizSHiVtmg/WWjB0LwEv/K1bPeyFNqwYwclikQsxJq9t1pCnagGzwXS59\nY8eddgRgu223BuD5uS+Uy266/kYA5qf71dSUte+B++9N534OgOEjhpXLRoyJFJBhI6Ity1csL5fl\nV+cTEREREfUci8gmwsymm5l3X7PTPm5m03upSSIiMggN2p7jufPmAtDUnPUONzREz+/QIdGzOmRI\ndvebmps61W/KTbvWksoam2Mg39+uuaZc9uyspwG48MJfAPD8/LnlsrWrYtDd4iXRW/vk7GfKZR3p\na8nihQvK20aOimna3n7SiQDce9eMctkvf3F+/JE6n0eNHZftNzJ6hUu90E/Myqaa222PmOZt+LA4\n9vLlWc/x8y9kPdMiIiIiMoiDYxERYE9gVbe1eslDc5Yy5eyr++v0dTX7m8f2dxNERPqEgmMRGbTc\n/ZH+boOIiGxaBm1wvHpVpBg0NreXtw0fEYPTVqxZFnVas4FrjWkgXmlq4aam7KFpTqkWpdXsnnry\nqXJZad7ha6+NVIvtJ08ql7WtizaMSoPhli56PmtgQ5xv7erV5U0LX4jBdjNnRjrFOlaWy4aNiQF4\nDSmvYtLkrctlHekurl0b8xy3rFhWLmttjzbssvMu0fanHi+XPfXU04gMBGb2JuAsYC9gPLAQeBy4\n3N1/UqjbBHwGOB3YHngBuBT4kruvK9R14CZ3n5rbNg34CnAksAPwMWAPYDnwV+Dz7v48IiKyWRq0\nwbGIbBrM7L+BnwPPA38BFgBbAvsSAfBPCrtcChwGXAMsA44hguUtU/1afRx4LXA58HfglWn/qWZ2\nkLu/WG3nXPvv6aJojx60RUREBohBGxyvXhXTlOVXwRs/PnpfV6yKnlX3rFe5La1Y15F6jltbW8tl\nK1asSPWj57g0uA1gt113B2Dmo9EjO3LUmOyYrdGJtfXwUQDsvtde5bI1rXG+RQuyAXnLR8Zgufnz\notOqpTGb+m1omkau1K5Zj2Ur/40fvyUAlup0eEe5rNSbPGxorKi3aOHCcllLSzYdnEg/ej+wDnip\nu3caJWpmW1SovzPwEndflOp8AXgAeLeZfa4Hvb5HAwe5+325851L9CR/E3hPj++JiIhs8jSVm4gM\nBG1Aa3Gjuy+oUPezpcA41VkJXEK8n728B+e8OB8YJ9OApcApZjZk/V3W5+77V7oAyncWEdkEDdqe\n47Wtkcu7rm1tedtjjz4IZDnEzS3ZdG2NjWkqt6bO1wAtLTGF25o1ccxtt8kWATno4IMBuOGGfwCw\naHHWMzssTRlnFud51VFHlMvGbxG9ve1ZJy8NKQ+5LfVQT7/hunLZhOborV7aFgPv1zVlvcoLliyO\n+2Cx/5hRI8tl1hH1lq9YCsDoMaPKZbvuql99ZUC4BPgeMMPMLgNuAm6rktZwd4Vtz6brcRXKunJT\ncYO7LzWz+4EjiJku7u/B8UREZBBQz7GI9Ct3/z5wKvA08FHgSmC+md1oZuv1BLv7kgqHaUvXjRXK\nujK/i+2ltIwxXZSLiMggpuBYRPqdu//G3V8BTACOBS4EDgf+YWYTe+m0W3WxvTQVzNJeOq+IiAxg\ngzatoq0tBts1NWSrzWar3kWqQUcup6Ex1fOOuG5vbyuXtbZ6uk6zRFnWOTV8eKQ7tKUBdgtzA+yG\nDUmr0i2LKdmWLss+a7fbfgcAFi3Kpl1bsyZSQMqr9TVl310aJ4yNbctj27iOrH2lgYKl+9CRG0y4\nYEH8Mj06rb43bnz2q3NDz1biFel1qVf4b8DfzKwBOIMIkv/YC6c7AvhNfoOZjQH2A9YAMzf2BHtP\nGsM9WjxDRGSTop5jEelXZnakmVmFoi3TdW+tcPcuM/uvwrZpRDrF79x97fq7iIjIYDdoe45Xr4ne\n2pbmlvK2JksD8dIUaaVBeADWlHpf08xvTU3Zfg1poNuQYaXP7+xz/MknnwCgPa3E8fz8bEDexC2i\n3qoXozf52TlzymX33ReDA0tTyAF0tKfe7jQYcMjQrA2L0xRsS9Pguy23HF8uK93HxpbYz9uyKepW\nrIr9SgMFhw7JTd+Wm8pOpB9dCawwszuA2cQL7DDgAOAe4Lqud90o1wC3mdkVwDxinuNXpjac3Uvn\nFBGRAU49xyLS384G7gJeBpxJLMTRDHwWONLd15virU7OTefbj2yVvIuAQ4rzLYuIyOZj0PYcf/Ss\nzwPQ2JQtAlLqKW5OubylKdqAfGcwALmZ3Mq5yqtWxWIgf/rjFeWy2U9Fz/HSpTGAfvWq7BfgEcNj\nKreG1FNdyg2Ov1vTddZ7W05ltsiFXrM2O1ZDOXc6rocMzZa+Xrc2ep9fWBhTv86fNzc7T6lOW+RL\nT5qUTUM3ZqwG40v/c/efAT+rod7UKmUXEYFtcXuldI1u9xMRkc2Xeo5FRERERBIFxyIiIiIiyaBN\nq7jkkl8D0NKSrQA7YngMRhs9ejQALyxYlNsjEhBKq9StWb26XLJ4cdRra4tUiMnbZtOjHvLKQwFY\nkgbKvfhClqq4PE3d9vQzT6ct2S+8Q4ZEu/Ir8TU2Rl5FU0r/GDosGzxXate8+bFuwZy588pl7WlK\nutL0da25QX7bT9kegG3HbxF1W7NUjXnzsmOIiIiIiHqORWQz4+7T3N3cfXp/t0VERAaeQdtzPOOh\newAYOXJ4edvY0dET2zwkti1fmQ2Cb2mJgXvj0yIZ7a3ZAiErl8dgu1mzZgGwzZZblMvGj40p1Xbb\nbQ8Anpr1VLms1Nu7dGks9LEqN1iv1Mu7evWa8jZriJ5lSz3MW2wxoVy27777RrvaSguSZG0fPXpU\n3IfmuA+jxmbTvO37spjGdemSmNruxblPl8uaWgbtv19ERERkg6jnWEREREQkUXAsIiIiIpIM2t/V\nt99hJwCGD8vSKlYsj0Fza9dEykRzYzbPceva9k5lY8aOLpdNSnMll1bBmzU7S5246+67AHgxrYK3\nbl224mxpfmNL6RXLlmeD/IYMiXO3tWaD57IBe5E6sWzZkqx9aTDgPvvunfbL0j5KA/nSaVizLku5\nWLIojjE/Db5rasq+D3V0ZPMui4iIiIh6jkVEREREygZtz/GIkSMAWLUyP+AteoBHjohp1BYtyqZy\nK02H5qSe3Mbse8Po8THAbedhsd+ShQvLZeMnRNl9998X58sNumtMPdNvfetbABgydGS5bMnS6MWe\n/+L88rbW1ljFrq0t2jI2N7Buzz32AuDfd9yezpO1ffmi5QAsWPBi3PcRI8pla9dGb3Vz6v1uaMj9\ny9VxLCIiItKJeo5FRERERJJB23PcnKY1GzIk1z3qsW348NLiGtmiHE3Nkbc7clRMizZkaLYAR2nb\nsKGp99WzY65cGVOktaY83zVrspzju+6K6eReeHEFAAcd9Mpy2W67xhRru+zeXt7W2BRt6Cgv6pEd\n6767o2f6X7feBsCqlcvLZR0dKf84NWtVbvq6Uk/4tttOjvs1ZNj6+4mIiIgIoJ5jEREREZEyBcci\nstkxsylm5mZ2UX+3RUREBpZBm1YxtCUGz7WtzdIWWtOqdKWsiNIUaABm8featTEoznIPTXv5WJE6\nUUrZALj99hggN2vWk1Ent3Ldc8892+n6tluvyxpYnrUtS+0obTRL2yyX9tAR28rTw+V283SHSivy\nDR+epVVMnLgVACNGxGDAIS1ZWsW6desQ6S1mNgV4Cvg/dz+tXxsjIiJSo0EbHIuI9LeH5ixlytlX\n93czNtrsbx7b300QEekzgz44tlwXa3NTTK3WlKY1K10DNKfe4ea0OEfLkGyBkJbh0dvakAbtNa/L\n9tt1110B8LRAyMIFC8pl69IgvdKgPe/Islic9vJf5W2FqdXMclkvpRU+Um9yc67to0fHgiUTt9wS\ngAlbTCyXDU2LoFg6eH4QnhYBEREREelMOcciUndmNo1IqQA4NeX3li6nmdnU9Pc0MzvQzK42s0Vp\n25R0DDez6V0c/6J83ULZgWZ2uZnNMbO1ZjbPzK41sxNraHeDmf0wHftPZjasu31ERGRwGbQ9x6Ve\n2Iam7C62DIne4ZGjYpGMpuasrJRHXMrfNfLLLMf12rTUs+d6XMdP2AKAHXeM5aonbLFFuaw0vdvC\nBYvSdbZ4yNq0zLR7fjq1zjnH+aWeSwt7jErLWo8ZO7ZcNnLkmLh/jek+5I5Y+tvSoib5zuh8r7pI\nnU0HxgJnAQ8Af86V3Z/KAA4GPgfcCvwK2ALY4GR4M3sf8FOgHfh/wOPAlsDLgTOBK6rsOxS4BHgL\n8GPgo975BSoiIpuBQRsci0j/cffpZjabCI7vd/dp+XIzm5r+fC3wAXf/+cae08z2An4CLAMOc/eH\nC+WTq+w7ngimDwHOdvdv9eC893RRtEetxxARkYFDwbGI9Kf76xEYJx8k3tP+pxgYA7j7c5V2MrMd\ngL8DOwPvcvdL6tQeERHZBA3a4HhdmlLNGrI8gqaWGGQ3bHhKq8ilXJR+PS2lVXS0Z8kJpfSI0tRv\nQ4ZlU6WV0jGaymkZWRua0/m232E7ACZPnlQua2trS3+t/6ttllaRDbprSqvnlQbmtVfInWhrb1tv\nP2vonKrhnh8AqAF50u/urOOxXpGur+nBPrsDtwMjgKPd/fqentTd96+0PfUov6ynxxMRkf6lAXki\n0p+er+OxSnnMc3qwz27ANsAs4N46tkVERDZRg7bnuL09emQ7LfRRWnkjdZjmp0praGjsVD/fq9ra\nGuODGtMAuazXF1auXNVpvyFp0F++XqktDbkBcC2pV9ksO09pEY9SW2jIDwrs6HTdmCsrDbYrbcvP\n0Jb1GHeUNiAygFT7+cLp+j1qbIVtS9L1JOCRGs//F+BR4OvA9Wb2Gndf2M0+IiIyiA3a4FhE+l1p\nMu/GqrW6thjYrrjRYjnL/SrUv4OYleJoag+OcfdvmNlq4Fxgupm92t3nb1iTO9t70hju0QIaIiKb\nFKVViEhvWUz0/m6/gfvfCWxvZq8tbP8isEOF+j8F2oAvpZkrOqk2W4W7/4AY0PcS4CYz23YD2ywi\nIpu4wdtz7KWr7Ffb0uC00nzC5fQFoKGxtAJdGsCWm960lCqxdt1qAFrTYD8gmwQ5pWEMHTo0VxRl\n7W3RgdaW26+9PbY1Ntp629rTfg2N2b+nNHiwlApSHqCX3R28nHKRlbk15u8WnVfk04A86T3uvsLM\n/g0cZmaXAI+RzT9ci+8CrwOuMrPLgUXEVGs7EvMoTy2cb4aZnQn8DLjPzK4i5jmeABxATPF2ZJX2\n/szM1gAXAjeb2avc/Zka2yoiIoPE4A2ORWQgeBeRrvB64GTiq9xzwOzudnT3683seODLwNuBlcA/\ngZOAc7rY5xdm9hDwKSJ4Ph5YADwI/LKGc15kZmuB35AFyLO6268LU2bOnMn++1eczEJERLoxc+ZM\ngCl9fV5T76GISP2lILuRWCFQZCAqLVRTc46+SB97KdDu7kO6rVlH6jkWEekdD0HX8yCL9LfS6o56\njspAVWUF0l6lAXkiIiIiIomCYxERERGRRMGxiIiIiEii4FhEREREJFFwLCIiIiKSaCo3EREREZFE\nPcciIiIiIomCYxERERGRRMGxiIiIiEii4FhEREREJFFwLCIiIiKSKDgWEREREUkUHIuIiIiIJAqO\nRUREREQSBcciIjUws8lm9iszm2tma81stpn9wMzG9fA449N+s9Nx5qbjTu6ttsvmoR7PUTObbmZe\n5TK0N++DDF5m9jYzO9/MbjGzZen59NsNPFZd3o+70lSPg4iIDGZmtjPwL2BL4CrgEeBA4Czg9WZ2\nqLsvrOE4E9JxdgNuAC4D9gBOB441s4PdfVbv3AsZzOr1HM05p4vtbRvVUNmcfRF4KbACeI547+ux\nXniur0fBsYhI935CvBF/1N3PL200s+8DHwe+BnyghuN8nQiMv+/un8wd56PAD9N5Xl/Hdsvmo17P\nUQDcfVq9GyibvY8TQfETwBHAjRt4nLo+1ysxd9+Y/UVEBrXUS/EEMBvY2d07cmWjgHmAAVu6+8oq\nxxkJvAB0ANu4+/JcWQMwC9ghnUO9x1Kzej1HU/3pwBHubr3WYNnsmdlUIji+xN3f2YP96vZcr0Y5\nx+rCoCMAACAASURBVCIi1R2Zrq/NvxEDpAD3NmA48IpujvMKYBhwWz4wTsfpAP5ROJ9Irer1HC0z\ns5PM7Gwz+4SZHW1mQ+rXXJENVvfneiUKjkVEqts9XT/WRfnj6Xq3PjqOSFFvPLcuA74BfA/4G/CM\nmb1tw5onUjd98j6q4FhEpLox6XppF+Wl7WP76DgiRfV8bl0FvBGYTPzSsQcRJI8FLjcz5cRLf+qT\n91ENyBMREREA3P3cwqZHgc+b2VzgfCJQ/nufN0ykD6nnWESkulJPxJguykvbl/TRcUSK+uK59Uti\nGrf90sAnkf7QJ++jCo5FRKp7NF13lcO2a7ruKgeu3scRKer155a7rwFKA0lHbOhxRDZSn7yPKjgW\nEamuNBfna9OUa2WpB+1QYBVwRzfHuQNYDRxa7HlLx31t4XwitarXc7RLZrY7MI4IkBds6HFENlKv\nP9dBwbGISFXu/iRwLTAF+FCh+ByiF+3i/JyaZraHmXVa/cndVwAXp/rTCsf5cDr+PzTHsfRUvZ6j\nZrajmY0vHt/MJgK/Tjcvc3etkie9ysya03N05/z2DXmub9D5tQiIiEh1FZYrnQkcRMy5+RhwSH65\nUjNzgOJCChWWj74T2BM4jlgg5JD05i/SI/V4jprZacDPgFuJRWkWAdsDxxC5nHcDr3F35cVLj5nZ\n8cDx6ebWwOuI59ktadsCd/9UqjsFeAp42t2nFI7To+f6BrVVwbGISPfMbDvgq8TyzhOIlZiuBM5x\n98WFuhWD41Q2HvgK8SGxDbAQuAb4srs/15v3QQa3jX2Omtk+wCeB/YFtgdFEGsXDwBXAz919Xe/f\nExmMzGwa8d7XlXIgXC04TuU1P9c3qK0KjkVEREREgnKORUREREQSBcciIiIiIomCYxERERGRRMtH\nD1Bp1PAU4M/ufn//tkZERERk86DgeOA6DTgCmA0oOBYRERHpA0qrEBERERFJFByLiIiIiCQKjjeA\nme1pZj8zs8fMbJWZLTGz/5jZeWa2f67eEDM7wcx+Y2YPmNkCM1tjZk+b2SX5url9TkuTsx+RNv3a\nzDx3md1Hd1NERERks6NFQHrIzD4CnAs0pk0rgVZgbLp9k7tPTXXfAPwlbXdgCTAMGJq2tQFnuPvF\nueOfBPwQGA80A8uA1bkmPOvuB9T3XomIiIgIqOe4R8zsBOA8IjD+A7CXu49093HE8oXvBO7J7bIi\n1T8cGOnu4919GLAD8ANiQOQFZrZ9aQd3v9zdtybWDQc4y923zl0UGIuIiIj0EvUc18jMmol1vicB\nv3P3U+pwzAuBM4Bp7n5OoWw6kVpxurtftLHnEhEREZHuqee4dkcRgXE78Ok6HbOUcnFonY4nIiIi\nIhtB8xzX7hXp+gF3n1PrTmY2HvgQcDSwOzCGLF+5ZNu6tFBERERENoqC49ptla6fqXUHM9sLuCG3\nL8ByYoCdAy3AOGBEndooIiIiIhtBaRW969dEYHwv8HpglLuPdvet0qC7E1I9668GioiIiEhGPce1\nm5+ud6ilcpqB4kAiR/lNXaRibFVhm4iIiIj0E/Uc1+6OdL2vmU2qof7kdP1ilRzlV1fZvyNdq1dZ\nREREpI8oOK7d9cAcYjDdd2qovzRdb2VmWxYLzWwfoNp0cMvS9dgqdURERESkjhQc18jdW4FPppsn\nm9kVZrZHqdzMxpvZ+8zsvLRpJvAc0fN7uZntkuo1m9lbgH8Si4R05eF0/RYzG1PP+yIiIiIilWkR\nkB4ys08QPcelLxYriGWgKy0f/WZiJb1S3eXAEGKWimeALwAX8//Zu/P4uq7y3v+fR/NgDZY8JXYc\nOwM4ECADGQihScolDLlQSuFSaIHQubSXsRfCdDEdgLaU0PKC0BYoP8LccoECAQIBh5CQpjgTSZw5\ncjwkHjXPOuf5/bHWHnR8JEuyZNlH3/frldeR9tp77bXlE2np0bOeBdvdfUPJfTYBd8VzJ4C9hG2q\nd7r7xQvwaCIiIiJLniLHs+TuHwPOJlSi6AJqCWXZ7gb+EXhb7txvAr9OiBL3x3O3Ax+Nfeyc5j73\nAy8AfkBI0VhDWAy4bqprREREROTIKHIsIiIiIhIpciwiIiIiEmlyLCIiIiISaXIsIiIiIhJpciwi\nIiIiEmlyLCIiIiISaXIsIiIiIhJpciwiIiIiEmlyLCIiIiISaXIsIiIiIhLVLPYAREQqkZk9BrQS\ntpkXEZHZ2wD0ufvGo3nTip0c3/qL2x2gqbkuPWYWt8pOtsy2LHA+NjYKQF1dOL+trS1tq60Nx6qr\nqwGoqrK0raqqKnZ5+G248+dkH2bHikWfdKxQKOTGNwbAgQMHADh4oDttW7asFYD6hvowzqrqbOx1\ntQCMj48c0ufoaHjm5zznOdkDich8aW1sbOw444wzOhZ7ICIix6Nt27YxPDx81O9bsZPjYnECgNGR\nYnosmQvX1IbJY2N9NnGuqQmNvb29ABQKE2lbZ2cnAO7J5DibVCcfV+UmpOn9rHTO6bm20iNAnLwX\nC2HMIyMjaVMyKb7vvvsAqK+vT9uWdyQT+TDxLXr2zOkk3IqTX4Hqiv3XFzkmdJ1xxhkdW7duXexx\niIgcl84991xuv/32rqN9X+Uci4iIiIhEmhyLyJJnZlsszbsSEZGlrGL/sF70kKM7UcjSHQpjIe2g\nsSqkJDhZ6kRtXfhSNDWFtv7+vrStoSHk7ba2hdze6prsd4okcSKfapG2laRVOLl0h2L4OJ+HnPxs\n7ukN+cQH9h9M2x5//PFw75j3nKR6QD5XOTxfPuXCklwST15zaSYcmgoiIvPnnl29bLjqe4s9jEXV\n9ZErFnsIIiKzosixiIiIiEhUsZHjkqVuwORKDQDFYhZFHR8fB6C6JkRTkyoPAPsP7AegpjZ8udrb\n29O2JGJsMYZcrmhFEh2evFCueMgYBvoHwv327QPgkUceS9v6+kIke9OmTQC0tLSkbUmFjaQvsywi\nnESOsyh2LprtKlIhxx8zOx94B3AxsAI4CPwK+Iy7fz2ecyXwUuBs4ARgPJ5zjbt/MdfXBuCx3Of5\n/4NvdPdLF+5JRETkWFTBk2MRqTRm9ofANYQcov8EHgJWAc8G3gR8PZ56DXAv8DPgCaATeAlwrZk9\n1d3fH8/rAT4IXAmcHD9OdM1wTFOVo9g0k+tFROTYUrGTYy/GiGl1FkX1GCmdmAgR5PHxsbStNMKa\nzxdOyqgl8iXWkvzepsZlANTWZeXhkjByoRjuVyiMp03l6iNPpOXjwr13796dtq1evRqA5cuXx+vz\nUd8kCh2ur67Jco6TVOiaJOc4l0lT9MmRdJFjmZk9DfgU0Ac8z93vLWlfl/v0THd/pKS9Dvg+cJWZ\nfdrdd7l7D7DZzC4FTnb3zQv5DCIicuyr2MmxiFScPyV8z/qr0okxgLvvzH38SJn2MTP7JPDrwPOB\nL8zHoNz93HLHY0T5nPm4h4iIHD2aHIvI8eLC+Pr9w51oZuuBdxEmweuBxpJT1s7v0EREpFJU7OQ4\nWefmuUVn47GUm/sQAFVVpT8voaYmfEnyi/eSxXATExOHtCXbGm7YsAGAxsasz6SvRD5VI2nLp1Uk\n5dm6u8P91qxZk7Y97WlPA2BkJGz53NOTpXq0toWUjhUrwvV1ddl9k0V6xapwH6vK7letSm5yfElW\nwu6a7iQzOwW4DVgO3ARcD/QS8pQ3AG8A6qe6XkRElraKnRyLSMXpia9rgfunOe/thAV4b3T3z+cb\nzOw1hMmxiIhIWRU7OU4ipkm0N38s2TQjH7VNIrnJJhv5qO9JJ50EZIvokkVxkEWOk/uMjo6mbWNj\nYcFfUmqttjYrD5dEn/PnJ/13dHQAcNZZZ6Vtvb29k17r6rO+WltbY//hPvkIdelGJHnTtYkcg24l\nVKV4MdNPjk+Lr98o03bJFNcUAMys2n3+VqqeubaNrdoEQ0TkuKJNQETkeHENMAG8P1aumCRXraIr\nvl5a0v5C4A+m6DvJU1p/xKMUEZHjWsVGjkWksrj7fWb2JuDTwB1m9m1CneNO4DxCibfLCOXe3gj8\nu5n9B7AbOBN4EaEO8qvLdH8D8Crg/5nZdcAwsN3dr13YpxIRkWNNxU6Ok7+M5negmyiEFAhPAua+\nLG0rTIQUi6qYarCsOduBrqmpCYCenpDy2NXVlbYldY6THfbyKRfJsUQ+VSNJq0jSMiBL6ejrDTvl\nHTzYk7b19/dP6r+lJRt7W2s4NjQUFxpalnJBSf3mJHUDJqeViBwP3P1fzewe4C8IkeGXA/uBu4HP\nxHPuNrPLgL8GriB8n7sLeAUhb7nc5PgzhE1Afht4Z7zmRkCTYxGRJaZiJ8ciUpnc/RfAbx3mnFsI\n9YzLOSTZPuYZvyf+JyIiS1jFTo6LXkw+SI81NYUo767dTwKTy7wlZdTGx5OIcxbRrYolz5LI7MMP\nP5y2JRHgZz877ANQnauPlrSV2w0vkV+kl0SHi/G8fFuyQ15LS4hot7e3p229vaH02/Bw2Lmvubk1\nN4YQvS63+E4L8kREREQm04I8EREREZGociPHacQ4yzmujZtjdB/sBmBsNMsJXrFiRfwoKfOWRVWr\nq8N169aFkm75jT5uu+02IIverl17UtpWX98QxxLGkM9BTsaXLzWXlH6rsnC/JJoNWfS5oSH0WVeX\nRZV7eg4CMDAwCEBHR5b3jE2dV6ycYxEREZHJFDkWEREREYk0ORYRERERiSo2rSJbj5eb/3tIRWhr\nC2kH4+NjaVOyU125xXNJ6kNzc1hst3rV2rTt5PWnADAyHM4xsgV52Xq38MH4WLbxVrLYrqY6S98o\nxAyL5taQtpHfBS9Z6JeUqBsdHUrbVq4K6Rf1DeH8/oGsBFxTTAFJFigmiwRhcpk7EREREVHkWERE\nREQkVbmR4zTymytXFqPIdXWhpNsDD9yfNiWL7FatWgVAc3Nzrq/wOjg4NOlzgBNOOBGArVu3AtDd\nnUVtV65cCWQbfeRLpyVjyC/SGx0Nkezq6mSc2T9PsoAvOX9gsD9tG4vXNTaGxXrJwj6A0bHRSX0q\nciwiIiIyNUWORURERESiyo0cx0ir5ab/4zGKOjAQtmfOb+e8Z88eAEZGwkYap5xyStqWlE9LIrJJ\nfjJkEebqmKvc25NFjtfGqHL/eCjzVhe3moYsajtpe+uY21yIx8pVWksi4hPjWQm4Ynq+H/JcoyNh\nrPXxGYz89tGKHIuIiIjkKXIsIiIiIhJpciwiIiIiElVsWgVx4VlhIstNGB4LO8jVxzJqmzZtStv6\n+kLqQ7Lgra6uLm1L0ii6u7sPaWtrbQVg7ZoTAFjW2JS2Jb95tLeEcyZyaQxJn0mKB2RpEXfecUcY\nZ0NWyu3ss88BspSJfKm5NNUit9te2hZfx8fCvZOScADFgh1yvoiIiMhSpsixiBwXzGyL2TT7oZe/\nxs1sywINSUREKlDFRo6TRWr5aOr4WIgKt8Rob36Tjfq4WK41tuVLrF1//fWT+sqXZKuPJdle8Ou/\nHvrMRZWTBXxNTSGaPBgX+wH0x4jxgQMH0mO33PILALp79gFQVZXdp7c3RLYvuugiYHJJtnSBYDy9\nuiqLDifnjcVNQ2prs0WBpsCxiIiIyCQVOzkWEQHOAIYOe9YCuWdXLxuu+t5i3X5BdH3kisUegojI\ngtLkWEQqlrvff/izREREMhU/Oc7XEfa4PM1L6gJDlg7R399/yHVJ22mnnTbpHIAdOx4H4O577wbg\nuTHtAWCsENId6jxcn+xSl793b29vemzNmtUAnLxhHQA1Ndn5yc57Q0MhCFZXm6Vv1NfUTuqzOlfn\nuIqQCjIYazxPTGTpIrW1WVqJyGIys5cBbwGeBnQAB4CHgK+5+6dKzq0B3gm8EVgP7AW+DLzf3cdK\nznXgRne/NHdsM/AB4DLgZOCtwCagH/gu8B53f3LeH1JERI4LWpAnIovKzP4I+DZhYvwd4B+A64BG\nwgS41JeB/w3cBFwDDBMmy/88y1u/Dfg0cBfwceCBeL9bzGzlrB9EREQqQsVGjpPFc/nd7JKPq2rD\nArn8wrrk/KRc23guwrpq1apwLC7S27gx2z1v0xmhHNy3vvUfADz9GU9L2+qbw+K3sYkkmJX9LjI4\nGMrK3XXXXemxCy98DgANjSGie//996Zta9eG3fZ27twJQGN9Q9rW2b580vNU1+SeK4mOx9f8M+ej\n4yKL6I+BMeBZ7r4332BmK8qcfyrwdHc/GM95L2GC+3oze/csor4vBi5w9zty97uaEEn+CPD7M+nE\nzLZO0bRpiuMiInIMU+RYRI4FE8B46UF331/m3HclE+N4ziDwJcL3s2fP4p7X5ifG0WagF3itmdUf\neomIiFS6io0cJyXM8pHjRE3M0a2tyXJuCxOFSceGh4fTtoaGEKVNor09Pd1p2/h4iAqPDIcybfl8\n5BWdnaHv2hCVrqrK7peMa+/eLFD20EMPhvNiJbZ8SVez8HvMnj17AFi9clXaNtbUDMBEfIZJG30U\nPfYZjo3kyskp51iOEV8ipFLcZ2ZfBW4Ebnb3fVOc/8syx3bE1+WzuO+NpQfcvdfM7gQuIVS6uPNw\nnbj7ueWOx4jyObMYj4iIHAMUORaRReXuHwPeAGwH3gx8E9hjZj81s0Miwe7eU6abpKB5dZm2qeyZ\n4niSltE2i75ERKRCaHIsIovO3b/g7hcCncAVwGeBXwN+uICL41ZPcXxNfO2dol1ERCpYxadV5Mu1\nJTvV1dWEMmhVngWZGuoaAWhuCAGowni2WG0iLsRrqmuI12V9bjgplF1rfP7zAWiNKQ4A4yMhdcLj\ndZZbKNfeEs577oXnp8eShX99MTUjv9Ndz8Hwc7rWwtjra7MFee7hd5xkR72amixVcmQ0pIIU4oLD\n/I6B+Y9FjgUxKnwdcJ2FXKLfI0ySv7EAt7sE+EL+gJm1AWcBI8C2I73BmWvb2KpNM0REjiuKHIvI\nojKzy8zKbmaeJNYv1A53rzOzs0uObSakU3zF3Q9dsCAiIhWvYiPHSdm1qqpD5/+jMaLbWN+YHksW\nsbW1hjTDKsuuG4mL8+obwpdr5arOtG3VqlBpatWKWHEq9yO+Orl3jDQnEWiA9tZWAJ75jDPTY8li\nvv6+8NrXO5C21cVocE1NGGdz07K0rVAIUe5ksV1LS2vuacOACmXKto2PH1IcQGQxfBMYMLNbgS7C\nm/Z5wHnAVuDHC3Tf7wM3m9nXgSeAi+N/XcBVC3RPERE5xilyLCKL7SrgvwmVHd5E2IijFngXcJm7\nL9RvcVfH+51Ftkve54GLSusti4jI0lGxkePkr7T5v9YmUeQkYjo4NJi2JfnIiXyZs6KHqGt7R4gq\nt7e3p20DsbxbXTw/X0Yt+TiLYtshbS2tLemx5uYwhkIx2bDjQNqWRIVra2sOGe+BA+G8pPxcvlxb\ncu/0dSzbXbdcVF3kaHP3TxN2qjvceZdO0/Z5wsS29Hi5dI3DXiciIkuXZkciIiIiIpEmxyIiIiIi\nUcWmVSRlysZyaQRJKkKS0pCUe4NDF6fV1Wfl0Kw2pkpUh98lhsfHcteFj5O/3ebTOJIycsmhJF0C\noOjh3vnUBo/tKzo7AGjOpU50dT0GQENDSN8YGsoW6yU795WmhsRBhJfxpERdVr6tqqZi//lFRERE\n5kSRYxFZUtx9s7ubu29Z7LGIiMixp2JDh8mitHzkOImwNjeHDTjyUd7kvORYfg/a8UKIttYVwpdr\nfCKLMieL9QrFEAnOR6BHR0PJuGJaRu3Q30UaGrLNPOrrw8dJaTbIIs3LloXSbXV1YROQnp5sB91k\nzA0NWWm6RBKZTqLljbn7FcuUdxMRERFZyhQ5FhERERGJNDkWEREREYkqNq0iWZCXLIoD6O3tBaAn\nvtbUZo+/Zs0JQG6xXm7RXV19UsM42fEuVzvZwvnFuJiuaFmqwk033QTAbbf9d3J22tbZGXbUe+Hl\nL0yPrVwVdsutr6+ZfD+gpiY5Fu6XX8iXpG8k5+fTJZKPk69HPpWk/I69IiIiIkuXIsciIiIiIlHF\nRo4T9bmSbMkOcjt37wYmBYAZj2XdOjpCGbXqasu1hS/T4FD4PFkUB7A87pZXKMTFd7md9U488UQA\nnnjiCQAefWx72vba17wWgNq67PwdOx4HoLNzeeh7eVvalkSAk4WGSRQ8PNfkhX/JgkPIosPJgsP8\nzn/53fxERERERJFjEREREZFUxUaOk4hpkmsLsH//fgDuv//+0FbMNgHZsWMHAKtXrwGgsTEXHe4I\nEdwVK0KecHuMFkOW77v90YeAyVHbTZs2AfDa174GgBtv/Hna9oIXXA5AfX12n/7+fiDLHU4i3ZBF\nfpNc47pcxLm2tn7SOQMD2QYhSTQ5yb1OSsIBmOl3IxEREZE8zY5ERERERCJNjkVEREREoopNqyjE\n3eXGcyXZuuKCt7379gKwLJcCMVIdvhQ9Bw8AULuyM21riecNxXSFlqbsutG4QK4mpjZUWfYl7e0N\naRInn3wKAC95SWvWZ0tIb8inTiSLARsamuP12S54IyPhOdraQorHqlUtaVtVVVJOLqRQJOkZAL09\nvbHPJgDc8yXgst38RI4XZtYF4O4bFnckIiJSiRQ5FhERERGJKjZyPDYeoqIPPfxweuy+bdtCW4z2\nnnvW2WnbWWedNen6/Fq1ZcsaAeju7gZgZ1y8B7A6btyxonMlMHlzjqTEWnd3T/x8JG3r6+sDoLGx\nMT2WlFZLIr8HD3anbQ0NDWHsY2GBYaGQbW6SlGcrxHJ0+c09krJz1VXhn7owkW0QkpwvIgvjnl29\nbLjqe4s9jGl1feSKxR6CiMgxRZFjEREREZGoYiPH+/aFsm2744YfkEVKk3Jmp592etqWlGBLzhkd\ny6K8SWm0JAqb35L6sa6ucOyxRwFoaMgiwX19Id93//6Qxzw4OJgb3z4ATjnllPRYsmFJT0+INDc1\nNaVtSeQ4uXdSQi4vP65EXexzYjw813iutN3oyKF9iBwLLPz548+APwVOBQ4A3wTeO8X59cDbgN+J\n508AdwGfcPevT9H/m4E/Bk4p6f8uUE6ziMhSVbGTYxE5rn2cMHl9AvgXYBz4DeACoA5IV9qaWR3w\nQ+AS4H7gk0AT8Erga2Z2lru/p6T/TxIm3rtj/2PAy4Dzgdp4PxERWYI0ORaRY4qZXUSYGD8CnO/u\nB+Px9wI/BU4AtucueQdhYvx94GXuPhHP/yBwG/BuM/uuu98Sjz+PMDF+ELjA3Xvi8fcAPwZOLOn/\ncOPdOkXTppn2ISIix44KnhyHFIPOzqwk2wMPPADAqRtDKkNbe1valuwul5RWy6coNDaFVInquNiu\noT4r5VZdHRbD/fjHPwBgopClLZz59KcDsH79OgBqarLd8GpqaifdF7KFeCNxweDatWsPeap8mbZE\nfhEgTF5oNzYa+i/G5xkZyUrHHTx48JC+RI4Bb4yvf5NMjAHcfcTM3k2YIOf9HuF/+LcnE+N4/l4z\n+yvgM8AfALfEpjfk+u/JnT8W+8+2shQRkSWngifHInKcOie+3lim7edA+tufmbUApwG73P3+Muf/\nJL6enTuWfFxuEnwrIV95xtz93HLHY0T5nHJtIiJy7KrYyXF7ezsANTXZIzbFRXfJsXzUdiIuVEsW\nw62KJdoAqpK6bvG1mFVDo7UlRJ/PPPNZAOzZ82TatnFjWPDX0BAWxSWl2iArt5ZfQveLW28F4LG4\nuK+1Nds0ZDyWpnsy9t/fl0WQV6xYAWSLCltasg1CkudKrs9vOjI8PITIMSj5k86e0gZ3nzCz/WXO\nfWKKvpLj7TPsv2BmB2YxVhERqTAq5SYix5re+Lq6tMHMaoAVZc5dM0VfJ5ScB9A3Tf/VQGfpcRER\nWToqNnIsIset2wnpCJcAj5a0XQykf4Jx934zewQ4xcxOd/eHSs6/LNdn4g5CasXFZfq/kHn8vnjm\n2ja2apMNEZHjSsVOjuvrQipD3fLsES88/zwAjJDSMNA/kLaNjYcUi97eEGBqbsoW3Y3WhHrA7iGf\nIqk5DNkOdx3tIZi194l9adtD94fd+TZs3ADAsmVZ3eJCMaRNducWxe3bszec1xjqMO98fGfa1tQc\nrrViGHtfb1/altRPbm0N6RQdHR1p27Jl4djQ4KELDZcvz84TOYZ8nrCA7r1m9u1ctYoG4MNlzv8c\n8DfA35vZb7l7IZ6/Anh/7pzEFwiL+JL+e+P5dcCHFuB5RETkOFKxk2MROT65+81m9gngfwP3mNl/\nkNU57ubQ/OKPAi+O7XeZ2XWEOsevAlYBf+fuP8/1f6OZ/QvwR8C9ZvaN2P9LCekXu4EiR27Dtm3b\nOPfcsuv1RETkMLZt2waw4Wjf18rtqiYisphyO+T9GZN3sHsPZXawi1HltwOvZfIOeZ9096+U6b8K\neAthh7yNJf3vBB5x97OO8BlGCSkgdx1JPyILKKnFXa7Si8ix4FlAwd3rj+ZNNTkWEYnM7HTC5iBf\ndffXHGFfW2HqUm8ii03vUTnWLdZ7VNUqRGTJMbM1MXqcP9ZE2LYaQhRZRESWIOUci8hS9FbgNWa2\nhZDDvAZ4PrCOsA31vy/e0EREZDFpciwiS9GPCLlslwMdhBzlB4F/Aj7uyjcTEVmyNDkWkSXH3W8A\nbljscYiIyLFHOcciIiIiIpGqVYiIiIiIRIoci4iIiIhEmhyLiIiIiESaHIuIiIiIRJoci4iIiIhE\nmhyLiIiIiESaHIuIiIiIRJoci4iIiIhEmhyLiIiIiESaHIuIzICZrTOzz5nZbjMbNbMuM/u4mS2f\nZT8d8bqu2M/u2O+6hRq7LA3z8R41sy1m5tP817CQzyCVy8xeaWafMLObzKwvvp++OMe+5uX78VRq\n5qMTEZFKZmanArcAq4BvA/cD5wNvAV5kZs919wMz6Kcz9vMU4CfAV4FNwBuBK8zsOe7+6MI8hVSy\n+XqP5nxwiuMTRzRQWcreBzwLGAB2Er73zdoCvNcPocmxiMjhfYrwjfjN7v6J5KCZfQx4G/A3wrPB\nqQAAIABJREFUwJ/MoJ8PESbGH3P3d+T6eTPwj/E+L5rHccvSMV/vUQDcffN8D1CWvLcRJsUPA5cA\nP51jP/P6Xi/H3P1IrhcRqWgxSvEw0AWc6u7FXFsL8ARgwCp3H5ymn2XAXqAInODu/bm2KuBR4OR4\nD0WPZcbm6z0az98CXOLutmADliXPzC4lTI6/5O6/O4vr5u29Ph3lHIuITO+y+Hp9/hsxQJzg3gw0\nARcepp8LgUbg5vzEOPZTBH5Ycj+RmZqv92jKzF5tZleZ2dvN7MVmVj9/wxWZs3l/r5ejybGIyPSe\nGl8fnKL9ofj6lKPUj0iphXhvfRX4MPAPwHXA42b2yrkNT2TeHJXvo5oci4hMry2+9k7RnhxvP0r9\niJSaz/fWt4GXAusIf+nYRJgktwNfMzPlxMtiOirfR7UgT0RERABw96tLDj0AvMfMdgOfIEyUf3DU\nByZyFClyLCIyvSQS0TZFe3K85yj1I1LqaLy3PkMo43ZWXPgkshiOyvdRTY5FRKb3QHydKoft9Pg6\nVQ7cfPcjUmrB31vuPgIkC0mb59qPyBE6Kt9HNTkWEZleUovz8lhyLRUjaM8FhoBbD9PPrcAw8NzS\nyFvs9/KS+4nM1Hy9R6dkZk8FlhMmyPvn2o/IEVrw9zpociwiMi13fwS4HtgA/FlJ8wcJUbRr8zU1\nzWyTmU3a/cndB4Br4/mbS/r589j/D1XjWGZrvt6jZrbRzDpK+zezlcC/xU+/6u7aJU8WlJnVxvfo\nqfnjc3mvz+n+2gRERGR6ZbYr3QZcQKi5+SBwUX67UjNzgNKNFMpsH30bcAbwG4QNQi6K3/xFZmU+\n3qNmdiXwaeDnhE1pDgLrgZcQcjl/CbzA3ZUXL7NmZi8HXh4/XQO8kPA+uyke2+/ufxHP3QA8Bmx3\n9w0l/czqvT6nsWpyLCJyeGZ2EvCXhO2dOwk7MX0T+KC7d5ecW3ZyHNs6gA8QfkicABwAvg/8X3ff\nuZDPIJXtSN+jZvYM4B3AucCJQCshjeJe4OvAP7v72MI/iVQiM9tM+N43lXQiPN3kOLbP+L0+p7Fq\nciwiIiIiEijnWEREREQk0uRYRERERCTS5FhEREREJNLk+DhkZhvMzJMFFSIiIiIyP2oWewCLKZat\n2QB8y93vXNzRiIiIiMhiW9KTY+BK4BKgC9DkWERERGSJU1qFiIiIiEikybGIiIiISLQkJ8dmdmVc\nzHZJPPRvyQK3+F9X/jwz2xI//x0zu9HMDsTjL4/HPx8/3zzNPbfEc66cor3WzP7IzG4ws31mNmpm\n283s+ni8eRbP9ywz2xPv90UzW+rpMyIiIiIzslQnTcPAHqADqAX64rHEvtILzOyfgP8NFIHe+Dov\nzGwt8F3grHioCPQQ9h5fD7yAsF/4lhn0dRHwPaAduAb4M9c2iCIiIiIzsiQjx+7+NXdfA9wSD73F\n3dfk/juv5JJzgT8n7Ane6e4dwPLc9XNmZvXAdwgT4/3AG4BWd+8EmuK9P87kyftUfV0O/IgwMf5b\nd3+TJsYiIiIiM7dUI8eztQz4sLv/ZXLA3fsIEecj9fvA2cAo8Hx3vzt3jwJwe/xvWmb2CuArQB3w\nbnf/yDyMTURERGRJ0eR4ZgrAxxao79fH13/LT4xnw8zeCPwr4S8Bb3L3a+ZrcCIiIiJLyZJMq5iD\nh919/3x3ama1hLQJgOvm2Mdbgc8CDrxeE2MRERGRuVPkeGYOWaA3TzrI/g0en2MfV8fXv3T3Lx75\nkERERESWLkWOZ6aw2AOYxlfj61+Y2fmLOhIRERGR45wmx/NjIr42THNOW5ljB3PXnjzHe78O+H9A\nK/BDMzt7jv2IiIiILHlLfXKc1Cq2I+ynJ76uK9cYN/A4o/S4u48DW+OnL5nLjd19AvhtQjm4duBH\nZvaMufQlIiIistQt9clxUoqt/Qj7+VV8vdzMykWP3wbUT3HtF+LrlWb2zLncPE6yXwX8AOgEfmxm\nh0zGRURERGR6S31yfG98fYWZlUt7mKnvEDbpWAl8wcxWAZhZm5m9F9hM2FWvnM8CdxImzzeY2evM\nrCleX21mzzazfzWzC6YbgLuPAr8J3ACsin2dfgTPJCIiIrLkLPXJ8bXAGHAxsN/MdplZl5n9fDad\nuPtB4Kr46auAPWbWTcgp/mvgLwkT4HLXjgIvA+4BVhAiyX1mth8YAv4b+AOgcQbjGIl93QicAPzE\nzDbO5llERERElrIlPTl29/uBFxDSEXqBNYSFcWVzhw/T1z8BrwZuJUxqq4Cbgd/M76w3xbU7gGcD\nbwZ+DvQTduV7AvghYXJ82wzHMQT8z3jvdcBPzWz9bJ9HREREZCkyd1/sMYiIiIiIHBOWdORYRERE\nRCRPk2MRERERkUiTYxERERGRSJNjEREREZFIk2MRERERkUiTYxERERGRSJNjEREREZFIk2MRERER\nkUiTYxERERGRqGaxByAiUonM7DGgFeha5KGIiByvNgB97r7xaN60kifHDlAsFtMDZjbji0dHh9OP\n+wd6AWhubgBgeGQgbXviye0APPjgvaFteCRta27qBGCoMAjAmhPWp23LmtoBeOSRB9JjQ/3hnqdu\nPA2Ak08+OW1btWo1ADU1jQBUV9dOOfb8nwNKnzi/XXhVemLVzL8wIjJTrY2NjR1nnHFGx2IPRETk\neLRt2zaGh4cPf+I8q+TJsYhIWWa2AXgM+P/c/coFuk3XGWec0bF169YF6l5EpLKde+653H777V1H\n+74VPznOR4unixwnEdXknKosrMrwcIgUV1WPA1Bf35C2NdSHCHB1zTIATj/9qWnbaac+HYB9A0+G\ncxtbs/sVqwEYK2SR7Sd3PQFAMUaFh8cLadtoPK8q/otVT3rI5MXyn07+2EsPHPKJyLw6ShNQERGR\neVXxk2MRkcVyz65eNlz1vcUexnGj6yNXLPYQRERUrUJEREREJFHxkeOZpFKUO6+mJvvS1NaFtuGR\nPgCqqrKkhoPd/QDUNbQAsHZdtqCyoakt9DUeFuQ1LMvW5dTGhXWb2rNjp54W0jDGx8bjfXOL7mrq\nk5GGsZNbWBd/x7FJZ5SIB/PPPDg6BMCy+uZyV4jMmZltBj4QP32Dmb0h1/xGQgWHnwIfBK6L5z4H\nWA5sdPcuM3PgRne/tEz/nwfekJxb0nY+8A7gYmAFcBD4FfAZd//6YcZdBVwNvBn4JvA77n70V4OI\niMiiqfjJsYgsii1AO/AW4C7gW7m2O2MbhAnxu4GfA58jTGbH5npTM/tD4BqgAPwn8BCwCng28CZg\nysmxmTUAXwJeAXwSeLO7F6c6X0REKtOSnhzno8Wjo6MAjMWobWNjfdpWVx8iszt3hYV1Dz7QlbZV\n1YWFeGc+85kANDRlUdhirJBWFxfiDY5kC+yqqsJ9amqzzJb6xtBXbQgq48UsyluIkeID3d3h3Fxk\ne/ny5ckTTfmsI0mJuVwizb0PbAPggmc+e8rrRObC3beYWRdhcnynu2/Ot5vZpfHDy4E/cfd/PtJ7\nmtnTgE8BfcDz3P3ekvZ101zbQZhMXwRc5e5/O4v7TlWOYtNM+xARkWPHkp4ci8iiu3M+JsbRnxK+\np/1V6cQYwN13lrvIzE4GfgCcCrzO3b80T+MREZHjUMVPjsvlFRcKEwAMDg2lbQMDIXf4vvtCNPX0\n00/P2oZDibV777sHgJWdp6RtT39aCA41tYTybiMj2V+ELUaO3cKXeeeTe9K2+tomAGpqs2hvVRxr\nTXU4vzqf91wT8pxHesKGJMtblqVtyUYn1bH8nOciyMlHvX3hugPdB9O2G376E0CRY1lUt81jXxfG\n1+/P4pqnAr8AmoEXu/sNs72pu59b7niMKJ8z2/5ERGRxqVqFiCymJ+exrySPedcsrnkKcALwKHD7\nPI5FRESOU5oci8hi8sO0TfXXrfYyx3ri69pZ3P87wHuAs4AbzKxzFteKiEgFqvi0ivyiu2RR20As\nyXbXtjvTtn0HDgDw2Pbt4fXJR9O28bFQim3V6hUAPOu8LA2huSWUcBsfH49Hsi9pMaZJVMVt7Wpr\nstJsybCKxWx8fYPhPtVJWkV1bh+8uGi+uaYutDU2pU2j8d71MfViuDCetlXFPgYnQjWqH9/0k7Tt\nxphW8Z63vBORBZCsQK2e9qypdQMnlR40s2rCZLbUrYSqFC8G7p/pTdz9w2Y2TCjhtsXM/oe77znc\ndTNx5to2tmpjCxGR44oixyKyULoJ0d/1c7z+NmC9mV1ecvx9wMllzr8GmADeHytXTDJdtQp3/zhh\nQd/TgRvN7MQ5jllERI5zFRs53vNESGUcGM4W3Z18ygYADvTsB+DBRx5I23btCYEiqwm/L3TvOJC2\nPeX0pwCwfkNYpFdTn5V5KxRDcCwEs0rE8HBDXTh/eVtL2tTTMwBAXVXWV0NDrOEWg8n5jUiqYl9J\nMLlQlf1eU4gR6h07dgDwi623pm2rVq8Oz7c7pGH+6MfXp20H9mfPKDLf3H3AzP4LeJ6ZfQl4kKz+\n8Ex8FHgh8G0z+xphM4+LgI2EOsqXltzvPjN7E/Bp4A4z+zahznEncB6hxNtl04z302Y2AnwW+JmZ\n/bq7Pz7DsYqISIVQ5FhEFtLrgO8BLyLsgvdXzLCCQ6wc8XLgXuC3CTvidQHnA9unuOZfCTvjfZcw\nef4/wMuAfYSNPQ53z88Dv0uITP/MzE6Z/goREak0FRs53vKTkE9blduCed368FfVu+/+FQCPPpr9\nfO3uC3nI7Z1hQ40Lz78gbTtpbVjf09gU8ny9OPWmWfltnZNNPJL84qrcHh1Dg6F0XKEulx9cG8ca\nLxjJlZprWRZKt7W3hy2pu/fvTdu+t+WnAOzqegyAbQ/dl7a1d4TnORBzqp/ck6VS5svciSwEd38Y\neOkUzVPvWpNd/5+UjzRfGf8rd80vgN86TL9dU93f3b8CfOVwYxMRkcqkyLGIiIiISKTJsYiIiIhI\nVLFpFT/6wQ8B2HTm09Njjz8W0ij6+0LJtGIhO7+lNZRNPf+8sMnWOWdnlaKKcUe9ZIFcPhkhSU2I\nGRRpKgVki+gsntO+LNvVrmHjhth3dn4ynPGJcL/h4eHcGEL6RVUs17Zn//607bvf/x4AQ72hzGvB\nJ9K2fQf2TxrzYf+OLSIiIrKEKXIsIiIiIhJVbOS492A3AHdszXaELcb46YanbATg9NOfmratXR9K\nsa7fEMqnJgvYANpaQwm2JHJcXZ39TmExFlsshLhvd09P2tbbGxbdWYz21tU3pG11dWFxX219tmCQ\n2G8aS17emjYVCmERoNWGMRQtF7+OEeqJGAofGh5Mm5Ix19XVTe6byRukiIiIiIgixyIiIiIiqYqN\nHCe5wIMD/emxe391DwD9oyGyesHFF6Vta9eFXWr7+sPmHDVVWUJyFin2SX0DjE+EXODb7whbUf/s\nxpvTtrq6ECnuWNEBwLp12UZh9UnkOLeldFLrLckrrq/LNgipbwh9FWNpun379qVtY3EMhVhirrY2\n67MQI9r9/eHrUMyXoStMXZJOREREZClS5FhEREREJNLkWEREREQkqti0iv6JESBbhAfAQNgFb2go\ntC3vWJE2TcQSbEkyRV1V9qWxuPhtbHwUgJqaLN2B6nDevv1hAd+9996TNq1aEfo/95wzATjrrKw8\nXHUxpE7UTIxlYx4Ii/mGR8OxsVypuaG+kO7xwKMPAXDn3XekbSODoa0QF+RZrpzc2MgIedWW/T6k\npAoRERGRyRQ5FhERERGJKjdyHCOmVVVZuTKrDtHaCQ8x04lCFppNgq3JejXPhVUn4iYgSVd9/dki\nv4KH3y82nnoKABdccF7atn/PXgB27XoSgMd3/zi7bjzccFlNNr6G+rgQr7ExfN6YbRrS1Bw+Pjku\nHLzjl/+dtg0Pxk1NPDxPdW7BYBpFTsu2ZW0+qbCbiIiIiChyLCIiIiISVWzkeNWq1QA05Dbe2Hj6\nqQCsPCG0FXLhYY8h47p0o49D+6yKB7sPZBt9TMTIcUMstbZ69Zq07b5f3QfAuD8GwIO7DqZtBwdC\nZLs2FzmuiaHpWMmN1qasJNuKuCFIfXXYzGNHjEYDVCXbWk+EyHExFxFPY8N+aBk6BY5FREREJlPk\nWESOC2a2xcxm9SudmbmZbVmgIYmISAXS5FhEREREJKrYtIoVnasAaG5sSo897ZnPAqBhWUiBqMnt\nJJcuV0tSE3J9JbvMJTvltbe1p239w6G82+hwSJNoa21N26qqYspFYwsAy1c2Zn22hD6rqqsOOb+x\nNoymNTudNWvCLnv7d4VFfsXqbOzLWsI9+3vCrnkTuV3wkjQKsyx9Q2QJOQMYWqyb37Orlw1XfW+x\nbp/q+sgViz0EEZHjRsVOjkVE3P3+xR6DiIgcXyp2cmwWVrVVWW5lXdyww+OKN88FU5MFeTVxk4z6\n+iwyW4yba4yPhc05amrrsutiZHZ4ZBiAg93dadvwcAhYWVUoBWeMZ9eNh2M+kY2vrj6EiutjVHhS\nemV12HiksS1EkJd3ZsGw8dFQWi5ZiFfIL8ibJnKsaLIcK8zsZcBbgKcBHcAB4CHga+7+qZJza4B3\nAm8E1gN7gS8D73f3sZJzHbjR3S/NHdsMfAC4DDgZeCuwCegHvgu8x92fRERElqSKnRyLyPHBzP4I\n+GfgSeA7wH5gFfBMwgT4UyWXfBl4HvB9oA94CWGyvCqeP1NvAy4Hvgb8ALg4Xn+pmV3g7vtmOP6t\nUzRtmsVYRETkGFG5k+MYAa6qzm0DneQYx5JphVxubiHmGlNIosRZ1LYqBncL8fSJQrYls8X7DA6G\nSO7YWBa4OumkdQCcGPOFm9qz/OLVg7Fsm2eR42RrZ4t5yBOWRZqJ59U1hM1AGppbsiZPhh7zpXPP\nlZRrS54mHyuedJ7I4vljYAx4lrvvzTeY2Yoy558KPN3dD8Zz3gvcBbzezN49i6jvi4EL3D3di93M\nriZEkj8C/P6sn0RERI57qlYhIseCCcjlHUXuvr/Mue9KJsbxnEHgS4TvZ8+exT2vzU+Mo81AL/Ba\nM6ufSSfufm65/wDlO4uIHIc0ORaRxfYloAm4z8yuNrOXm9nKac7/ZZljO+Lr8lnc98bSA+7eC9wJ\nNBAqXYiIyBJTsWkVEzUhkWC0OksdiJXSSDal8+JE2lYkflwVrhvLtVV7WIBXFxfiHTyY7XRXVRPS\nHDo6w8/yk048MW27+OILAahvCue0NHZk1xVDikd1Vfb7yUTMfdged7978NHtaVsSUquuCv9kB1uW\npW09+3YBsGfPHgAKY9kOfhaL0iULDqtyi/C8oAV5svjc/WNmth94E/BmQlqDm9mNwP9x91+WnN9T\nppvkf9gye1tOac8Ux5O0jLZZ9CUiIhVCkWMRWXTu/gV3vxDoBK4APgv8GvDDw0SRj8TqKY4ne8D3\nLtB9RUTkGFaxkeOh/hDdrcpVdnr0vl8BsGJNJwCNp2xM25K9ODwuXavNRVjrYpTX43K2puZsY5GR\n0RCoSjYIaWnLIrrtbeG8/r6wWK+tPvtyNzU0HzLm4Rjk/q9v3AzA9T/O/dU3Lixctixs+LGsJdsh\nxKrC+DpXngDAE4N9WVuMGMe1fmm5NwBzRY7l2BKjwtcB11lY7fp7hEnyNxbgdpcAX8gfMLM24Cxg\nBNh2pDc4c20bW7UBh4jIcUWRYxFZVGZ2mZUvur0qvi7UDnevM7OzS45tJqRTfMXdRxfoviIicgyr\n2MixiBw3vgkMmNmtQBeh4uDzgPOArcCPF+i+3wduNrOvA08Q6hxfHMdw1QLdU0REjnEVOzmuHgq7\nxtVUZfWKdz9wX/zodABWrlqVtlksFlwbayF7dbauZ3hoJPYVvlwNdVlKQ39/CGqNx3SM/QeyGshN\njSGl0cfCsb7BLBA1UDwAQMeaE7K+YvuDDz0IwNh4rq/6sDaoWAxpIkNDWXrEyEgcX0y9yNdOrokL\n/mric42NZWMoWLboUGQRXQW8EDiHsKHHCLAdeBdwjbsfUuJtnlxNmJi/FXg1MAB8nrBD3t5prhMR\nkQpWsZNjETk+uPungU/P4LxLp2n7PGFiW3p82sT6qa4TEZGlq2Inx3WxFFvVeBYpTRfdxd3wRoeH\n07b6mhBZbWmKC+WKWcR5KC6oq7Fwznht1tbTExa0NzU1AFBbnaVxF8bDfU5YGSLUB/dk+xns2PE4\nAKvWrUuPVVWHn+MNDeE+z3jG07LxNYZxdfeGhYbuWYm6kzeEPnq6Q1v/k7vTtsG+UPWqMBoiztnI\nYXRioQJyIiIiIscnLcgTEREREYkqNnLcNxrycIdy0dGGiVBabXznTgBGJ7Kc286OsEHH8viaXzzv\nMR95xYrQ58hIFo2eqAo7zHZ0tMd+WtK24TiGW+4Lu8ievv60tG3tKacC8ODOHemx1tawuVd7eyjX\ntvvJfWnbvrjxSE9vdizR3x9Kt7UuC3nJF1x8cdo2FNtG4liq8rnUo1nkXEREREQUORaRJcbdN7u7\nufuWxR6LiIgcezQ5FhERERGJKjat4mAsv+a5JWiNcSHeeE9INXi0a3valiRR1NfXx9eGtK2mOiyQ\nW706LKyrjmXRAFadtAGA4eEBAIqnZgvsRkcGAbj6o/8IwOtf84a07VnnnAfADbfekh573gUXArB8\neUiPuP+hh9O2iWSBoIVnKBSyBXm9fWFRYLIwv2Hl8rStOu7md+KJoWTcmlzpuH1PHpqiISIiIrKU\nKXIsIiIiIhJVbOR4MJYuK+ZKno0WQvR1bCREXydvWBsjsrEE3MR4tpCvpSUskNu394nQZ/4+t9wK\nQFXcbOO5F12QtjU1hujz7kdDabX7730gbTvvoosAWL9+bXqsc+XK8Bo3J6lvqEvb2ptCBHh4JES2\n3XJF2eLmJEUPr1aTRb3HR8JzbN+xB4CRsey6u355W/zoHYiIiIiIIsciIiIiIqmKjRyvXHEiAFVV\nWXi4sSFs+zwxEaKnE4UsOlxVFeLBI6Nhw499e59M21asCDm8ra0hF3hwYDBt27XzEQCKMSf4v265\nLW1ri+c3N4QNPLq2d6Vt924LUWSLOc4ADzwc2rt2hAj1srYsd/ikk04K4xsLke18RNzi5iH79oVy\nb53LV6Ztp50Stsreuy/kF+/bsydt279POcciIiIieYoci4iIiIhEmhyLiIiIiEQVm1Zx8vqnAJPT\nKtraQppDMS66GxoaSNsGBkM5tMGhUOZtzxNZWkV/f0i1aGpcBkBNbbZQrr4+fFwdy70lu+kBjI6F\ncnLPOO8sAA72ZekYt98dds1raG5Pj40Xwrh6esO4rDpbWLfvYH/4IN6nWMjuMz4SdrobHQyv23u7\n0rbBFSviOEP6huW+HvUxzUREREREAkWOReSYZGZuZltmcf6l8ZrNJce3mOXLu4iIiEytYiPHxLJm\noyNj6aE9wwcAKBbDQrzungNpW3fPfgAslnQbiwvfAPbtPTCpL7Psd4pkId7atWuAbOEcwJ69YfFb\nS2eIDo9addpWVxdKs7W3ZYvnxgqjQLYByUMPPZS2HTgYFtt1LG8J545kP+uH+kPEeHwsRJdr67JF\nfiMjI/F+IcLd1JhFi0/ZeCpSOeIE8EZ3v3SxxyIiInK8qtzJsYgsNbcBZwD7F3sgIiJy/KrYyfH4\neIjCJptzAHjcJNo9RHDr6rKc3saGEMkdGAw5x81NLWlbsRiiyUODoc/xiawEXLKTyJNPhhzlgYEs\njzkxNBhylsdGR9JjW28L20avWpVtAlKMW10XYpm2/fuyn/ETE+Ha2pGw5fVgfxY57hsIYxgaD/dp\nbmlL21avWROfIfRpVVn0evnyzkPGKnK8cvch4P7FHkfePbt62XDV947a/bo+csVRu5eISKVSzrHI\nUWJmV5rZN8zsUTMbNrM+M7vZzH63zLldZtY1RT+bY27tpbl+k9+WLoltPkX+7f8ys5+ZWW8cw6/M\n7N1mVl9ym3QMZrbMzK42sx3xmjvN7OXxnBoze6+ZPWRmI2b2iJn9+RTjrjKzPzGz/zazATMbjB//\nqeVzlQ697kQzu9bM9sb7bzWz15Y5r2zO8XTM7IVmdp2Z7Tez0Tj+vzez9sNfLSIilahiI8cix6Br\ngHuBnwFPAJ3AS4Brzeyp7v7+OfZ7J/BB4APAduDzubYtyQdm9iHg3YS0gy8DA8CLgQ8BLzSzy919\njMlqgR8BHcC3gTrgNcA3zOxy4E3ABcD3gVHgVcAnzGyfu3+tpK9rgdcCO4DPAA78JvAp4GLgd8o8\n23LgFqAH+DegHfhfwJfMbK27//1hvzpTMLMPAJuBg8B3gb3AM4G/AF5iZs9x97659i8iIsenip0c\nNzaFQFi+lFtDXOhWUxPaJiZWpG2jYyH9YDCmVYyODqdtI8Ph497e0NY/kP28HBoeiueHlItkARzA\nsmWh9Ft9vF//wR1p28MPhsV2jzfdlx4rxn+OZEFdXW5hXVNDCKydtC48w5NDWVm4B7aHlI6RqhA8\nXJkLwiUBuUIhpIZMjGcpIYWJbJc9OSrOdPdH8gfMrI4wsbzKzD7t7rtm26m73wncGSd7Xe6+ufQc\nM3sOYWK8Azjf3Z+Mx98NfBP4n4RJ4YdKLj0RuB241N1H4zXXEib4/w48Ep+rJ7Z9jJDacBWQTo7N\n7DWEifEdwK+5+0A8/j7gRuC1ZvY9d/9yyf2fGe/z2+4h38jMPgJsBf7GzL7h7o/O7isGZnYZYWL8\nC+Alyfhj25WEifgHgbfNoK+tUzRtmu24RERk8SmtQuQoKZ0Yx2NjwCcJv6g+fwFv/3vx9a+TiXG8\n/wTwDqAI/MEU1741mRjHa24CHiNEdd+Vn1jGierNwJlmufIs2f2vSibG8fxB4F3x03L3L8R7FHPX\nPAb8EyGq/bopn3h6b46vf5gff+z/84RofLlItoiIVLiKjRw3N4cFdflNOepjJLamJpQ1a8htglE7\nGn6OJ5t6ZPFmGB0Lf2nu6AhR4sHBLGq7f18o19bb2x3ahobSNos/zsfj5hz9B7qzTgtj3rxiAAAg\nAElEQVShcSi3gM+qwxgK4+H8aprTtobW5QCMFFoB6MtFtgsWo8HxWetzEee4Do/R0Yn4efZkTcuU\nVnk0mdl6wkTw+cB6oHQXlrWHXDR/zomvPyltcPcHzWwnsNHM2ty9N9fcU25SD+wGNhIiuKV2Eb63\nrIkfJ/cvkkvzyLmRMAk+u0zb43EyXGoLIY2k3DUz8RxgHHiVmb2qTHsdsNLMOt39QJn2lLufW+54\njCifU65NRESOXRU7ORY5lpjZKYRSY8uBm4DrgV7CpHAD8AbgkEVx8ygpYfLEFO1PECbs7XFcid7y\npzMBUDKRntRGiOzm73+wTE4z7j5hZvuBVWX62jPF/ZPod9sU7YfTSfj+94HDnLcMmHZyLCIilaVi\nJ8fVVeHncnV1buONuO1zkoebXx+fRJGrJ0JktVDI5ePG8me1tWHu0tiURXRbW0Je8eBA2ICjuzuL\nDvf2hL/W3nPnXQD09GW5yh43KbHq7J/A45xiohDmDwODhbRtbDwc23twKJ6TbVIyEePc1dXhtakx\nG9/w0Gi8XyzllouJJ7nQclS8nTAhe2P8s30q5uO+oeT8IiF6Wc5cQv7JJHYNIU+41Akl5823XqDD\nzGrdfTzfYGY1wAqg3OK31VP0tybX71zHU+XuHXO8XkREKpRyjkWOjtPi6zfKtF1S5lg3sNrMasu0\nPXuKexSB6ina7oivl5Y2mNlpwDrgsdL823l0B+H7za+Vafs1wrhvL9O23sw2lDl+aa7fubgVWG5m\nT5/j9SIiUqEqNnIscozpiq+XAt9JDprZCym/EO02Qr7qG4F/yZ1/JfDcKe5xADhpirbPAb8PvM/M\n/tPd98X+qoGPEiaun53Rk8zN5wi51h82s0vjhh2YWRPwkXhOuftXA39rZq/JVavYSFhQNwF8cY7j\nuRq4AvhXM3ulu+/ON5pZM/AMd791jv0DcObaNrZqYw4RkeNKxU6Om5tDakF+h7y6urp4LATXjGyx\nXqEYvhQ9vaEUW39/9hfepAxakqJRX5+lIzQ0hL9wty8PC+ZOWJvNTfp6QxBux/awnmhgOEu3rI1l\n1/JpFVi4j3shjilLqxgeDX+JHhoL6RuWK1FXUxvKu9XFceV3wRuOpeaampoO+XpMSh2RhfYpwkT3\n383sPwgL2s4EXgR8HXh1yfmfiOdfY2bPJ5RgO4uwkOy7hNJrpW4AftvMvkOIwo4DP3P3n7n7LWb2\nd8A7gXviGAYJdY7PBH4OzLlm8OG4+5fN7DcINYrvNbNvEeocv5ywsO9r7v6lMpfeTaijvNXMrier\nc9wOvHOKxYIzGc8NZnYV8GHgITO7jlCBYxlwMiGa/3PCv4+IiCwhFTs5FjmWuPvdsbbuXxMiljXA\nXcArCBtcvLrk/PvM7H8Q6g6/lBAlvYkwOX4F5SfHbyFMOJ9P2FykilCr92exz3eZ2R3AnwOvJyyY\newR4H/AP5RbLzbPXECpT/B7wx/HYNuAfCBuklNNNmMD/HeGXhVbgPuCjZWoiz4q7/62Z3UyIQl8M\n/AYhF3kXIVp/RP0DG7Zt28a555YtZiEiIoexbds2CIvWjyrLlzoTEZH5YWajhLSQuxZ7LCJTSDaq\nuX9RRyEytWcBBXc/qhUEFDkWEVkY98DUdZBFFluyu6Peo3KsmmYH0gWlahUiIiIiIpEmxyIiIiIi\nkSbHIiIiIiKRJsciIiIiIpEmxyIiIiIikUq5iYiIiIhEihyLiIiIiESaHIuIiIiIRJoci4iIiIhE\nmhyLiIiIiESaHIuIiIiIRJoci4iIiIhEmhyLiIiIiESaHIuIiIiIRJoci4jMgJmtM7PPmdluMxs1\nsy4z+7iZLZ9lPx3xuq7Yz+7Y77qFGrssDfPxHjWzLWbm0/zXsJDPIJXLzF5pZp8ws5vMrC++n744\nx77m5fvxVGrmoxMRkUpmZqcCtwCrgG8D9wPnA28BXmRmz3X3AzPopzP28xTgJ8BXgU3AG4ErzOw5\n7v7owjyFVLL5eo/mfHCK4xNHNFBZyt4HPAsYAHYSvvfN2gK81w+hybGIyOF9ivCN+M3u/onkoJl9\nDHgb8DfAn8ygnw8RJsYfc/d35Pp5M/CP8T4vmsdxy9IxX+9RANx983wPUJa8txEmxQ8DlwA/nWM/\n8/peL8fc/UiuFxGpaDFK8TDQBZzq7sVcWwvwBGDAKncfnKafZcBeoAic4O79ubYq4FHg5HgPRY9l\nxubrPRrP3wJc4u62YAOWJc/MLiVMjr/k7r87i+vm7b0+HeUci4hM77L4en3+GzFAnODeDDQBFx6m\nnwuBRuDm/MQ49lMEflhyP5GZmq/3aMrMXm1mV5nZ283sxWZWP3/DFZmzeX+vl6PJsYjI9J4aXx+c\nov2h+PqUo9SPSKmFeG99Ffgw8A/AdcDjZvbKuQ1PZN4cle+jmhyLiEyvLb72TtGeHG8/Sv2IlJrP\n99a3gZcC6wh/6dhEmCS3A18zM+XEy2I6Kt9HtSBPREREAHD3q0sOPQC8x8x2A58gTJR/cNQHJnIU\nKXIsIjK9JBLRNkV7crznKPUjUupovLc+QyjjdlZc+CSyGI7K91FNjkVEpvdAfJ0qh+30+DpVDtx8\n9yNSasHfW+4+AiQLSZvn2o/IEToq30c1ORYRmV5Si/PyWHItFSNozwWGgFsP08+twDDw3NLIW+z3\n8pL7iczUfL1Hp2RmTwWWEybI++faj8gRWvD3OmhyLCIyLXd/BLge2AD8WUnzBwlRtGvzNTXNbJOZ\nTdr9yd0HgGvj+ZtL+vnz2P8PVeNYZmu+3qNmttHMOkr7N7OVwL/FT7/q7tolTxaUmdXG9+ip+eNz\nea/P6f7aBEREZHpltivdBlxAqLn5IHBRfrtSM3OA0o0UymwffRtwBvAbhA1CLorf/EVmZT7eo2Z2\nJfBp4OeETWkOAuuBlxByOX8JvMDdlRcvs2ZmLwdeHj9dA7yQ8D67KR7b7+5/Ec/dADwGbHf3DSX9\nzOq9PqexanIsInJ4ZnYS8JeE7Z07CTsxfRP4oLt3l5xbdnIc2zqADxB+SJwAHAC+D/xfd9+5kM8g\nle1I36Nm9gzgHcC5wIlAKyGN4l7g68A/u/vYwj+JVCIz20z43jeVdCI83eQ4ts/4vT6nsWpyLCIi\nIiISKOdYRERERCTS5FhEREREJNLk+DhkZhvMzJOcMRERERGZH0t6++i4MncD8C13v3NxRyMiIiIi\ni21JT46BK4FLgC5Ak2MRERGRJU5pFSIi8v+3d+dxktXlvcc/T1Xve/fsC7OwDUPYFERwY9CIaxLi\nDaKJRshyY4hXY+KNeK9ewRjFLJrEV8RcN27QRExM4oIEFB12RIYdhmWWhmHW3ve9fveP51fnFE11\nT0/TPUv19/16zauqz3PO7/yqul49Tz/9W0REJFJyLCIiIiISLcjk2Mwui5PZLoiHvpGf4Bb/tRae\nZ2ab49e/ZWa3mVlHPH5xPH5d/Pqqae65OZ5z2RTxcjP772Z2q5m1mdmImT1rZrfE47WH8PrONLP9\n8X7fNLOFPnxGREREZEYWatI0BOwHWoByoDcey2ubfIGZ/T3wP4Ac0BMf54SZrQJ+CJwVD+WAbnx7\nxTXAG/EtETfPoK1XATcCTcC1wB8F7fQiIiIiMiMLsnIcQrghhLAc35sb4EMhhOUF/14x6ZKzgQ/g\n2x4uCiG0AM0F18+amVUCP8AT43bgfUBDCGERUBPv/be8MHmfqq2LgB/jifHnQghXKDEWERERmbmF\nWjk+VHXAZ0MIn8ofCCH04hXnl+p3gZcBI8AbQgiPFNxjAngg/puWmb0D+BegAvhYCOGaOeibiIiI\nyIKi5HhmJoDPz1Pbvx0fv1GYGB8KM7sc+Ar+l4ArQgjXzlXnRERERBaSBTmsYha2hRDa57pRMyvH\nh00A/GiWbfwx8DUgAL+txFhERERk9lQ5npkXTdCbIy2k34PnZtnGF+Ljp0II33zpXRIRERFZuFQ5\nnpmJI92BaXw7Pn7EzM49oj0REREROcYpOZ4b4/GxappzGosc6yy4du0s7/1e4N+BBuBmM3vZLNsR\nERERWfAWenKcX6vYXmI73fFxdbFg3MBj4+TjIYQxYEv88q2zuXEIYRx4F74cXBPwYzM7fTZtiYiI\niCx0Cz05zi/F1vQS23k0Pl5kZsWqxx8GKqe49p/i42VmdsZsbh6T7EuA/wIWAT8xsxcl4yIiIiIy\nvYWeHD8eH99hZsWGPczUD/BNOpYA/2RmSwHMrNHM/jdwFb6rXjFfAx7Ck+dbzey9ZlYTr8+a2Tlm\n9hUze+V0HQghjAC/DtwKLI1tnfQSXpOIiIjIgrPQk+PrgVHgNUC7me02s1Yzu/NQGgkhdAJXxi8v\nAfabWRc+pvjTwKfwBLjYtSPArwKPAYvxSnKvmbUDg8AvgN8DqmfQj+HY1m3ACuCnZrb+UF6LiIiI\nyEK2oJPjEMKTwBvx4Qg9wHJ8YlzRscMHaevvgUuBe/GkNgPcBfx64c56U1y7CzgH+CBwJ9CH78q3\nF7gZT47vm2E/BoG3x3uvBn5mZmsO9fWIiIiILEQWQjjSfRAREREROSos6MqxiIiIiEghJcciIiIi\nIpGSYxERERGRSMmxiIiIiEik5FhEREREJFJyLCIiIiISKTkWEREREYmUHIuIiIiIREqORUREREQi\nJcciIiIiIlHZke6AiEgpMrOdQAPQeoS7IiJyrFoH9IYQ1h/Om5Zsctzb2xsAQgjJsUwmFsqz/mhm\nScxyMWRZf5KbSGL7du8CoH+gD4CR0aEkNjLsz7s6uwBo3ftsEqutrwagrtof733ssSTWPTjo9xtP\n+1Ax7J3YcM6rAHjdyevS+/T5vQdCnV/3/PNJbHSkHYCqU/yzc9JZL09iVeWVAFSWex++9vVvJLHP\nXvOXAOzasyfthIjMlYbq6uqWjRs3thzpjoiIHIu2bt3K0NDQwU+cYyWbHGeznuTmcrnkWD45tkyR\n5Bg/L5vxZLqnuzuJjcRvTHm8bn9HRxLb9vSTAJx37isAqGiqSmLdOb9uIuuJ9hnnn5DEBoZGARgc\nTr8FubEKAEarywHY2bc/idUGT3LbOzoBePbRh5LYkgZ/rB7p8XP2pQn6+lM2AtBYvwqAiy58fRIb\n7j/8HziRo4GZrQN2Av8vhHDZPN2mdePGjS1btmyZp+ZFRErb2WefzQMPPNB6uO+rMcciMi/MbJ2Z\nBTO77kj3RUREZKZKtnIsInKkPba7h3VX3niku3HUaL3mbUe6CyIiB1WyyXF+yMQLhk7kj4X4SBrL\nxKe58REAerrToRPZGMwPx2hvS4c7NDU3AlDdWA/A+J7BJFZfGWNVfl2mvDaJ5fx0Rgv6NxIfu4b8\nWV8cJgHQn/FhGO2V4wA825SOiX507z4A1vb7+IqmtrYkdufd93nfsx5bsmxFElu8dDkiIiIiktKw\nChGZc2Z2FT6mF+B9cXhF/t9lZrYpPr/KzM41sxvNrDMeWxfbCGa2eYr2rys8d1LsXDO7wcx2m9mI\nme01s1vM7J0z6HfGzP4utv3vZlY9u3dARESOVSVbOS4mv3JFvlZrpCtZlJX57wndPV6tHRroTWJV\ncaWHgYF+ACrK07etvMon0XUPe0X3ppvvSmLbn94OwNrliwBYVVC1XbViGQCNi2vS+zT7ZL4Vtb4i\nRa4undzXN+7tN9b4xMH1jWcmsZ1P+WT47U9tA2DRUFpVrqryavXDD/tKGf1b7k9iufi70eXv/wNE\n5thmoAn4EPAw8J8FsYdiDOB84GPAncDXgcXA6Gxvama/D1wLTADfB54BlgLnAFcA35nm2irgW8A7\ngH8APhhCyE11fsF1U824O+WQOi8iIkeFBZUci8jhEULYbGateHL8UAjhqsK4mW2KTy8C3h9C+MeX\nek8zOxX4EtALvDaE8Pik+Opprm3Bk+lXAVeGED73UvsjIiLHpgWaHHvtuHBMydjoMAB745rGI8Pp\n2OHKrFeHLV9EmhhPYg/84lEALlxxkp9TuSiJ5Wq9+vzAc3sA+MmDjySx2qwv17aovj45tuI4rywv\nX+qPS1Y3JbHly5a/4PzqinT8MvG6rI0BsKe1NQmtblkKwGnn+1Jz27bvSGJdPX2IHGEPzUViHP0h\n/jPtzycnxgAhhOdffAmY2Vrgv4ATgPeGEL51KDcNIZw9RbtbgJcXi4mIyNFrgSbHInKUuG8O2zov\nPt50CNdsAO4BaoG3hBBuncP+iIjIMUgT8kTkSNo3h23l/9Sy+xCuORlYAewAHpjDvoiIyDFqQVWO\n02XdfCJeJu6iB9Adt3/uavdl0Kqr02ELY3E0RS7O36upSJdfa6z2YCYO1ahrWJrElp3ib2/LmO9O\nt6QjXWItjPqco4nRdL7Pjq4BAJ7a7X8Rrnoy7d/apccDUBv7Xt+cTiZcsX4NAOtafEhlbjwd9rGt\n1RcMWFG3BIBFKxenfSirROQICweJTfUzqqnIsfy2lquAJ2d4/x8ATwGfAW41szeGEDoOco2IiJSw\nBZUci8hhlV82JTvtWVPrAo6bfNDMssBZRc6/F1+V4i3MPDkmhPBZMxsCvgBsNrNfDiHsP9h1M3Ha\nqka2aOMLEZFjSsknx8U2Ackv6ZYjrdoO9PsybeMjcSuOqnSJtYHhIQD6uzoBGI5LugFUV3tbTz3l\nS6UZBcuvDXk1uiLjE/qWNKVLuWV8Ph4TafdoyXkukTHv18jQUBKbGPdC2eNbvaq88+Z7k9jiRl/K\n7cSN6/3x5ScksTXLvKo8OuiV6tWrV6ZtDh1AZB514dXfNbO8/j7gzWZ2UQjhloLjHwfWFjn/WuD9\nwCfM7OYQwhOFQTNbPdWkvBDC35rZML7axW1m9voQwp5Z9ltERI5hJZ8ci8iREULoN7OfA681s28B\nT5OuPzwTfw28Cfiemd0AdOJLra3H11HeNOl+T5jZFcCXgQfN7Hv4OseLgFfgS7xdOE1/vxwT5K8B\nt8cE+bkZ9lVEREqEJuSJyHx6L3Aj8Gbgk8CfM8PlzeLKERcDjwPvAt4HtALnAs9Occ1XgNcAP8ST\n5/8J/CrQhm/scbB7Xge8B69M325mx8+kryIiUjpKvnKcH0JRKJv13wlyBesV98VhFaMTcYKdpUMu\nBvvaAdi7xyfBDw0NJ7H2fj+vrtaHLRgVSaypwsdOWBxDkaW8oBNxUmBhX5MRmt5GZiyNVZX5cI2Q\n8/u0d6QT8vv6ffhGe/deAO69/84ktup4/7/9zDM9HxnsHkhi4+MFNxCZByGEbcCvTBG2KY4XXv99\nileaL4v/il1zD/DfDtJu61T3DyH8C/AvB+ubiIiUJlWORURERESikq8cF5uQNz7mk+66O9qT2ECc\nZGcZf0tyE2lVtbfLJ65ny3zSfXVzulzbeI/vgrcsTnzrGOhMYicu9eXTuoe92ts1mFaqk+p1Lq1s\nl2W8/Zq6RgBGRyeS2PiIPx8b8ap1tjJ9XWe84nQA1q/0Pjz5+NYkluv39p/f4a9hw8ZlSWysoHIu\nIiIiIqoci4iIiIgkSrZyXFgxnnxseNDH3e7bk67q1N/nleNM1scFjxaMx+3t8zG9w7lqAHIF44pr\nGrw6vGbdiQDs2PNQEjtj7SIAtu71scBdI71JrKomLhVXUDkeGhgE0m9KZcEmJRaf1zY2ANC4ZFHa\nh2Y/FjL+u07LojS2es0GAJ7d1wNARU1dEmtoHEFEREREUqoci4iIiIhESo5FRERERKKSHVaRV2wp\nt7I4GY6QLtc2Eie61dT5EIWyyvS6jm4fVhHKfVhGS9PyJLakwodajI56WxUV6Vu6rNaHTnQ0+QS7\nZzr7kth4nAyXCenwj6ZGP68iTvwbzKR9GMv4eXXNPmSifnE6sW5fp/dvd6sv75YbKRguUdcMQH+o\nBeD5tnQSYm9POnlQRERERFQ5FhERERFJlHzl+AXT8mIhtr3tAAADvT1JqKrc34rmOOFt/4F0sl5X\nvy/FdtKGVQCMhPRt27HzaQDuvW8bAKeefk4SKy/3yX2VsVLdUFOfxPLz8Aq/AVVllTHoy7ZVZNOJ\nf+Pm1eRc/H2mtmBiXXWl36crDPlrrq5OYtm4mckE/jha0PdMeSUiIiIiklLlWEREREQkKtnKscWx\nvJmC0vHQoC+V1tHpY20Lt0+urfJqa35zjmdad6WxOt/0o77Ox/ne/OM7ktja47zSXBWLsGE43bhj\n+4F4nwkvE9eVp7+LjMbxzuUFv59Y8P6MjsXxy9VVSWwilpqryzxWla7yxrIa78OZrzoLgIdjNRvg\nhBYfxzy839sejBuSAExMpGOuRURERESVYxERERGRhJJjEREREZGoZIdVkN8hL5eOq+hs94l4Yzkf\nTlC7aEkSq8z4W9HW7ZP0tj71TBJb1tjisQMdADzy6BNJ7MIL3wPAmgvWALBn12AS6xn2JdaGfJU4\nClZmoyz48Iss6fiIidGJeJ4/NtWnwyrG465+dXGoRUNdSxJbt/5kAC599zsBuPoLVyexinJfPq6+\n0odedHany7cNjac79okcbcwsALeFEDbN8PxNwM+Aq0MIVxUc3wxcEEJ48baZIiIik6hyLFIizCzE\nRFBERERmqXQrx3ERt4mCjT4G4qYXE8NeTe0oqKLamG/KsWffHgCy42kFuLnOl3AbGvFjlkl/p1i2\n1DflqGn2Kuyy8nQDjlx8e/sHvA+LxtPNQ2qrmwAot3S5to4DXrUeGvbJcwMjA0msrNmXbquZ8GXb\nsmUF37py3+DjJ5vvBaBtb3pdTza/5JtvSBLG0thETr8bSUm5D9gItB/sRBERkamUcHIsIgtJCGEQ\nePJI96PQY7t7WHfljUe6G0eN1mvedqS7ICJyUCodihwmZnaZmX3XzHaY2ZCZ9ZrZXWb2niLntppZ\n6xTtXBWHUGwqaDc/ov2CGMv/u2rSte80s9vNrCf24VEz+5iZvWhHmHwfzKzOzL5gZrviNQ+Z2cXx\nnDIz+99m9oyZDZvZdjP7wBT9zpjZ+83sF2bWb2YD8fkfmtmUP4vMbKWZXW9mB+L9t5jZbxY5b1Ox\n1zwdM3uTmf3IzNrNbCT2/6/MrGmmbYiISGkp2cpxiMMqhoaHk2M/v/M2ALr27wBgsCf962tmzIcy\nVMYFhDcsW5nEmup92ELbiO9AlylL/x/P4DnFU09s9/sVTHLLVvrbm8uMx7bTIRRV5kM0auIaxQDL\nVvvjRPA1l4eH0hl8Y4P+vM+XWmbxQPqt6+nz9g90eZsnnPjaJNbW65MCDwz668tk093zxgc0P+kw\nuxZ4HLgd2AssAt4KXG9mG0IIn5hluw8BVwOfBJ4FriuIbc4/MbPPAB/Dhx38M9APvAX4DPAmM7so\nhDDKC5UDPwZagO8BFcC7ge+a2UXAFcArgZuAEeAS4Itm1hZCuGFSW9cDvwnsAr6K71n568CXgNcA\nv1XktTUDdwPdwDeAJuCdwLfMbFUI4a8O+u5Mwcw+CVwFdAI/BA4AZwAfAd5qZueHEDRrVURkgSnZ\n5FjkKHRaCGF74QEzq8ATyyvN7MshhN2H2mgI4SHgoZjstRau1FBwn/PxxHgXcG4IYV88/jHgP4C3\n40nhZyZduhJ4ANgUQhiJ11yPJ/j/CmyPr6s7xj6PD224EkiSYzN7N54YPwi8LoTQH49/HLgN+E0z\nuzGE8M+T7n9GvM+7QvAJBGZ2DbAF+Asz+24IYcehvWNgZhfiifE9wFvz/Y+xy/BE/GrgwzNoa8sU\noVMOtV8iInLklWxyHOJSbqMj6S5427Y+BkBm2CvGi+vSpdLKMr58WnO1V3erqtO3Jl8p7u/0iXxW\nsO3e2JA/3/WUV5W7hpL/Yxkc8baGJ7wP5dXpdfW1XuVdsjitbJfFPo/kvI1stieJZYP3p6LGq9iV\njWlbzb4JHo1rPNbcU5/E7n1kLwAdw14Ay/T0p/1r248cPpMT43hs1Mz+AXg98Abgn+bp9r8THz+d\nT4zj/cfN7E/xCvbv8eLkGOCP84lxvOYOM9sJrAc+WphYhhB2mNldwGvMLBtCyG8Zmb//lfnEOJ4/\nYGYfBX4S7z85OZ6I98gVXLPTzP4er5S/F09iD9UH4+PvF/Y/tn+dmX0Ir2QfNDkWEZHSUrLJscjR\nxszWAB/Fk+A1QPWkU1bN4+1fHh9/OjkQQnjazJ4H1ptZYwihpyDcXSypB/bgyXGxqulu/GfL8vg8\nf/8cBcM8CtyGJ8EvKxJ7LoSws8jxzXhyXOyamTgfGAMuMbNLisQrgCVmtiiE0DFdQyGEs4sdjxXl\nlxeLiYjI0atkk+POLl+mzQqWcjtutQ/qbdvlVV6y6Rhgyy9rFpdWGxubSGLlOX/e0+PjdyvK07ct\nXxgbH/eqcl1Nmu+0HYgV50GvUA8XbEiSi/1asmg8OdYYN/3IZXz5tdradIMQG/ehoIMTXmnuG0rz\nl84+f62jvsobFdWNSWygyu9pTV697m9Pq8VDvYf8F3yZJTM7Hl9qrBm4A7gF6MGTwnXA+4AXTYqb\nQ/kPxd4p4nvxhL0p9iuvp/jpjANMSqRfEMPHKxfev7PImOZ89bodWFqkran+vJGvfjdOET+YRfjP\nv08e5Lw6YNrkWERESkvJJsciR5k/wROyy0MI1xUG4njc9006P4dXL4uZzUoK+SR2OT5OeLIVk86b\naz1Ai5mVhxDGCgNmVgYsBopNfls2RXv5RcNn298eIBNCaDnomSIisqBoKTeRw+PE+PjdIrELihzr\nApaZWXmR2DlT3CMHBfuRv9CD8XHT5ICZnQisBnZOHn87hx7Ef968rkjsdXi/HygSW2Nm64oc31TQ\n7mzcCzSb2S/N8noRESlRJVs5vn/LPQAsbUoLQ/UrTgAgU+eFt0zBbnHZuPvdGP5X34FMWrQrH/dl\n1MYHvUi1qDrNPzr6fZ7Sc23+Vg4OD6WdMB9ikSmrjNf3JaGxnP/luW84/f0kJMe8zSVL0m/Ponrv\nT0OZT7arLV+cxMrGvbj2yFNt3pfd6V+Bx8t87lMm5/2qGE//qj1h6ZAOmXet8VEzXMEAABF2SURB\nVHET8IP8QTN7Ez4RbbL78PGqlwP/t+D8y4BXT3GPDuC4KWJfB34X+LiZfT+E0BbbywJ/jSeuX5vR\nK5mdr+NjrT9rZpvihh2YWQ1wTTyn2P2zwOfM7N0Fq1WsxyfUjQPfnGV/vgC8DfiKmf1GCGFPYdDM\naoHTQwj3zrJ9AE5b1cgWbXwhInJMKdnkWOQo8yU80f1XM/s3fELbacCbge8Al046/4vx/GvN7A34\nEmxn4RPJfogvvTbZrcC7zOwHeBV2DLg9hHB7COFuM/tL4M+Ax2IfBvB1jk8D7gRmvWbwwYQQ/tnM\nfg1fo/hxM/tPfJ3ji/GJfTeEEL5V5NJH8HWUt5jZLaTrHDcBfzbFZMGZ9OdWM7sS+CzwjJn9CNiJ\njzFei1fz78S/PyIisoCUbHK8d9czAAx3ppts9PR55XZw1CfRrV+5Nonll3CzOF+ot2AJuGyFV2tf\nv9iHOe7uSKuv+/d5tbazN07gK0uvCxU+4W1gZCR+nVZqLU786xtLl5PLZL3K3dXvFeq27nQIZk3G\n2y2LFe3aqvR1EeI8rlHv1/hgulxbfkKixde8bHlaSa+rVOX4cAkhPBLX1v00XrEsAx4G3oFvcHHp\npPOfMLNfxpdW+xW8SnoHnhy/g+LJ8YfwhPMN+NJsGXyZs9tjmx81sweBDwC/jU+Y2w58HPibYpPl\n5ti78ZUpfgf4g3hsK/A3+AYpxXThCfxf4r8sNABPAH9dZE3kQxJC+Fxcdu6D+CYkv4aPRd6NV+tf\nUvsiInJsKtnkWORoE0K4G1/PuJgXbVcYQriT4mN0H8E3sJh8/gF8o43p+vBt4NsH62s8d900sU3T\nxC4DLityPIdX0L80w/sXvicv2mK7yPmbKf4+bprmmjvxCrGIiAhQwsnxuuU+rrgqm85n6tjvwwrv\nuO1uAPafeFISO+Nkf75qla8mNRi3XQaomPDK79mnn+7nDKVjjv/zp/cB0NXhK2QtXZfG6pfHLakH\nfWxzZTb9f7u80ivGu/cdSI4tWembeOzp8Q2/qmrqkthwrxf1hkd8A5OBkXSS/tCQV4WHMl4lLm8e\nTGITI3ELa7xqnq1Ml5prqiq2cpaIiIjIwqXVKkREREREIiXHIiIiIiJRyQ6rOG5JMwBVFemmY5UV\nPpThB7dsBuCHN92SxO6505d+O26t7+Db0NycxDas9yXgGPGhDfXN6S6/Y0M+ZGKw23epa6hcncQ2\nnnAqAENDPtxhfDRdOm40TpQr2IiP5sV+rLrWh0w0VKeT9SoafIjFyJgPzRgYS+dO9WS93WVrvc9j\ne9OlarvbPTZqPjRkrLo2iVXX1iAiIiIiKVWORURERESikq0c11d7pdUszf8XL/ZK6YkbfPLdzufT\ndf+3P9cKwN5u30Bj5XFrklhlhU9iu+9un8j3W+9N92yoLPf262KBuiO2A9CzyKu7lWU+8c1CWrXd\ns9cn4nUNpqXjSt8XgaZq39Sjpjw9v77Kn4esP+7tTDf6yG87ki3zflYWVJxrGrz9MvNjFQXV4tEJ\nLeUmIiIiUkiVYxERERGRSMmxiIiIiEhUssMqsubjHHIhPTYx4ZPZ+gZ8klp9U30Sq6zwned+6awz\n/PqKdD3gZ/f48IvWZ54CYGh0JIktXb4YgFNPPA6A0dFtSezkZf72luV8ot0TrelEuT3bfV3ktp60\nrUFfwpgD+4cBqKkYSmKNjT6MoqrO12/e396XXhcnCvbu86ET/UPpesoh629Aec6HUFSmG/ixYsVK\nRERERCSlyrGIiIiISFSylePhEa+65iepAYQ4Oa+ruzeek05IC8F3thsf8yrvyhWLk9izTz4JQHmt\nT4YbLlhG7WWnbwDgtBPXA/CL+9PYoqYWAFoafYLdnr508t2m154HQFPj8uRYZ4fvytfd75XtjKX9\nG+zbD8CQF5WpWZIuNdfT3+9Pso0AdIykuwIuWeLtZ7N+zmhfWo1e05RO+BMRERERVY5FRERERBIl\nWzmeyMXBteNp/p8L8Xnwcbj79nelsVGv0mbi5hxV8RFgdMjH9x5/ki8BV1aWVmZP3eBLvjUu9c0/\n+kY6k9j9W37hbWZ8MPHwePp2v/OStwNwwtp1ybFt23y88uZ7fMm4hqZFSWxs0CvFZVkfV1xenU1i\nA6NeFT7lFB8v3dOd9uH7/3EDAIODPqY6s7iC1CgiIiIiklLlWEREREQkUnIsIiIiIhKV7LCKp5/b\nB8CJ69anBy2u62a+1NnoWDrhLTvhQwwufN0rAVi6uCVta9sTADQ0+tJvvd09SeyRhx8BoG6xL822\nbMWyJHbBhRcAMD7sk+Dyu/YBLGvw4Q0To+nQjkzwiXhjAz75bqIqXXetu8cnEZaV+XUVg+nwiJEh\n7/vq8xoAOHVNukTbo7/wIRr72nyoRU1TukNeb88BRATMbDNwQQjBDnauiIiUtpJNjkVEjrTHdvew\n7sob5/Uerde8bV7bFxFZaEo2Of7mD38GwHln9ybHVq1aAcBExie11Tc0JbGRXt9kY+lSnwR30kkn\nJLGmn/tkuIYGr/zmJtKKbscBrxg//Oh9ALTv25/Eait94l5jjS8nV1uwsci2x34OQHVjWk0eGfYN\nQRrLvMLd1562VVHlfa6o9MJWtqAP2TAIwCN33wxAWWVaVV63wpd3O26xV5W37dqRxMpyg4iIiIhI\nSmOOReSYYmbnmtkNZrbbzEbMbK+Z3WJm7yw45zIz+66Z7TCzITPrNbO7zOw9k9paZ2YBuCB+HQr+\nbT68r0xERI4GJVs5fmT7LgCe2L47OdbQ6NXTsbj98xlnnpnEhvp9HPFzB9oA2LU/HY87POqbd1RU\n+pbUFQWroW3c4NtGr1zsv2fsb0x/31jS4Pfrjm1mytO3u7LFK8Y9Q/3JseUtfn5z1s/bsa89ibV1\nexvNi5cCUFeWLuVW1+TV5Mrg1e+BgbSqnMt5LPT7a64K6f3qKzW8Uo4tZvb7wLXABPB94BlgKXAO\ncAXwnXjqtcDjwO3AXmAR8FbgejPbEEL4RDyvG7gauAxYG5/ntc7jSxERkaNUySbHIlJazOxU4EtA\nL/DaEMLjk+KrC748LYSwfVK8ArgJuNLMvhxC2B1C6AauMrNNwNoQwlWz6NeWKUKnHGpbIiJy5GlY\nhYgcK/4Q/4X+zycnxgAhhOcLnm8vEh8F/iG28YZ57KeIiBzDSrZyfNxSn4g2NprudDec8+ER5XF4\nxEjBUm5VjT4R77nOOOxgbCKJZct8CERbmw9tWFafLoe29/mnANi+/WkAVrQsTmLjIz7hrTzjE+yq\n6tPxGFbjv5dkx9L+VWf8uU34km6nHJ+2tfN+/79+T9tOAJqz6US+c07Y6PcLvqRbtmBC3kgcQjIx\n6m1OZAqGUlTWI3IMOS8+3nSwE81sDfBRPAleA1RPOmXVXHUqhHD2FH3YArx8ru4jIiKHR8kmxyJS\ncvLLy+ye7iQzOx64D2gG7gBuAXrwccrrgPcBlfPWSxEROaaVbHJ86dt9A46MVSXHeuJSae1dXh3u\n7Eonpw2MDAPQ1eeT2p59LvkLLauWeZGpvNonzO3vSjcBuf8x31yjq9s381i+cm0S29fpxxprawEY\nSovR7Nv5HADLlqWbhuzb45MAl8Yl5gbGB5LYcSu84ts/6svD1Vc0JLFQ5ZPzOnuG4n3SCXnPd/pS\ndkN9fQB0DIymnVDlWI4t3fFxFfDkNOf9CT4B7/IQwnWFATN7N54ci4iIFFWyybGIlJx78VUp3sL0\nyfGJ8fG7RWIXTHHNBICZZUMIE1Occ8hOW9XIFm3SISJyTNGEPBE5VlwLjAOfiCtXvEDBahWt8XHT\npPibgN+bou2O+LjmJfdSRESOaSVbOW5u9KEMRjo5rbHRh1gsb/LYyMpFSWwi55PzhkeWA3BPWTpZ\n7+GnfYhFpsavK6tamsR2b3sWgLb9PkyifzB9S9fEIRPl3T7Mobo6nchXXeVDJ/r70vWKG1rikIx6\nHzLRu/fpJHbgua3ez3KPZZekk/WePODDL/qG/HedXW0dSayz14eOjPT7X6RzIQnR1b0LkWNFCOEJ\nM7sC+DLwoJl9D1/neBHwCnyJtwvx5d4uB/7VzP4N2AOcBrwZXwf50iLN3wpcAvy7mf0IGAKeDSFc\nP7+vSkREjjYlmxyLSOkJIXzFzB4DPoJXhi8G2oFHgK/Gcx4xswuBTwNvw3/OPQy8Ax+3XCw5/iq+\nCci7gD+L19wGvJTkeN3WrVs5++yii1mIiMhBbN26FXwi9WFlIYSDnyUiIofEzEaALJ6YixyN8hvV\nTDeGX+RIOhOYCCEc1hWGVDkWEZkfj8HU6yCLHGn53R31GZWj1TQ7kM4rTcgTEREREYmUHIuIiIiI\nREqORUREREQiJcciIiIiIpGSYxERERGRSEu5iYiIiIhEqhyLiIiIiERKjkVEREREIiXHIiIiIiKR\nkmMRERERkUjJsYiIiIhIpORYRERERCRSciwiIiIiEik5FhGZATNbbWZfN7M9ZjZiZq1m9rdm1nyI\n7bTE61pjO3tiu6vnq++yMMzFZ9TMNptZmOZf1Xy+BildZvYbZvZFM7vDzHrj5+mbs2xrTn4eT6Vs\nLhoRESllZnYCcDewFPge8CRwLvAh4M1m9uoQQscM2lkU2zkZ+CnwbeAU4HLgbWZ2fghhx/y8Cill\nc/UZLXD1FMfHX1JHZSH7OHAm0A88j//sO2Tz8Fl/ESXHIiIH9yX8B/EHQwhfzB80s88DHwb+Anj/\nDNr5DJ4Yfz6E8KcF7XwQ+Lt4nzfPYb9l4ZirzygAIYSr5rqDsuB9GE+KtwEXAD+bZTtz+lkvRttH\ni4hMI1YptgGtwAkhhFxBrB7YCxiwNIQwME07dcABIAesCCH0FcQywA5gbbyHqscyY3P1GY3nbwYu\nCCHYvHVYFjwz24Qnx98KIbznEK6bs8/6dDTmWERkehfGx1sKfxADxAT3LqAGOO8g7ZwHVAN3FSbG\nsZ0ccPOk+4nM1Fx9RhNmdqmZXWlmf2JmbzGzyrnrrsiszflnvRglxyIi09sQH5+eIv5MfDz5MLUj\nMtl8fLa+DXwW+BvgR8BzZvYbs+ueyJw5LD9HlRyLiEyvMT72TBHPH286TO2ITDaXn63vAb8CrMb/\n0nEKniQ3ATeYmcbEy5F0WH6OakKeiIiIABBC+MKkQ08B/8vM9gBfxBPl/zrsHRM5jFQ5FhGZXr4S\n0ThFPH+8+zC1IzLZ4fhsfRVfxu2sOPFJ5Eg4LD9HlRyLiEzvqfg41Ri2k+LjVGPg5rodkcnm/bMV\nQhgG8hNJa2fbjshLdFh+jio5FhGZXn4tzovikmuJWEF7NTAI3HuQdu4FhoBXT668xXYvmnQ/kZma\nq8/olMxsA9CMJ8jts21H5CWa9886KDkWEZlWCGE7cAuwDvijSeGr8Sra9YVraprZKWb2gt2fQgj9\nwPXx/KsmtfOB2P7NWuNYDtVcfUbNbL2ZtUxu38yWAN+IX347hKBd8mRemVl5/IyeUHh8Np/1Wd1f\nm4CIiEyvyHalW4FX4mtuPg28qnC7UjMLAJM3UiiyffR9wEbg1/ANQl4Vf/iLHJK5+Iya2WXAl4E7\n8U1pOoE1wFvxsZz3A28MIWhcvBwyM7sYuDh+uRx4E/45uyMeaw8hfCSeuw7YCTwbQlg3qZ1D+qzP\nqq9KjkVEDs7MjgM+hW/vvAjfiek/gKtDCF2Tzi2aHMdYC/BJ/D+JFUAHcBPwf0IIz8/na5DS9lI/\no2Z2OvCnwNnASqABH0bxOPAd4B9DCKPz/0qkFJnZVfjPvqkkifB0yXGMz/izPqu+KjkWEREREXEa\ncywiIiIiEik5FhERERGJlByLiIiIiERKjkVEREREIiXHIiIiIiKRkmMRERERkUjJsYiIiIhIpORY\nRERERCRSciwiIiIiEik5FhERERGJlByLiIiIiERKjkVEREREIiXHIiIiIiKRkmMRERERkUjJsYiI\niIhIpORYRERERCRSciwiIiIiEv1/r6AjlDmNU4AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f835aff75f8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
